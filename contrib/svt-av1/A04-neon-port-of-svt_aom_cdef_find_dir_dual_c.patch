From 66ce52d6bdbedfbbe2242b814396e256a38a391e Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Wed, 31 Jan 2024 17:22:54 +0000
Subject: [PATCH 2/9] neon port of svt_aom_cdef_find_dir_dual_c

---
 Source/Lib/Common/ASM_NEON/CMakeLists.txt    |   1 +
 Source/Lib/Common/ASM_NEON/cdef_block_neon.c | 200 +++++++++++++++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c    |   5 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h    |   3 +
 test/CMakeLists.txt                          |   2 +-
 test/CdefTest.cc                             |  89 ++++++++-
 6 files changed, 288 insertions(+), 12 deletions(-)
 create mode 100644 Source/Lib/Common/ASM_NEON/cdef_block_neon.c

diff --git a/Source/Lib/Common/ASM_NEON/CMakeLists.txt b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
index 129b474c..090f6d97 100644
--- a/Source/Lib/Common/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
@@ -18,6 +18,7 @@ target_sources(
   PUBLIC aom_convolve8_neon.c
   PUBLIC av1_inv_txfm_neon.c
   PUBLIC av1_inv_txfm_neon.c
+  PUBLIC cdef_block_neon.c
   PUBLIC cdef_filter_block_neon.c
   PUBLIC cfl_neon.c
   PUBLIC compound_convolve_neon.c
diff --git a/Source/Lib/Common/ASM_NEON/cdef_block_neon.c b/Source/Lib/Common/ASM_NEON/cdef_block_neon.c
new file mode 100644
index 00000000..67e0a92d
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/cdef_block_neon.c
@@ -0,0 +1,200 @@
+/*
+ * Copyright (c) 2024, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+#include "EbDefinitions.h"
+#include <arm_neon.h>
+
+/* Here partial A is a 16-bit vector of the form: [x8 x7 x6 x5 x4 x3 x2 x1]
+ * and partial B has the form:[0  y1 y2 y3 y4 y5 y6 y7].
+ * This function computes (x1^2+y1^2)*C1 + (x2^2+y2^2)*C2 + ... + (x7^2+y2^7)*C7 + (x8^2+0^2)*C8
+ * where the C1..C8 constants are in const1  and const2.
+ */
+static INLINE int32x4_t fold_mul_and_sum(int16x8_t partiala, int16x8_t partialb, int32x4_t const1, int32x4_t const2) {
+    /* Reverse partial B. */
+    partialb = vextq_s16(partialb, partialb, 7);
+    partialb = vrev64q_s16(partialb);
+    partialb = vextq_s16(partialb, partialb, 4);
+
+    /* Interleave the x and y values of identical indices and pair x8 with 0. */
+    const int16x8x2_t tmp = vzipq_s16(partialb, partiala);
+
+    /* Square and add the corresponding x and y values. */
+    const int32x4_t partiala_s32 = vpaddq_s32(vmull_s16(vget_low_s16(tmp.val[0]), vget_low_s16(tmp.val[0])),
+                                              vmull_high_s16(tmp.val[0], tmp.val[0]));
+    const int32x4_t partialb_s32 = vpaddq_s32(vmull_s16(vget_low_s16(tmp.val[1]), vget_low_s16(tmp.val[1])),
+                                              vmull_high_s16(tmp.val[1], tmp.val[1]));
+
+    /* Multiply by constant. */
+    const int32x4_t scaled_partiala_s32 = vmulq_s32(partiala_s32, const1);
+    const int32x4_t scaled_partialb_s32 = vmulq_s32(partialb_s32, const2);
+
+    /* Sum all results. */
+    return vaddq_s32(scaled_partiala_s32, scaled_partialb_s32);
+}
+
+static INLINE int32x4_t hsum4(int32x4_t x0, int32x4_t x1, int32x4_t x2, int32x4_t x3) {
+    const int32x4x2_t t0 = vzipq_s32(x0, x1);
+    const int32x4x2_t t1 = vzipq_s32(x2, x3);
+
+    x0 = vcombine_s32(vget_low_s32(t0.val[0]), vget_low_s32(t1.val[0]));
+    x1 = vcombine_s32(vget_high_s32(t0.val[0]), vget_high_s32(t1.val[0]));
+    x2 = vcombine_s32(vget_low_s32(t0.val[1]), vget_low_s32(t1.val[1]));
+    x3 = vcombine_s32(vget_high_s32(t0.val[1]), vget_high_s32(t1.val[1]));
+
+    return vaddq_s32(vaddq_s32(x0, x1), vaddq_s32(x2, x3));
+}
+
+static INLINE void compute_directions(int16x8_t lines[8], int32_t tmp_cost1[4]) {
+    /* Partial sums for lines 0 and 1. */
+    int16x8_t partial4a = vextq_s16(vdupq_n_s16(0), lines[0], 8 - 7);
+    int16x8_t partial4b = vextq_s16(lines[0], vdupq_n_s16(0), 1);
+    partial4a           = vaddq_s16(partial4a, vextq_s16(vdupq_n_s16(0), lines[1], 8 - 6));
+    partial4b           = vaddq_s16(partial4b, vextq_s16(lines[1], vdupq_n_s16(0), 2));
+    int16x8_t tmp       = vaddq_s16(lines[0], lines[1]);
+    int16x8_t partial5a = vextq_s16(vdupq_n_s16(0), tmp, 8 - 5);
+    int16x8_t partial5b = vextq_s16(tmp, vdupq_n_s16(0), 3);
+    int16x8_t partial7a = vextq_s16(vdupq_n_s16(0), tmp, 8 - 2);
+    int16x8_t partial7b = vextq_s16(tmp, vdupq_n_s16(0), 6);
+    int16x8_t partial6  = tmp;
+
+    /* Partial sums for lines 2 and 3. */
+    partial4a = vaddq_s16(partial4a, vextq_s16(vdupq_n_s16(0), lines[2], 8 - 5));
+    partial4b = vaddq_s16(partial4b, vextq_s16(lines[2], vdupq_n_s16(0), 3));
+    partial4a = vaddq_s16(partial4a, vextq_s16(vdupq_n_s16(0), lines[3], 8 - 4));
+    partial4b = vaddq_s16(partial4b, vextq_s16(lines[3], vdupq_n_s16(0), 4));
+    tmp       = vaddq_s16(lines[2], lines[3]);
+    partial5a = vaddq_s16(partial5a, vextq_s16(vdupq_n_s16(0), tmp, 8 - 4));
+    partial5b = vaddq_s16(partial5b, vextq_s16(tmp, vdupq_n_s16(0), 4));
+    partial7a = vaddq_s16(partial7a, vextq_s16(vdupq_n_s16(0), tmp, 8 - 3));
+    partial7b = vaddq_s16(partial7b, vextq_s16(tmp, vdupq_n_s16(0), 5));
+    partial6  = vaddq_s16(partial6, tmp);
+
+    /* Partial sums for lines 4 and 5. */
+    partial4a = vaddq_s16(partial4a, vextq_s16(vdupq_n_s16(0), lines[4], 8 - 3));
+    partial4b = vaddq_s16(partial4b, vextq_s16(lines[4], vdupq_n_s16(0), 5));
+    partial4a = vaddq_s16(partial4a, vextq_s16(vdupq_n_s16(0), lines[5], 8 - 2));
+    partial4b = vaddq_s16(partial4b, vextq_s16(lines[5], vdupq_n_s16(0), 6));
+    tmp       = vaddq_s16(lines[4], lines[5]);
+    partial5a = vaddq_s16(partial5a, vextq_s16(vdupq_n_s16(0), tmp, 8 - 3));
+    partial5b = vaddq_s16(partial5b, vextq_s16(tmp, vdupq_n_s16(0), 5));
+    partial7a = vaddq_s16(partial7a, vextq_s16(vdupq_n_s16(0), tmp, 8 - 4));
+    partial7b = vaddq_s16(partial7b, vextq_s16(tmp, vdupq_n_s16(0), 4));
+    partial6  = vaddq_s16(partial6, tmp);
+
+    /* Partial sums for lines 6 and 7. */
+    partial4a = vaddq_s16(partial4a, vextq_s16(vdupq_n_s16(0), lines[6], 8 - 1));
+    partial4b = vaddq_s16(partial4b, vextq_s16(lines[6], vdupq_n_s16(0), 7));
+    partial4a = vaddq_s16(partial4a, lines[7]);
+    tmp       = vaddq_s16(lines[6], lines[7]);
+    partial5a = vaddq_s16(partial5a, vextq_s16(vdupq_n_s16(0), tmp, 8 - 2));
+    partial5b = vaddq_s16(partial5b, vextq_s16(tmp, vdupq_n_s16(0), 6));
+    partial7a = vaddq_s16(partial7a, vextq_s16(vdupq_n_s16(0), tmp, 8 - 5));
+    partial7b = vaddq_s16(partial7b, vextq_s16(tmp, vdupq_n_s16(0), 3));
+    partial6  = vaddq_s16(partial6, tmp);
+
+    /* Compute costs in terms of partial sums. */
+    const int32x4_t c11           = {840, 420, 280, 210};
+    const int32x4_t c12           = {168, 140, 120, 105};
+    int32x4_t       partial4a_s32 = fold_mul_and_sum(partial4a, partial4b, c11, c12);
+
+    const int32x4_t c21           = {0, 0, 420, 210};
+    const int32x4_t c22           = {140, 105, 105, 105};
+    int32x4_t       partial7a_s32 = fold_mul_and_sum(partial7a, partial7b, c21, c22);
+
+    const int32x4_t c31           = {0, 0, 420, 210};
+    const int32x4_t c32           = {140, 105, 105, 105};
+    int32x4_t       partial5a_s32 = fold_mul_and_sum(partial5a, partial5b, c31, c32);
+
+    int32x4_t partial6_s32 = vpaddq_s32(vmull_s16(vget_low_s16(partial6), vget_low_s16(partial6)),
+                                        vmull_high_s16(partial6, partial6));
+
+    partial6_s32 = vmulq_s32(partial6_s32, vdupq_n_s32(105));
+
+    partial4a_s32 = hsum4(partial4a_s32, partial5a_s32, partial6_s32, partial7a_s32);
+    vst1q_s32(tmp_cost1, partial4a_s32);
+}
+
+/* transpose and reverse the order of the lines -- equivalent to a 90-degree
+ * counter-clockwise rotation of the pixels. */
+static INLINE void array_reverse_transpose_8x8(int16x8_t *in, int16x8_t *res) {
+    const int32x4_t tr0_0 = vreinterpretq_s32_s16(vzip1q_s16(in[0], in[1]));
+    const int32x4_t tr0_1 = vreinterpretq_s32_s16(vzip1q_s16(in[2], in[3]));
+    const int32x4_t tr0_2 = vreinterpretq_s32_s16(vzip2q_s16(in[0], in[1]));
+    const int32x4_t tr0_3 = vreinterpretq_s32_s16(vzip2q_s16(in[2], in[3]));
+    const int32x4_t tr0_4 = vreinterpretq_s32_s16(vzip1q_s16(in[4], in[5]));
+    const int32x4_t tr0_5 = vreinterpretq_s32_s16(vzip1q_s16(in[6], in[7]));
+    const int32x4_t tr0_6 = vreinterpretq_s32_s16(vzip2q_s16(in[4], in[5]));
+    const int32x4_t tr0_7 = vreinterpretq_s32_s16(vzip2q_s16(in[6], in[7]));
+
+    const int32x4_t tr1_0 = vzip1q_s32(tr0_0, tr0_1);
+    const int32x4_t tr1_1 = vzip1q_s32(tr0_4, tr0_5);
+    const int32x4_t tr1_2 = vzip2q_s32(tr0_0, tr0_1);
+    const int32x4_t tr1_3 = vzip2q_s32(tr0_4, tr0_5);
+    const int32x4_t tr1_4 = vzip1q_s32(tr0_2, tr0_3);
+    const int32x4_t tr1_5 = vzip1q_s32(tr0_6, tr0_7);
+    const int32x4_t tr1_6 = vzip2q_s32(tr0_2, tr0_3);
+    const int32x4_t tr1_7 = vzip2q_s32(tr0_6, tr0_7);
+
+    res[7] = vreinterpretq_s16_s32(vcombine_s32(vget_low_s32(tr1_0), vget_low_s32(tr1_1)));
+    res[6] = vreinterpretq_s16_s32(vcombine_s32(vget_high_s32(tr1_0), vget_high_s32(tr1_1)));
+    res[5] = vreinterpretq_s16_s32(vcombine_s32(vget_low_s32(tr1_2), vget_low_s32(tr1_3)));
+    res[4] = vreinterpretq_s16_s32(vcombine_s32(vget_high_s32(tr1_2), vget_high_s32(tr1_3)));
+    res[3] = vreinterpretq_s16_s32(vcombine_s32(vget_low_s32(tr1_4), vget_low_s32(tr1_5)));
+    res[2] = vreinterpretq_s16_s32(vcombine_s32(vget_high_s32(tr1_4), vget_high_s32(tr1_5)));
+    res[1] = vreinterpretq_s16_s32(vcombine_s32(vget_low_s32(tr1_6), vget_low_s32(tr1_7)));
+    res[0] = vreinterpretq_s16_s32(vcombine_s32(vget_high_s32(tr1_6), vget_high_s32(tr1_7)));
+}
+
+uint8_t svt_aom_cdef_find_dir_neon(const uint16_t *img, int32_t stride, int32_t *var, int32_t coeff_shift) {
+    int16x8_t       lines[8];
+    const int16x8_t const_128 = vdupq_n_s16(128);
+
+    for (int i = 0; i < 8; ++i) {
+        int16x8_t tmp = vld1q_s16((const int16_t *)img + i * stride);
+        lines[i]      = vsubq_s16(vshlq_s16(tmp, vdupq_n_s16(-(int16_t)coeff_shift)), const_128);
+    }
+    int32_t cost[8];
+
+    /* Compute "mostly vertical" directions. */
+    compute_directions(lines, cost + 4);
+
+    array_reverse_transpose_8x8(lines, lines);
+
+    /* Compute "mostly horizontal" directions. */
+    compute_directions(lines, cost);
+
+    int     best_dir  = 0;
+    int32_t best_cost = 0;
+    for (int i = 0; i < 8; i++) {
+        if (cost[i] > best_cost) {
+            best_cost = cost[i];
+            best_dir  = i;
+        }
+    }
+
+    /* Difference between the optimal variance and the variance along the
+     orthogonal direction. Again, the sum(x^2) terms cancel out. */
+    *var = best_cost - cost[(best_dir + 4) & 7];
+
+    /* We'd normally divide by 840, but dividing by 1024 is close enough
+     for what we're going to do with this. */
+    *var >>= 10;
+    return best_dir;
+}
+
+void svt_aom_cdef_find_dir_dual_neon(const uint16_t *img1, const uint16_t *img2, int stride, int32_t *var_out_1st,
+                                     int32_t *var_out_2nd, int32_t coeff_shift, uint8_t *out_dir_1st_8x8,
+                                     uint8_t *out_dir_2nd_8x8) {
+    // Process first 8x8.
+    *out_dir_1st_8x8 = svt_aom_cdef_find_dir_neon(img1, stride, var_out_1st, coeff_shift);
+
+    // Process second 8x8.
+    *out_dir_2nd_8x8 = svt_aom_cdef_find_dir_neon(img2, stride, var_out_2nd, coeff_shift);
+}
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 0fe2b034..2f27652b 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -1156,9 +1156,10 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_aom_h_predictor_64x32, svt_aom_h_predictor_64x32_c, svt_aom_h_predictor_64x32_neon);
     SET_NEON(svt_aom_h_predictor_64x64, svt_aom_h_predictor_64x64_c, svt_aom_h_predictor_64x64_neon);
 
-    SET_ONLY_C(svt_aom_cdef_find_dir, svt_aom_cdef_find_dir_c);
-    SET_ONLY_C(svt_aom_cdef_find_dir_dual, svt_aom_cdef_find_dir_dual_c);
+    SET_NEON(svt_aom_cdef_find_dir, svt_aom_cdef_find_dir_c, svt_aom_cdef_find_dir_neon);
+    SET_NEON(svt_aom_cdef_find_dir_dual, svt_aom_cdef_find_dir_dual_c, svt_aom_cdef_find_dir_dual_neon);
     SET_NEON(svt_cdef_filter_block, svt_cdef_filter_block_c, svt_cdef_filter_block_neon);
+
     SET_ONLY_C(svt_aom_copy_rect8_8bit_to_16bit, svt_aom_copy_rect8_8bit_to_16bit_c);
     SET_ONLY_C(svt_av1_highbd_warp_affine, svt_av1_highbd_warp_affine_c);
     SET_ONLY_C(dec_svt_av1_highbd_warp_affine, svt_aom_dec_svt_av1_highbd_warp_affine_c);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index f3e88e96..34d61038 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1143,6 +1143,8 @@ extern "C" {
     void svt_residual_kernel8bit_neon(uint8_t *input, uint32_t input_stride, uint8_t *pred, uint32_t pred_stride, int16_t *residual, uint32_t residual_stride, uint32_t area_width, uint32_t area_height);
 
     void svt_cdef_filter_block_neon(uint8_t *dst8, uint16_t *dst16, int32_t dstride, const uint16_t *in, int32_t pri_strength, int32_t sec_strength, int32_t dir, int32_t pri_damping, int32_t sec_damping, int32_t bsize, int32_t coeff_shift, uint8_t subsampling_factor);
+    uint8_t svt_aom_cdef_find_dir_neon(const uint16_t *img, int32_t stride, int32_t *var, int32_t coeff_shift);
+    void svt_aom_cdef_find_dir_dual_neon(const uint16_t *img1, const uint16_t *img2, int stride, int32_t *var1, int32_t *var2, int32_t coeff_shift, uint8_t *out1, uint8_t *out2);
 
     void svt_aom_cfl_predict_lbd_neon(const int16_t *pred_buf_q3, uint8_t *pred, int32_t pred_stride, uint8_t *dst, int32_t dst_stride, int32_t alpha_q3, int32_t bit_depth, int32_t width, int32_t height);
 
@@ -1361,6 +1363,7 @@ extern "C" {
     void svt_aom_paeth_predictor_64x16_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
     void svt_aom_paeth_predictor_64x32_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
     void svt_aom_paeth_predictor_64x64_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
+
 #endif
 
 #ifdef ARCH_X86_64
diff --git a/test/CMakeLists.txt b/test/CMakeLists.txt
index 5b375dd9..c97174ee 100644
--- a/test/CMakeLists.txt
+++ b/test/CMakeLists.txt
@@ -87,12 +87,12 @@ set(multi_arch_files
     PictureOperatorTest.cc
     SadTest.cc
     selfguided_filter_test.cc
+    CdefTest.cc
     )
 
 if(HAVE_X86_PLATFORM)
     set(x86_arch_files
         AdaptiveScanTest.cc
-        CdefTest.cc
         CompoundUtilTest.cc
         DeblockTest.cc
         EbNoiseModel_test.cc
diff --git a/test/CdefTest.cc b/test/CdefTest.cc
index 52cdd53f..e4cb7706 100644
--- a/test/CdefTest.cc
+++ b/test/CdefTest.cc
@@ -54,10 +54,17 @@ typedef void (*svt_cdef_filter_block_8xn_16_func)(
 
 static const svt_cdef_filter_block_8xn_16_func
     svt_cdef_filter_block_8xn_16_func_table[] = {
+#if defined(ARCH_X86_64)
         svt_cdef_filter_block_8xn_16_avx2,
 #if EN_AVX512_SUPPORT
         svt_cdef_filter_block_8xn_16_avx512
 #endif
+#endif
+
+#if defined(ARCH_AARCH64)
+            NULL,  // No C version, only applicable for avx2 and avx512
+#endif
+
 };
 using cdef_dir_param_t =
     ::testing::tuple<CdefFilterBlockFunc, CdefFilterBlockFunc, BlockSize, int,
@@ -98,7 +105,9 @@ class CDEFBlockTest : public ::testing::TestWithParam<cdef_dir_param_t> {
         bsize_ = TEST_GET_PARAM(2);
         boundary_ = TEST_GET_PARAM(3);
         bd_ = TEST_GET_PARAM(4);
-        svt_cdef_filter_block_8xn_16 = TEST_GET_PARAM(5);
+        if (TEST_GET_PARAM(5) != NULL) {
+            svt_cdef_filter_block_8xn_16 = TEST_GET_PARAM(5);
+        }
 
         memset(dst_ref_, 0, sizeof(dst_ref_));
         memset(dst_tst_, 0, sizeof(dst_tst_));
@@ -364,6 +373,8 @@ TEST_P(CDEFBlockTest, DISABLED_SpeedTest) {
     speed_cdef();
 }
 
+#if defined(ARCH_X86_64)
+
 // VS compiling for 32 bit targets does not support vector types in
 // structs as arguments, which makes the v256 type of the intrinsics
 // hard to support, so optimizations for this target are disabled.
@@ -387,6 +398,19 @@ INSTANTIATE_TEST_CASE_P(
         ::testing::ValuesIn(svt_cdef_filter_block_8xn_16_func_table)));
 
 #endif  // defined(_WIN64) || !defined(_MSC_VER)
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+INSTANTIATE_TEST_CASE_P(
+    Cdef_neon, CDEFBlockTest,
+    ::testing::Combine(
+        ::testing::Values(&svt_cdef_filter_block_neon),
+        ::testing::Values(&svt_cdef_filter_block_c),
+        ::testing::Values(BLOCK_4X4, BLOCK_4X8, BLOCK_8X4, BLOCK_8X8),
+        ::testing::Range(0, 16), ::testing::Range(8, 13, 2),
+        ::testing::ValuesIn(svt_cdef_filter_block_8xn_16_func_table)));
+#endif  // defined(ARCH_AARCH64)
+
 using FindDirFunc = uint8_t (*)(const uint16_t *img, int stride, int32_t *var,
                                 int coeff_shift);
 using TestFindDirParam = ::testing::tuple<FindDirFunc, FindDirFunc>;
@@ -472,6 +496,8 @@ TEST_P(CDEFFindDirTest, MatchTest) {
     test_finddir();
 }
 
+#if defined(ARCH_X86_64)
+
 // VS compiling for 32 bit targets does not support vector types in
 // structs as arguments, which makes the v256 type of the intrinsics
 // hard to support, so optimizations for this target are disabled.
@@ -483,6 +509,17 @@ INSTANTIATE_TEST_CASE_P(
         make_tuple(&svt_aom_cdef_find_dir_avx2, &svt_aom_cdef_find_dir_c)));
 #endif  // defined(_WIN64) || !defined(_MSC_VER)
 
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+
+INSTANTIATE_TEST_CASE_P(
+    Cdef, CDEFFindDirTest,
+    ::testing::Values(make_tuple(&svt_aom_cdef_find_dir_neon,
+                                 &svt_aom_cdef_find_dir_c)));
+
+#endif
+
 using FindDirDualFunc = void (*)(const uint16_t *img1, const uint16_t *img2,
                                  int stride, int32_t *var1, int32_t *var2,
                                  int32_t coeff_shift, uint8_t *out1,
@@ -601,6 +638,8 @@ TEST_P(CDEFFindDirDualTest, MatchTest) {
     test_finddir();
 }
 
+#if defined(ARCH_X86_64)
+
 // VS compiling for 32 bit targets does not support vector types in
 // structs as arguments, which makes the v256 type of the intrinsics
 // hard to support, so optimizations for this target are disabled.
@@ -613,8 +652,22 @@ INSTANTIATE_TEST_CASE_P(
                                  &svt_aom_cdef_find_dir_dual_c)));
 
 #endif  // defined(_WIN64) || !defined(_MSC_VER)
+
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+
+INSTANTIATE_TEST_CASE_P(
+    Cdef, CDEFFindDirDualTest,
+    ::testing::Values(make_tuple(&svt_aom_cdef_find_dir_dual_neon,
+                                 &svt_aom_cdef_find_dir_dual_c)));
+
+#endif  // defined(ARCH_AARCH64)
+
 }  // namespace
 
+#if defined(ARCH_X86_64)
+
 /**
  * @brief Unit test for svt_aom_copy_rect8_8bit_to_16bit_avx2
  *
@@ -657,6 +710,9 @@ TEST(CdefToolTest, CopyRectMatchTest) {
 
             svt_aom_copy_rect8_8bit_to_16bit_c(
                 dst_ref_, CDEF_BSTRIDE, src_, CDEF_BSTRIDE, vsize, hsize);
+
+            // Test the SSE4.1 copy function
+            memset(dst_data_tst_, 0, sizeof(dst_data_tst_));
             svt_aom_copy_rect8_8bit_to_16bit_sse4_1(
                 dst_tst_, CDEF_BSTRIDE, src_, CDEF_BSTRIDE, vsize, hsize);
 
@@ -681,6 +737,9 @@ TEST(CdefToolTest, CopyRectMatchTest) {
             }
         }
 }
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_X86_64)
 
 /**
  * @brief Unit test for svt_aom_compute_cdef_dist_16bit_avx2
@@ -750,6 +809,7 @@ TEST(CdefToolTest, ComputeCdefDistMatchTest) {
                                                         coeff_shift,
                                                         plane,
                                                         subsampling);
+                        // SSE4.1
                         const uint64_t sse_mse =
                             svt_aom_compute_cdef_dist_16bit_sse4_1(dst_data_,
                                                                    stride,
@@ -760,6 +820,12 @@ TEST(CdefToolTest, ComputeCdefDistMatchTest) {
                                                                    coeff_shift,
                                                                    plane,
                                                                    subsampling);
+                        ASSERT_EQ(c_mse, sse_mse)
+                            << "svt_aom_compute_cdef_dist_16bit_sse4_1 failed "
+                            << "bitdepth: " << bd << " plane: " << plane
+                            << " BlockSize " << test_bs[i] << " loop: " << k;
+
+                        // AVX2
                         const uint64_t avx_mse =
                             svt_aom_compute_cdef_dist_16bit_avx2(dst_data_,
                                                                  stride,
@@ -770,10 +836,6 @@ TEST(CdefToolTest, ComputeCdefDistMatchTest) {
                                                                  coeff_shift,
                                                                  plane,
                                                                  subsampling);
-                        ASSERT_EQ(c_mse, sse_mse)
-                            << "svt_aom_compute_cdef_dist_16bit_sse4_1 failed "
-                            << "bitdepth: " << bd << " plane: " << plane
-                            << " BlockSize " << test_bs[i] << " loop: " << k;
                         ASSERT_EQ(c_mse, avx_mse)
                             << "svt_aom_compute_cdef_dist_16bit_avx2 failed "
                             << "bitdepth: " << bd << " plane: " << plane
@@ -784,6 +846,9 @@ TEST(CdefToolTest, ComputeCdefDistMatchTest) {
         }
     }
 }
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_X86_64)
 
 TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
     const int stride = 1 << MAX_SB_SIZE_LOG2;
@@ -835,6 +900,7 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
                                                              coeff_shift,
                                                              plane,
                                                              subsampling);
+                        // SSE4.1
                         const uint64_t sse_mse =
                             svt_aom_compute_cdef_dist_8bit_sse4_1(dst_data_,
                                                                   stride,
@@ -845,6 +911,12 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
                                                                   coeff_shift,
                                                                   plane,
                                                                   subsampling);
+                        ASSERT_EQ(c_mse, sse_mse)
+                            << "svt_aom_compute_cdef_dist_8bit_sse4_1 failed "
+                            << "bitdepth: " << bd << " plane: " << plane
+                            << " BlockSize " << test_bs[i] << " loop: " << k;
+
+                        // AVX2
                         const uint64_t avx_mse =
                             svt_aom_compute_cdef_dist_8bit_avx2(dst_data_,
                                                                 stride,
@@ -855,10 +927,6 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
                                                                 coeff_shift,
                                                                 plane,
                                                                 subsampling);
-                        ASSERT_EQ(c_mse, sse_mse)
-                            << "svt_aom_compute_cdef_dist_8bit_sse4_1 failed "
-                            << "bitdepth: " << bd << " plane: " << plane
-                            << " BlockSize " << test_bs[i] << " loop: " << k;
                         ASSERT_EQ(c_mse, avx_mse)
                             << "svt_aom_compute_cdef_dist_8bit_avx2 failed "
                             << "bitdepth: " << bd << " plane: " << plane
@@ -869,6 +937,7 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
         }
     }
 }
+#endif  // defined(ARCH_X86_64)
 
 /**
  * @brief Unit test for svt_search_one_dual_avx2
@@ -894,10 +963,12 @@ typedef uint64_t (*svt_search_one_dual_func)(int *lev0, int *lev1,
                                              int start_gi, int end_gi);
 
 static const svt_search_one_dual_func search_one_dual_func_table[] = {
+#if defined(ARCH_X86_64)
     svt_search_one_dual_avx2,
 #if EN_AVX512_SUPPORT
     svt_search_one_dual_avx512
 #endif
+#endif
 };
 
 TEST(CdefToolTest, SearchOneDualMatchTest) {
-- 
2.39.3 (Apple Git-146)

