From 36ce6915f01c2a970bcd9a37036d993886462aa1 Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Wed, 3 Apr 2024 15:39:26 +0000
Subject: [PATCH 01/13] NEON port of svt_av1_highbd_convolve_2d_sr_c

---
 Source/Lib/Common/ASM_NEON/CMakeLists.txt     |   1 +
 Source/Lib/Common/ASM_NEON/convolve_neon.h    |   4 +-
 .../Common/ASM_NEON/highbd_convolve_2d_neon.c | 424 ++++++++++++++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   2 +
 test/convolve_2d_test.cc                      |  59 ++-
 6 files changed, 479 insertions(+), 13 deletions(-)

diff --git a/Source/Lib/Common/ASM_NEON/CMakeLists.txt b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
index ba6f9dbf6..380f8fb4c 100644
--- a/Source/Lib/Common/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
@@ -25,6 +25,7 @@ target_sources(
   PUBLIC compound_convolve_neon.c
   PUBLIC convolve_2d_neon.c
   PUBLIC convolve_neon.c
+  PUBLIC highbd_convolve_2d_neon.c
   PUBLIC highbd_inv_txfm_neon.c
   PUBLIC EbBlend_a64_mask_neon.c
   PUBLIC EbDeblockingFilter_Intrinsic_neon.c
diff --git a/Source/Lib/Common/ASM_NEON/convolve_neon.h b/Source/Lib/Common/ASM_NEON/convolve_neon.h
index 6c5fef83b..0a9a4d42b 100644
--- a/Source/Lib/Common/ASM_NEON/convolve_neon.h
+++ b/Source/Lib/Common/ASM_NEON/convolve_neon.h
@@ -12,6 +12,8 @@
 #ifndef AOM_AV1_COMMON_ARM_CONVOLVE_NEON_H_
 #define AOM_AV1_COMMON_ARM_CONVOLVE_NEON_H_
 
+#include "convolve.h"
+#include "EbDefinitions.h"
 #include "EbInterPrediction.h"
 
 static INLINE Bool is_convolve_2tap(const int16_t *const filter) {
@@ -39,4 +41,4 @@ static INLINE int32_t get_convolve_tap(const int16_t *const filter) {
         return 8;
 }
 
-#endif
+#endif // AOM_AV1_COMMON_ARM_CONVOLVE_NEON_H_
diff --git a/Source/Lib/Common/ASM_NEON/highbd_convolve_2d_neon.c b/Source/Lib/Common/ASM_NEON/highbd_convolve_2d_neon.c
index 63721782c..7cf130a0c 100644
--- a/Source/Lib/Common/ASM_NEON/highbd_convolve_2d_neon.c
+++ b/Source/Lib/Common/ASM_NEON/highbd_convolve_2d_neon.c
@@ -275,3 +275,427 @@ void svt_av1_highbd_jnt_convolve_2d_neon(const uint16_t *src, int32_t src_stride
         }
     }
 }
+
+static INLINE void svt_prepare_coeffs_12tap(const int16_t *const filter, int16x8_t *coeffs /* [6] */) {
+    int32x4_t coeffs_y  = vld1q_s32((int32_t const *)filter);
+    int32x4_t coeffs_y2 = vld1q_s32((int32_t const *)(filter + 8));
+
+    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 0))); // coeffs 0 1 0 1 0 1 0 1
+    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 1))); // coeffs 2 3 2 3 2 3 2 3
+    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 2))); // coeffs 4 5 4 5 4 5 4 5
+    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 3))); // coeffs 6 7 6 7 6 7 6 7
+
+    coeffs[4] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y2, 0))); // coeffs 8 9 8 9 8 9 8 9
+    coeffs[5] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y2, 1))); // coeffs 10 11 10 11 10 11 10 11
+}
+
+static INLINE int32x4_t convolve_12tap(const int16x8_t *s, const int16x8_t *coeffs) {
+    const int32x4_t d0     = vmull_s16(vget_low_s16(s[0]), vget_low_s16(coeffs[0]));
+    const int32x4_t d1     = vmull_s16(vget_low_s16(s[1]), vget_low_s16(coeffs[1]));
+    const int32x4_t d2     = vmull_s16(vget_low_s16(s[2]), vget_low_s16(coeffs[2]));
+    const int32x4_t d3     = vmull_s16(vget_low_s16(s[3]), vget_low_s16(coeffs[3]));
+    const int32x4_t d4     = vmull_s16(vget_low_s16(s[4]), vget_low_s16(coeffs[4]));
+    const int32x4_t d5     = vmull_s16(vget_low_s16(s[5]), vget_low_s16(coeffs[5]));
+    const int32x4_t d_0123 = vaddq_s32(vaddq_s32(d0, d1), vaddq_s32(d2, d3));
+    const int32x4_t d      = vaddq_s32(vaddq_s32(d4, d5), d_0123);
+    return d;
+}
+
+static INLINE void prepare_coeffs(const int16_t *const filter, int16x8_t *const coeffs /* [4] */) {
+    const int16x8_t coeff = vld1q_s16(filter);
+
+    // coeffs 0 1 0 1 0 1 0 1
+    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 0)));
+    // coeffs 2 3 2 3 2 3 2 3
+    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 1)));
+    // coeffs 4 5 4 5 4 5 4 5
+    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 2)));
+    // coeffs 6 7 6 7 6 7 6 7
+    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 3)));
+}
+
+static INLINE int32x4_t svt_aom_convolve(const int16x8_t *const s, const int16x8_t *const coeffs) {
+    const int32x4_t res_0 = vpaddq_s32(vmulq_s32(vmovl_s16(vget_low_s16(s[0])), vmovl_s16(vget_low_s16(coeffs[0]))),
+                                       vmulq_s32(vmovl_s16(vget_high_s16(s[0])), vmovl_s16(vget_high_s16(coeffs[0]))));
+    const int32x4_t res_1 = vpaddq_s32(vmulq_s32(vmovl_s16(vget_low_s16(s[1])), vmovl_s16(vget_low_s16(coeffs[1]))),
+                                       vmulq_s32(vmovl_s16(vget_high_s16(s[1])), vmovl_s16(vget_high_s16(coeffs[1]))));
+    const int32x4_t res_2 = vpaddq_s32(vmulq_s32(vmovl_s16(vget_low_s16(s[2])), vmovl_s16(vget_low_s16(coeffs[2]))),
+                                       vmulq_s32(vmovl_s16(vget_high_s16(s[2])), vmovl_s16(vget_high_s16(coeffs[2]))));
+    const int32x4_t res_3 = vpaddq_s32(vmulq_s32(vmovl_s16(vget_low_s16(s[3])), vmovl_s16(vget_low_s16(coeffs[3]))),
+                                       vmulq_s32(vmovl_s16(vget_high_s16(s[3])), vmovl_s16(vget_high_s16(coeffs[3]))));
+
+    const int32x4_t res = vaddq_s32(vaddq_s32(res_0, res_1), vaddq_s32(res_2, res_3));
+
+    return res;
+}
+
+void svt_av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride,
+                                        int32_t w, int32_t h, const InterpFilterParams *filter_params_x,
+                                        const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
+                                        const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd) {
+    DECLARE_ALIGNED(32, int16_t, im_block[(MAX_SB_SIZE + MAX_FILTER_TAP) * 8]);
+    int                   im_h      = h + filter_params_y->taps - 1;
+    int                   im_stride = 8;
+    int                   i, j;
+    const int             fo_vert  = filter_params_y->taps / 2 - 1;
+    const int             fo_horiz = filter_params_x->taps / 2 - 1;
+    const uint16_t *const src_ptr  = src - fo_vert * src_stride - fo_horiz;
+
+    // Check that, even with 12-bit input, the intermediate values will fit
+    // into an unsigned 16-bit intermediate array.
+    assert(bd + FILTER_BITS + 2 - conv_params->round_0 <= 16);
+
+    const int32x4_t round_const_x = vdupq_n_s32(((1 << conv_params->round_0) >> 1) + (1 << (bd + FILTER_BITS - 1)));
+    const int32x4_t round_shift_x = vsetq_lane_s32(conv_params->round_0, vdupq_n_s32(0), 0);
+
+    const int32x4_t round_const_y = vdupq_n_s32(((1 << conv_params->round_1) >> 1) -
+                                                (1 << (bd + 2 * FILTER_BITS - conv_params->round_0 - 1)));
+    const int32x4_t round_shift_y = vsetq_lane_s32(conv_params->round_1, vdupq_n_s32(0), 0);
+
+    const int       bits             = FILTER_BITS * 2 - conv_params->round_0 - conv_params->round_1;
+    const int32x4_t round_shift_bits = vsetq_lane_s32(bits, vdupq_n_s32(0), 0);
+    const int32x4_t round_const_bits = vdupq_n_s32((1 << bits) >> 1);
+    const int16x8_t clip_pixel       = vdupq_n_s16(bd == 10 ? 1023 : (bd == 12 ? 4095 : 255));
+    const int16x8_t zero             = vdupq_n_s16(0);
+
+    const int16_t *const x_filter = av1_get_interp_filter_subpel_kernel(*filter_params_x, subpel_x_q4 & SUBPEL_MASK);
+    const int16_t *const y_filter = av1_get_interp_filter_subpel_kernel(*filter_params_y, subpel_y_q4 & SUBPEL_MASK);
+
+    if (filter_params_x->taps == 12) {
+        int16x8_t coeffs_x[6], coeffs_y[6], s[24];
+        svt_prepare_coeffs_12tap(x_filter, coeffs_x);
+        svt_prepare_coeffs_12tap(y_filter, coeffs_y);
+
+        for (j = 0; j < w; j += 8) {
+            /* Horizontal filter */
+            {
+                for (i = 0; i < im_h; i += 1) {
+                    const int16x8_t row00 = vld1q_s16((const int16_t *)&src_ptr[i * src_stride + j]);
+                    const int16x8_t row01 = vld1q_s16((const int16_t *)&src_ptr[i * src_stride + (j + 8)]);
+                    const int16x8_t row02 = vld1q_s16((const int16_t *)&src_ptr[i * src_stride + (j + 16)]);
+
+                    // even pixels
+                    s[0] = vextq_s16(row00, row01, 0);
+                    s[1] = vextq_s16(row00, row01, 2);
+                    s[2] = vextq_s16(row00, row01, 4);
+                    s[3] = vextq_s16(row00, row01, 6);
+                    s[4] = vextq_s16(row01, row02, 0);
+                    s[5] = vextq_s16(row01, row02, 2);
+
+                    int32x4_t res_even = convolve_12tap(s, coeffs_x);
+
+                    res_even = vshlq_s32(vaddq_s32(res_even, round_const_x), -round_shift_x);
+
+                    // odd pixels
+                    s[0] = vextq_s16(row00, row01, 1);
+                    s[1] = vextq_s16(row00, row01, 4);
+                    s[2] = vextq_s16(row00, row01, 5);
+                    s[3] = vextq_s16(row00, row01, 7);
+                    s[4] = vextq_s16(row01, row02, 1);
+                    s[5] = vextq_s16(row01, row02, 3);
+
+                    int32x4_t res_odd = convolve_12tap(s, coeffs_x);
+                    res_odd           = vshlq_s32(vaddq_s32(res_odd, round_const_x), -round_shift_x);
+
+                    int16x8_t res_even1 = vqmovn_high_s32(vqmovn_s32(res_even), res_even);
+                    int16x8_t res_odd1  = vqmovn_high_s32(vqmovn_s32(res_odd), res_odd);
+                    int16x8_t res       = vzip1q_s16(res_even1, res_odd1);
+
+                    vst1q_s16(&im_block[i * im_stride], res);
+                }
+            }
+
+            /* Vertical filter */
+            {
+                int16x8_t s0  = vld1q_s16(im_block + 0 * im_stride);
+                int16x8_t s1  = vld1q_s16(im_block + 1 * im_stride);
+                int16x8_t s2  = vld1q_s16(im_block + 2 * im_stride);
+                int16x8_t s3  = vld1q_s16(im_block + 3 * im_stride);
+                int16x8_t s4  = vld1q_s16(im_block + 4 * im_stride);
+                int16x8_t s5  = vld1q_s16(im_block + 5 * im_stride);
+                int16x8_t s6  = vld1q_s16(im_block + 6 * im_stride);
+                int16x8_t s7  = vld1q_s16(im_block + 7 * im_stride);
+                int16x8_t s8  = vld1q_s16(im_block + 8 * im_stride);
+                int16x8_t s9  = vld1q_s16(im_block + 9 * im_stride);
+                int16x8_t s10 = vld1q_s16(im_block + 10 * im_stride);
+
+                s[0] = vzip1q_s16(s0, s1);
+                s[1] = vzip1q_s16(s2, s3);
+                s[2] = vzip1q_s16(s4, s5);
+                s[3] = vzip1q_s16(s6, s7);
+                s[4] = vzip1q_s16(s8, s9);
+
+                s[6]  = vzip2q_s16(s0, s1);
+                s[7]  = vzip2q_s16(s2, s3);
+                s[8]  = vzip2q_s16(s4, s5);
+                s[9]  = vzip2q_s16(s6, s7);
+                s[10] = vzip2q_s16(s8, s9);
+
+                s[12] = vzip1q_s16(s1, s2);
+                s[13] = vzip1q_s16(s3, s4);
+                s[14] = vzip1q_s16(s5, s6);
+                s[15] = vzip1q_s16(s7, s8);
+                s[16] = vzip1q_s16(s9, s10);
+
+                s[18] = vzip2q_s16(s1, s2);
+                s[19] = vzip2q_s16(s3, s4);
+                s[20] = vzip2q_s16(s5, s6);
+                s[21] = vzip2q_s16(s7, s8);
+                s[22] = vzip2q_s16(s9, s10);
+
+                for (i = 0; i < h; i += 2) {
+                    const int16_t *data = &im_block[i * im_stride];
+
+                    int16x8_t s11 = vld1q_s16(data + 11 * im_stride);
+                    int16x8_t s12 = vld1q_s16(data + 12 * im_stride);
+
+                    s[5]  = vzip1q_s16(s10, s11);
+                    s[11] = vzip2q_s16(s10, s11);
+
+                    s[17] = vzip1q_s16(s11, s12);
+                    s[23] = vzip2q_s16(s11, s12);
+
+                    const int32x4_t res_a0       = convolve_12tap(s, coeffs_y);
+                    int32x4_t       res_a_round0 = vshlq_s32(vaddq_s32(res_a0, round_const_y), -round_shift_y);
+                    res_a_round0 = vshlq_s32(vaddq_s32(res_a_round0, round_const_bits), -round_shift_bits);
+
+                    const int32x4_t res_a1       = convolve_12tap(s + 12, coeffs_y);
+                    int32x4_t       res_a_round1 = vshlq_s32(vaddq_s32(res_a1, round_const_y), -round_shift_y);
+                    res_a_round1 = vshlq_s32(vaddq_s32(res_a_round1, round_const_bits), -round_shift_bits);
+
+                    if (w - j > 4) {
+                        const int32x4_t res_b0       = convolve_12tap(s + 6, coeffs_y);
+                        int32x4_t       res_b_round0 = vshlq_s32(vaddq_s32(res_b0, round_const_y), -round_shift_y);
+                        res_b_round0 = vshlq_s32(vaddq_s32(res_b_round0, round_const_bits), -round_shift_bits);
+
+                        const int32x4_t res_b1       = convolve_12tap(s + 18, coeffs_y);
+                        int32x4_t       res_b_round1 = vshlq_s32(vaddq_s32(res_b1, round_const_y), -round_shift_y);
+                        res_b_round1 = vshlq_s32(vaddq_s32(res_b_round1, round_const_bits), -round_shift_bits);
+
+                        int16x8_t res_16bit0 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_b_round0);
+                        res_16bit0           = vminq_s16(res_16bit0, clip_pixel);
+                        res_16bit0           = vmaxq_s16(res_16bit0, zero);
+
+                        int16x8_t res_16bit1 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_b_round1);
+                        res_16bit1           = vminq_s16(res_16bit1, clip_pixel);
+                        res_16bit1           = vmaxq_s16(res_16bit1, zero);
+
+                        vst1q_s16((int16_t *)&dst[i * dst_stride + j], res_16bit0);
+                        vst1q_s16((int16_t *)&dst[i * dst_stride + j + dst_stride], res_16bit1);
+                    } else if (w == 4) {
+                        int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                        res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                        res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                        int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                        res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                        res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                        vst1_s64((int64_t *)&dst[i * dst_stride + j],
+                                 vreinterpret_s64_s16(vget_low_s16(res_a_round0_s16)));
+                        vst1_s64((int64_t *)&dst[i * dst_stride + j + dst_stride],
+                                 vreinterpret_s64_s16(vget_low_s16(res_a_round1_s16)));
+                    } else {
+                        int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                        res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                        res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                        int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                        res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                        res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                        *((uint32_t *)(&dst[i * dst_stride + j])) = vgetq_lane_s32(
+                            vreinterpretq_s32_s16(res_a_round0_s16), 0);
+
+                        *((uint32_t *)(&dst[i * dst_stride + j + dst_stride])) = vgetq_lane_s32(
+                            vreinterpretq_s32_s16(res_a_round1_s16), 0);
+                    }
+                    s[0] = s[1];
+                    s[1] = s[2];
+                    s[2] = s[3];
+                    s[3] = s[4];
+                    s[4] = s[5];
+
+                    s[6]  = s[7];
+                    s[7]  = s[8];
+                    s[8]  = s[9];
+                    s[9]  = s[10];
+                    s[10] = s[11];
+
+                    s[12] = s[13];
+                    s[13] = s[14];
+                    s[14] = s[15];
+                    s[15] = s[16];
+                    s[16] = s[17];
+
+                    s[18] = s[19];
+                    s[19] = s[20];
+                    s[20] = s[21];
+                    s[21] = s[22];
+                    s[22] = s[23];
+
+                    s10 = s12;
+                }
+            }
+        }
+    } else {
+        int16x8_t coeffs_x[4], coeffs_y[4], s[16];
+        prepare_coeffs(x_filter, coeffs_x);
+        prepare_coeffs(y_filter, coeffs_y);
+
+        for (j = 0; j < w; j += 8) {
+            /* Horizontal filter */
+            {
+                for (i = 0; i < im_h; i += 1) {
+                    const int16x8_t row00 = vld1q_s16((const int16_t *)&src_ptr[i * src_stride + j]);
+                    const int16x8_t row01 = vld1q_s16((const int16_t *)&src_ptr[i * src_stride + (j + 8)]);
+
+                    // even pixels
+                    s[0] = vextq_s16(row00, row01, 0);
+                    s[1] = vextq_s16(row00, row01, 2);
+                    s[2] = vextq_s16(row00, row01, 4);
+                    s[3] = vextq_s16(row00, row01, 6);
+
+                    int32x4_t res_even = svt_aom_convolve(s, coeffs_x);
+                    res_even           = vshlq_s32(vaddq_s32(res_even, round_const_x), vdupq_n_s32(-round_shift_x[0]));
+
+                    // odd pixels
+                    s[0] = vextq_s16(row00, row01, 1);
+                    s[1] = vextq_s16(row00, row01, 3);
+                    s[2] = vextq_s16(row00, row01, 5);
+                    s[3] = vextq_s16(row00, row01, 7);
+
+                    int32x4_t res_odd = svt_aom_convolve(s, coeffs_x);
+                    res_odd           = vshlq_s32(vaddq_s32(res_odd, round_const_x), vdupq_n_s32(-round_shift_x[0]));
+
+                    int16x8_t res_even1 = vqmovn_high_s32(vqmovn_s32(res_even), res_even);
+                    int16x8_t res_odd1  = vqmovn_high_s32(vqmovn_s32(res_odd), res_odd);
+                    int16x8_t res       = vzip1q_s16(res_even1, res_odd1);
+
+                    vst1q_s16(&im_block[i * im_stride], res);
+                }
+            }
+
+            /* Vertical filter */
+            {
+                int16x8_t s0 = vld1q_s16(im_block + 0 * im_stride);
+                int16x8_t s1 = vld1q_s16(im_block + 1 * im_stride);
+                int16x8_t s2 = vld1q_s16(im_block + 2 * im_stride);
+                int16x8_t s3 = vld1q_s16(im_block + 3 * im_stride);
+                int16x8_t s4 = vld1q_s16(im_block + 4 * im_stride);
+                int16x8_t s5 = vld1q_s16(im_block + 5 * im_stride);
+                int16x8_t s6 = vld1q_s16(im_block + 6 * im_stride);
+
+                s[0] = vzip1q_s16(s0, s1);
+                s[1] = vzip1q_s16(s2, s3);
+                s[2] = vzip1q_s16(s4, s5);
+
+                s[4] = vzip2q_s16(s0, s1);
+                s[5] = vzip2q_s16(s2, s3);
+                s[6] = vzip2q_s16(s4, s5);
+
+                s[0 + 8] = vzip1q_s16(s1, s2);
+                s[1 + 8] = vzip1q_s16(s3, s4);
+                s[2 + 8] = vzip1q_s16(s5, s6);
+
+                s[4 + 8] = vzip2q_s16(s1, s2);
+                s[5 + 8] = vzip2q_s16(s3, s4);
+                s[6 + 8] = vzip2q_s16(s5, s6);
+
+                for (i = 0; i < h; i += 2) {
+                    const int16_t *data = &im_block[i * im_stride];
+
+                    int16x8_t s7 = vld1q_s16(data + 7 * im_stride);
+                    int16x8_t s8 = vld1q_s16(data + 8 * im_stride);
+
+                    s[3] = vzip1q_s16(s6, s7);
+                    s[7] = vzip2q_s16(s6, s7);
+
+                    s[3 + 8] = vzip1q_s16(s7, s8);
+                    s[7 + 8] = vzip2q_s16(s7, s8);
+
+                    const int32x4_t res_a0       = svt_aom_convolve(s, coeffs_y);
+                    int32x4_t       res_a_round0 = vshlq_s32(vaddq_s32(res_a0, round_const_y),
+                                                       vdupq_n_s32(-round_shift_y[0]));
+                    res_a_round0                 = vshlq_s32(vaddq_s32(res_a_round0, round_const_bits),
+                                             vdupq_n_s32(-round_shift_bits[0]));
+
+                    const int32x4_t res_a1       = svt_aom_convolve(s + 8, coeffs_y);
+                    int32x4_t       res_a_round1 = vshlq_s32(vaddq_s32(res_a1, round_const_y),
+                                                       vdupq_n_s32(-round_shift_y[0]));
+                    res_a_round1                 = vshlq_s32(vaddq_s32(res_a_round1, round_const_bits),
+                                             vdupq_n_s32(-round_shift_bits[0]));
+
+                    if (w - j > 4) {
+                        const int32x4_t res_b0       = svt_aom_convolve(s + 4, coeffs_y);
+                        int32x4_t       res_b_round0 = vshlq_s32(vaddq_s32(res_b0, round_const_y),
+                                                           vdupq_n_s32(-round_shift_y[0]));
+                        res_b_round0                 = vshlq_s32(vaddq_s32(res_b_round0, round_const_bits),
+                                                 vdupq_n_s32(-round_shift_bits[0]));
+
+                        const int32x4_t res_b1       = svt_aom_convolve(s + 4 + 8, coeffs_y);
+                        int32x4_t       res_b_round1 = vshlq_s32(vaddq_s32(res_b1, round_const_y),
+                                                           vdupq_n_s32(-round_shift_y[0]));
+                        res_b_round1                 = vshlq_s32(vaddq_s32(res_b_round1, round_const_bits),
+                                                 vdupq_n_s32(-round_shift_bits[0]));
+
+                        int16x8_t res_16bit0 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_b_round0);
+                        res_16bit0           = vminq_s16(res_16bit0, clip_pixel);
+                        res_16bit0           = vmaxq_s16(res_16bit0, zero);
+
+                        int16x8_t res_16bit1 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_b_round1);
+                        res_16bit1           = vminq_s16(res_16bit1, clip_pixel);
+                        res_16bit1           = vmaxq_s16(res_16bit1, zero);
+
+                        vst1q_s16((int16_t *)&dst[i * dst_stride + j], res_16bit0);
+                        vst1q_s16((int16_t *)&dst[i * dst_stride + j + dst_stride], res_16bit1);
+                    } else if (w == 4) {
+                        int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                        res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                        res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                        int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                        res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                        res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                        vst1_s64((int64_t *)&dst[i * dst_stride + j],
+                                 vreinterpret_s64_s16(vget_low_s16(res_a_round0_s16)));
+                        vst1_s64((int64_t *)&dst[i * dst_stride + j + dst_stride],
+                                 vreinterpret_s64_s16(vget_low_s16(res_a_round1_s16)));
+                    } else {
+                        int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                        res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                        res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                        int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                        res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                        res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                        *((uint32_t *)(&dst[i * dst_stride + j])) = vgetq_lane_s32(
+                            vreinterpretq_s32_s16(res_a_round0_s16), 0);
+
+                        *((uint32_t *)(&dst[i * dst_stride + j + dst_stride])) = vgetq_lane_s32(
+                            vreinterpretq_s32_s16(res_a_round1_s16), 0);
+                    }
+                    s[0] = s[1];
+                    s[1] = s[2];
+                    s[2] = s[3];
+
+                    s[4] = s[5];
+                    s[5] = s[6];
+                    s[6] = s[7];
+
+                    s[0 + 8] = s[1 + 8];
+                    s[1 + 8] = s[2 + 8];
+                    s[2 + 8] = s[3 + 8];
+
+                    s[4 + 8] = s[5 + 8];
+                    s[5 + 8] = s[6 + 8];
+                    s[6 + 8] = s[7 + 8];
+
+                    s6 = s8;
+                }
+            }
+        }
+    }
+}
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 4ce045387..16e13849c 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -888,7 +888,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_av1_wiener_convolve_add_src, svt_av1_wiener_convolve_add_src_c, svt_av1_wiener_convolve_add_src_neon);
     SET_ONLY_C(svt_av1_convolve_2d_scale, svt_av1_convolve_2d_scale_c);
     SET_ONLY_C(svt_av1_highbd_convolve_y_sr, svt_av1_highbd_convolve_y_sr_c);
-    SET_ONLY_C(svt_av1_highbd_convolve_2d_sr, svt_av1_highbd_convolve_2d_sr_c);
+    SET_NEON(svt_av1_highbd_convolve_2d_sr, svt_av1_highbd_convolve_2d_sr_c, svt_av1_highbd_convolve_2d_sr_neon);
     SET_ONLY_C(svt_av1_highbd_convolve_2d_scale, svt_av1_highbd_convolve_2d_scale_c);
     SET_ONLY_C(svt_av1_highbd_convolve_2d_copy_sr, svt_av1_highbd_convolve_2d_copy_sr_c);
     SET_NEON(svt_av1_highbd_jnt_convolve_2d, svt_av1_highbd_jnt_convolve_2d_c, svt_av1_highbd_jnt_convolve_2d_neon);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index 3202cdc49..2f9dc8e67 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1375,6 +1375,8 @@ extern "C" {
                                            const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
                                            const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
+    void svt_av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
+
 #endif
 
 #ifdef ARCH_X86_64
diff --git a/test/convolve_2d_test.cc b/test/convolve_2d_test.cc
index 417d332b1..d6e5b0ef7 100644
--- a/test/convolve_2d_test.cc
+++ b/test/convolve_2d_test.cc
@@ -1117,8 +1117,6 @@ INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTest2D, AV1HbdJntConvolve2DTest,
 
 #endif  // ARCH_AARCH64
 
-#ifdef ARCH_X86_64
-
 class AV1HbdSrConvolve2DTest : public AV1HbdConvolve2DTest {
   public:
     AV1HbdSrConvolve2DTest() {
@@ -1127,6 +1125,9 @@ class AV1HbdSrConvolve2DTest : public AV1HbdConvolve2DTest {
         const int has_subx = TEST_GET_PARAM(1);
         const int has_suby = TEST_GET_PARAM(2);
         const int fn_idx = TEST_GET_PARAM(3);
+
+#if defined(ARCH_X86_64)
+
         if (fn_idx == 0) {  // avx2
             if (has_subx == 1 && has_suby == 1)
                 func_tst_ = svt_av1_highbd_convolve_2d_sr_avx2;
@@ -1136,7 +1137,7 @@ class AV1HbdSrConvolve2DTest : public AV1HbdConvolve2DTest {
                 func_tst_ = svt_av1_highbd_convolve_y_sr_avx2;
             else
                 func_tst_ = svt_av1_highbd_convolve_2d_copy_sr_avx2;
-        } else {  // SSE
+        } else if (fn_idx == 1) {  // SSE
             if (has_subx == 1 && has_suby == 1)
                 func_tst_ = svt_av1_highbd_convolve_2d_sr_ssse3;
             else if (has_subx == 1)
@@ -1146,6 +1147,24 @@ class AV1HbdSrConvolve2DTest : public AV1HbdConvolve2DTest {
             else
                 func_tst_ = svt_av1_highbd_convolve_2d_copy_sr_ssse3;
         }
+
+#endif  // ARCH_X86_64
+
+#if defined(ARCH_AARCH64)
+
+        if (fn_idx == 2) {  // neon
+            if (has_subx == 1 && has_suby == 1)
+                func_tst_ = svt_av1_highbd_convolve_2d_sr_neon;
+            // else if (has_subx == 1)                      Not yet implemented
+            //     func_tst_ = svt_av1_highbd_convolve_x_sr_neon;
+            // else if (has_suby == 1)                      Not yet implemented
+            //     func_tst_ = svt_av1_highbd_convolve_y_sr_neon;
+            // else                                         Not yet implemented
+            //     func_tst_ = svt_av1_highbd_convolve_2d_copy_sr_neon;
+        }
+
+#endif  // ARCH_AARCH64
+
         bd_ = TEST_GET_PARAM(0);
     }
     virtual ~AV1HbdSrConvolve2DTest() {
@@ -1160,14 +1179,7 @@ TEST_P(AV1HbdSrConvolve2DTest, DISABLED_SpeedTest) {
     speed_test();
 }
 
-INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTestX, AV1HbdSrConvolve2DTest,
-                         BuildParams(1, 0, 1, 1));
-INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTest2D, AV1HbdSrConvolve2DTest,
-                         BuildParams(1, 1, 1, 1));
-INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTestY, AV1HbdSrConvolve2DTest,
-                         BuildParams(0, 1, 1, 1));
-INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
-                         BuildParams(0, 0, 1, 1));
+#if defined(ARCH_X86_64)
 
 INSTANTIATE_TEST_SUITE_P(ConvolveTestX, AV1HbdSrConvolve2DTest,
                          BuildParams(1, 0, 0, 1));
@@ -1178,6 +1190,31 @@ INSTANTIATE_TEST_SUITE_P(ConvolveTestY, AV1HbdSrConvolve2DTest,
 INSTANTIATE_TEST_SUITE_P(ConvolveTestCopy, AV1HbdSrConvolve2DTest,
                          BuildParams(0, 0, 0, 1));
 
+INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestX, AV1HbdSrConvolve2DTest,
+                        BuildParams(1, 0, 1, 1));
+INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTest2D, AV1HbdSrConvolve2DTest,
+                        BuildParams(1, 1, 1, 1));
+INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestY, AV1HbdSrConvolve2DTest,
+                        BuildParams(0, 1, 1, 1));
+INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
+                        BuildParams(0, 0, 1, 1));
+
 #endif  // ARCH_X86_64
 
+#if defined(ARCH_AARCH64)
+
+// INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestX, AV1HbdSrConvolve2DTest,
+//                         BuildParams(1, 0, 2, 1));            Not yet
+//                         implemented
+INSTANTIATE_TEST_CASE_P(NEON_ConvolveTest2D, AV1HbdSrConvolve2DTest,
+                        BuildParams(1, 1, 2, 1));
+// INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdSrConvolve2DTest,
+//                         BuildParams(0, 1, 2, 1));            Not yet
+//                         implemented
+// INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
+//                         BuildParams(0, 0, 2, 1));            Not yet
+//                         implemented
+
+#endif  // ARCH_AARCH64
+
 }  // namespace
-- 
GitLab


From 68e74530965ba6b76a2accf7c93ab57886d76ab6 Mon Sep 17 00:00:00 2001
From: Gerardo Puga <glpuga@gmail.com>
Date: Mon, 15 Apr 2024 19:21:03 +0000
Subject: [PATCH 02/13] Add port of unpack_and_2bcompress

---
 .../EbPictureOperators_Intrinsic_neon.c       | 595 ++++++++++++++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   4 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   7 +
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.c       |   2 +-
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.h       |   4 +
 test/CMakeLists.txt                           |  97 +--
 test/PackUnPackTest.cc                        | 164 ++++-
 7 files changed, 802 insertions(+), 71 deletions(-)

diff --git a/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c b/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
index 261e13140..73d1fbe93 100644
--- a/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
+++ b/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
@@ -11,6 +11,7 @@
 
 #include <arm_neon.h>
 #include "EbDefinitions.h"
+#include "EbPackUnPack.h"
 #include "mem_neon.h"
 #include "transpose_neon.h"
 
@@ -335,3 +336,597 @@ void svt_full_distortion_kernel32_bits_neon(int32_t *coeff, uint32_t coeff_strid
 
     vst1q_s64((int64_t *)distortion_result, tmp3);
 }
+
+static INLINE void unpack_and_2bcompress_32_neon(uint16_t *in16b_buffer, uint8_t *out8b_buffer, uint8_t *out2b_buffer,
+                                                 uint32_t width_rep) {
+    const uint16x8_t ymm_00ff = vdupq_n_u16(0x00FF);
+    const uint16x8_t msk_2b   = vdupq_n_u16(0x0003); //0000.0000.0000.0011
+
+    const uint32x4_t msk0 = vdupq_n_u32(0x000000C0); //1100.0000
+    const uint32x4_t msk1 = vdupq_n_u32(0x00000030); //0011.0000
+    const uint32x4_t msk2 = vdupq_n_u32(0x0000000C); //0000.1100
+
+    for (uint32_t w = 0; w < width_rep; w++) {
+        const uint16x8_t in1 = vld1q_u16(in16b_buffer + w * 16);
+        const uint16x8_t in2 = vld1q_u16(in16b_buffer + w * 16 + 8);
+
+        const uint16x8_t tmp_2b1 = vandq_u16(in1, msk_2b); //0000.0011.1111.1111 -> 0000.0000.0000.0011
+        const uint16x8_t tmp_2b2 = vandq_u16(in2, msk_2b);
+        const uint32x4_t tmp_2b  = vreinterpretq_u32_u8(vcombine_u8(vqmovn_u16(tmp_2b1), vqmovn_u16(tmp_2b2)));
+
+        const uint32x4_t ext0 = vshrq_n_u32(
+            tmp_2b,
+            24); //0000.0011.0000.0000.0000.0000.0000.0000 -> 0000.0000.0000.0000.0000.0000.0000.0011
+        const uint32x4_t ext1 = vandq_u32(
+            vshrq_n_u32(tmp_2b, 14),
+            msk2); //0000.0000.0000.0011.0000.0000.0000.0000 -> 0000.0000.0000.0000.0000.0000.0000.1100
+        const uint32x4_t ext2 = vandq_u32(
+            vshrq_n_u32(tmp_2b, 4),
+            msk1); //0000.0000.0000.0000.0000.0011.0000.0000 -> 0000.0000.0000.0000.0000.0000.0011.0000
+        const uint32x4_t ext3 = vandq_u32(
+            vshlq_n_u32(tmp_2b, 6),
+            msk0); //0000.0000.0000.0000.0000.0000.0000.0011 -> 0000.0000.0000.0000.0000.0000.1100.0000
+
+        const uint32x4_t ext0123 = vorrq_u32(vorrq_u32(ext0, ext1),
+                                             vorrq_u32(ext2, ext3)); //0000.0000.0000.0000.0000.0000.1111.1111
+
+        const uint32_t ext0123_packed32 = vget_lane_u32(
+            vreinterpret_u32_u8(vqmovn_u16(vcombine_u16(vqmovn_u32(ext0123), vdup_n_u16(0)))), 0);
+        *((uint32_t *)(out2b_buffer + w * 4)) = ext0123_packed32;
+
+        const uint8x16_t out8_u8 = vcombine_u8(vqmovn_u16(vandq_u16(vshrq_n_u16(in1, 2), ymm_00ff)),
+                                               vqmovn_u16(vandq_u16(vshrq_n_u16(in2, 2), ymm_00ff)));
+
+        vst1q_u8(out8b_buffer + w * 16, out8_u8);
+    }
+}
+
+static INLINE void svt_unpack_and_2bcompress_remainder(uint16_t *in16b_buffer, uint8_t *out8b_buffer,
+                                                       uint8_t *out2b_buffer, uint32_t width) {
+    uint32_t col;
+    uint16_t in_pixel;
+    uint8_t  tmp_pixel;
+
+    uint32_t w_m4  = (width / 4) * 4;
+    uint32_t w_rem = width - w_m4;
+
+    for (col = 0; col < w_m4; col += 4) {
+        uint8_t compressed_unpacked_pixel = 0;
+        //+0
+        in_pixel                  = in16b_buffer[col + 0];
+        out8b_buffer[col + 0]     = (uint8_t)(in_pixel >> 2);
+        tmp_pixel                 = (uint8_t)(in_pixel << 6);
+        compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 0) & 0xC0); //1100.0000
+
+        //+1
+        in_pixel                  = in16b_buffer[col + 1];
+        out8b_buffer[col + 1]     = (uint8_t)(in_pixel >> 2);
+        tmp_pixel                 = (uint8_t)(in_pixel << 6);
+        compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 2) & 0x30); //0011.0000
+
+        //+2
+        in_pixel                  = in16b_buffer[col + 2];
+        out8b_buffer[col + 2]     = (uint8_t)(in_pixel >> 2);
+        tmp_pixel                 = (uint8_t)(in_pixel << 6);
+        compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 4) & 0x0C); //0000.1100
+
+        //+3
+        in_pixel                  = in16b_buffer[col + 3];
+        out8b_buffer[col + 3]     = (uint8_t)(in_pixel >> 2);
+        tmp_pixel                 = (uint8_t)(in_pixel << 6);
+        compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 6) & 0x03); //0000.0011
+
+        out2b_buffer[col / 4] = compressed_unpacked_pixel;
+    }
+
+    //we can have up to 3 pixels remaining
+    if (w_rem > 0) {
+        uint8_t compressed_unpacked_pixel = 0;
+        //+0
+        in_pixel                  = in16b_buffer[col + 0];
+        out8b_buffer[col + 0]     = (uint8_t)(in_pixel >> 2);
+        tmp_pixel                 = (uint8_t)(in_pixel << 6);
+        compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 0) & 0xC0); //1100.0000
+
+        if (w_rem > 1) {
+            //+1
+            in_pixel                  = in16b_buffer[col + 1];
+            out8b_buffer[col + 1]     = (uint8_t)(in_pixel >> 2);
+            tmp_pixel                 = (uint8_t)(in_pixel << 6);
+            compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 2) & 0x30); //0011.0000
+        }
+        if (w_rem > 2) {
+            //+2
+            in_pixel                  = in16b_buffer[col + 2];
+            out8b_buffer[col + 2]     = (uint8_t)(in_pixel >> 2);
+            tmp_pixel                 = (uint8_t)(in_pixel << 6);
+            compressed_unpacked_pixel = compressed_unpacked_pixel | ((tmp_pixel >> 4) & 0x0C); //0000.1100
+        }
+
+        out2b_buffer[col / 4] = compressed_unpacked_pixel;
+    }
+}
+
+void svt_unpack_and_2bcompress_neon(uint16_t *in16b_buffer, uint32_t in16b_stride, uint8_t *out8b_buffer,
+                                    uint32_t out8b_stride, uint8_t *out2b_buffer, uint32_t out2b_stride, uint32_t width,
+                                    uint32_t height) {
+    if (width == 32) {
+        for (uint32_t h = 0; h < height; h++) {
+            unpack_and_2bcompress_32_neon(
+                in16b_buffer + h * in16b_stride, out8b_buffer + h * out8b_stride, out2b_buffer + h * out2b_stride, 2);
+        }
+    } else if (width == 64) {
+        for (uint32_t h = 0; h < height; h++) {
+            unpack_and_2bcompress_32_neon(
+                in16b_buffer + h * in16b_stride, out8b_buffer + h * out8b_stride, out2b_buffer + h * out2b_stride, 4);
+        }
+    } else {
+        uint32_t offset_rem   = width & 0xfffffff0;
+        uint32_t offset2b_rem = offset_rem >> 2;
+        uint32_t remainder    = width & 0xf;
+        for (uint32_t h = 0; h < height; h++) {
+            unpack_and_2bcompress_32_neon(in16b_buffer + h * in16b_stride,
+                                          out8b_buffer + h * out8b_stride,
+                                          out2b_buffer + h * out2b_stride,
+                                          width >> 4);
+            if (remainder)
+                svt_unpack_and_2bcompress_remainder(in16b_buffer + h * in16b_stride + offset_rem,
+                                                    out8b_buffer + h * out8b_stride + offset_rem,
+                                                    out2b_buffer + h * out2b_stride + offset2b_rem,
+                                                    remainder);
+        }
+    }
+}
+
+static INLINE void compressed_packmsb_32x2h(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8_t *inn_bit_buffer,
+                                            uint32_t inn_stride, uint16_t *out16_bit_buffer, uint32_t out_stride,
+                                            uint32_t height) {
+    const uint8x16_t msk0 = vdupq_n_u8(0xC0); //1100.000
+
+    // processing 2 lines for chroma
+    for (uint32_t y = 0; y < height; y += 2) {
+        // 2 Lines Stored in 1D format-Could be replaced by 2 _mm_loadl_epi64
+        const uint8x16_t in_2_bit = vreinterpretq_u8_u64(
+            vzip1q_u64(vreinterpretq_u64_u8(vld1q_u8(inn_bit_buffer)),
+                       vreinterpretq_u64_u8(vld1q_u8(inn_bit_buffer + inn_stride))));
+
+        const uint8x16_t ext0 = vandq_u8(in_2_bit, msk0);
+        const uint8x16_t ext1 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 2)), msk0);
+        const uint8x16_t ext2 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 4)), msk0);
+        const uint8x16_t ext3 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 6)), msk0);
+
+        const uint8x16_t ext01   = vzip1q_u8(ext0, ext1);
+        const uint8x16_t ext23   = vzip1q_u8(ext2, ext3);
+        const uint8x16_t ext0_15 = vreinterpretq_u8_u16(
+            vzip1q_u16(vreinterpretq_u16_u8(ext01), vreinterpretq_u16_u8(ext23)));
+        const uint8x16_t ext16_31 = vreinterpretq_u8_u16(
+            vzip2q_u16(vreinterpretq_u16_u8(ext01), vreinterpretq_u16_u8(ext23)));
+
+        const uint8x16_t ext01h = vzip2q_u8(ext0, ext1);
+        const uint8x16_t ext23h = vzip2q_u8(ext2, ext3);
+
+        const uint8x16_t ext32_47 = vreinterpretq_u8_u16(
+            vzip1q_u16(vreinterpretq_u16_u8(ext01h), vreinterpretq_u16_u8(ext23h)));
+        const uint8x16_t ext48_63 = vreinterpretq_u8_u16(
+            vzip2q_u16(vreinterpretq_u16_u8(ext01h), vreinterpretq_u16_u8(ext23h)));
+
+        const uint8x16_t in_8_bit0 = vld1q_u8(in8_bit_buffer + 0);
+        const uint8x16_t in_8_bit1 = vld1q_u8(in8_bit_buffer + 16);
+        const uint8x16_t in_8_bit2 = vld1q_u8(in8_bit_buffer + in8_stride);
+        const uint8x16_t in_8_bit3 = vld1q_u8(in8_bit_buffer + in8_stride + 16);
+
+        // (out_pixel | n_bit_pixel) concatenation is done with unpacklo_epi8 and unpackhi_epi8
+        const uint16x8_t concat00 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext0_15, in_8_bit0)), 6);
+        const uint16x8_t concat01 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext0_15, in_8_bit0)), 6);
+        const uint16x8_t concat02 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext16_31, in_8_bit1)), 6);
+        const uint16x8_t concat03 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext16_31, in_8_bit1)), 6);
+
+        vst1q_u16(out16_bit_buffer + 0, concat00);
+        vst1q_u16(out16_bit_buffer + 8, concat01);
+        vst1q_u16(out16_bit_buffer + 16, concat02);
+        vst1q_u16(out16_bit_buffer + 24, concat03);
+
+        // (out_pixel | n_bit_pixel) concatenation is done with unpacklo_epi8 and unpackhi_epi8
+        const uint16x8_t concat10 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext32_47, in_8_bit2)), 6);
+        const uint16x8_t concat11 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext32_47, in_8_bit2)), 6);
+        const uint16x8_t concat12 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext48_63, in_8_bit3)), 6);
+        const uint16x8_t concat13 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext48_63, in_8_bit3)), 6);
+
+        vst1q_u16(out16_bit_buffer + out_stride + 0, concat10);
+        vst1q_u16(out16_bit_buffer + out_stride + 8, concat11);
+        vst1q_u16(out16_bit_buffer + out_stride + 16, concat12);
+        vst1q_u16(out16_bit_buffer + out_stride + 24, concat13);
+
+        in8_bit_buffer += in8_stride << 1;
+        inn_bit_buffer += inn_stride << 1;
+        out16_bit_buffer += out_stride << 1;
+    }
+}
+
+static INLINE void compressed_packmsb_64xh(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8_t *inn_bit_buffer,
+                                           uint32_t inn_stride, uint16_t *out16_bit_buffer, uint32_t out_stride,
+                                           uint32_t height) {
+    const uint8x16_t msk0 = vdupq_n_u8(0xC0); //1100.000
+
+    // one row per iteration
+    for (uint32_t y = 0; y < height; y++) {
+        const uint8x16_t in_2_bit = vld1q_u8(inn_bit_buffer);
+
+        const uint8x16_t ext0 = vandq_u8(in_2_bit, msk0);
+        const uint8x16_t ext1 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 2)), msk0);
+        const uint8x16_t ext2 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 4)), msk0);
+        const uint8x16_t ext3 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 6)), msk0);
+
+        const uint8x16_t ext01 = vzip1q_u8(ext0, ext1);
+        const uint8x16_t ext23 = vzip1q_u8(ext2, ext3);
+
+        const uint8x16_t ext0_15 = vreinterpretq_u8_u16(
+            vzip1q_u16(vreinterpretq_u16_u8(ext01), vreinterpretq_u16_u8(ext23)));
+        const uint8x16_t ext16_31 = vreinterpretq_u8_u16(
+            vzip2q_u16(vreinterpretq_u16_u8(ext01), vreinterpretq_u16_u8(ext23)));
+
+        const uint8x16_t ext01h = vzip2q_u8(ext0, ext1);
+        const uint8x16_t ext23h = vzip2q_u8(ext2, ext3);
+
+        const uint8x16_t ext32_47 = vreinterpretq_u8_u16(
+            vzip1q_u16(vreinterpretq_u16_u8(ext01h), vreinterpretq_u16_u8(ext23h)));
+        const uint8x16_t ext48_63 = vreinterpretq_u8_u16(
+            vzip2q_u16(vreinterpretq_u16_u8(ext01h), vreinterpretq_u16_u8(ext23h)));
+
+        const uint8x16_t in_8_bit0 = vld1q_u8(in8_bit_buffer + 0);
+        const uint8x16_t in_8_bit1 = vld1q_u8(in8_bit_buffer + 16);
+        const uint8x16_t in_8_bit2 = vld1q_u8(in8_bit_buffer + 32);
+        const uint8x16_t in_8_bit3 = vld1q_u8(in8_bit_buffer + 48);
+
+        // (out_pixel | n_bit_pixel) concatenation
+        const uint16x8_t concat00 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext0_15, in_8_bit0)), 6);
+        const uint16x8_t concat01 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext0_15, in_8_bit0)), 6);
+        const uint16x8_t concat02 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext16_31, in_8_bit1)), 6);
+        const uint16x8_t concat03 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext16_31, in_8_bit1)), 6);
+
+        vst1q_u16(out16_bit_buffer + 0, concat00);
+        vst1q_u16(out16_bit_buffer + 8, concat01);
+        vst1q_u16(out16_bit_buffer + 16, concat02);
+        vst1q_u16(out16_bit_buffer + 24, concat03);
+
+        // (out_pixel | n_bit_pixel) concatenation
+        const uint16x8_t concat10 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext32_47, in_8_bit2)), 6);
+        const uint16x8_t concat11 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext32_47, in_8_bit2)), 6);
+        const uint16x8_t concat12 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext48_63, in_8_bit3)), 6);
+        const uint16x8_t concat13 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext48_63, in_8_bit3)), 6);
+
+        vst1q_u16(out16_bit_buffer + 32, concat10);
+        vst1q_u16(out16_bit_buffer + 40, concat11);
+        vst1q_u16(out16_bit_buffer + 48, concat12);
+        vst1q_u16(out16_bit_buffer + 56, concat13);
+
+        in8_bit_buffer += in8_stride;
+        inn_bit_buffer += inn_stride;
+        out16_bit_buffer += out_stride;
+    }
+}
+
+static INLINE void compressed_packmsb_64(uint8_t *in8_bit_buffer, uint8_t *inn_bit_buffer, uint16_t *out16_bit_buffer,
+                                         uint32_t width_rep) {
+    const uint8x16_t msk0 = vdupq_n_u8(0xC0); //1100.000
+
+    // one row per iteration
+    for (uint32_t w = 0; w < width_rep; w++) {
+        const uint8x16_t in_2_bit = vld1q_u8(inn_bit_buffer);
+
+        const uint8x16_t ext0 = vandq_u8(in_2_bit, msk0);
+        const uint8x16_t ext1 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 2)), msk0);
+        const uint8x16_t ext2 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 4)), msk0);
+        const uint8x16_t ext3 = vandq_u8(vreinterpretq_u8_u16(vshlq_n_u16(vreinterpretq_u16_u8(in_2_bit), 6)), msk0);
+
+        const uint8x16_t ext01 = vzip1q_u8(ext0, ext1);
+        const uint8x16_t ext23 = vzip1q_u8(ext2, ext3);
+
+        const uint8x16_t ext0_15 = vreinterpretq_u8_u16(
+            vzip1q_u16(vreinterpretq_u16_u8(ext01), vreinterpretq_u16_u8(ext23)));
+        const uint8x16_t ext16_31 = vreinterpretq_u8_u16(
+            vzip2q_u16(vreinterpretq_u16_u8(ext01), vreinterpretq_u16_u8(ext23)));
+
+        const uint8x16_t ext01h = vzip2q_u8(ext0, ext1);
+        const uint8x16_t ext23h = vzip2q_u8(ext2, ext3);
+
+        const uint8x16_t ext32_47 = vreinterpretq_u8_u16(
+            vzip1q_u16(vreinterpretq_u16_u8(ext01h), vreinterpretq_u16_u8(ext23h)));
+        const uint8x16_t ext48_63 = vreinterpretq_u8_u16(
+            vzip2q_u16(vreinterpretq_u16_u8(ext01h), vreinterpretq_u16_u8(ext23h)));
+
+        const uint8x16_t in_8_bit0 = vld1q_u8(in8_bit_buffer + 0);
+        const uint8x16_t in_8_bit1 = vld1q_u8(in8_bit_buffer + 16);
+        const uint8x16_t in_8_bit2 = vld1q_u8(in8_bit_buffer + 32);
+        const uint8x16_t in_8_bit3 = vld1q_u8(in8_bit_buffer + 48);
+
+        // (out_pixel | n_bit_pixel) concatenation
+        const uint16x8_t concat00 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext0_15, in_8_bit0)), 6);
+        const uint16x8_t concat01 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext0_15, in_8_bit0)), 6);
+        const uint16x8_t concat02 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext16_31, in_8_bit1)), 6);
+        const uint16x8_t concat03 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext16_31, in_8_bit1)), 6);
+
+        vst1q_u16(out16_bit_buffer + 0, concat00);
+        vst1q_u16(out16_bit_buffer + 8, concat01);
+        vst1q_u16(out16_bit_buffer + 16, concat02);
+        vst1q_u16(out16_bit_buffer + 24, concat03);
+
+        // (out_pixel | n_bit_pixel) concatenation
+        const uint16x8_t concat10 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext32_47, in_8_bit2)), 6);
+        const uint16x8_t concat11 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext32_47, in_8_bit2)), 6);
+        const uint16x8_t concat12 = vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(ext48_63, in_8_bit3)), 6);
+        const uint16x8_t concat13 = vshrq_n_u16(vreinterpretq_u16_u8(vzip2q_u8(ext48_63, in_8_bit3)), 6);
+
+        vst1q_u16(out16_bit_buffer + 32, concat10);
+        vst1q_u16(out16_bit_buffer + 40, concat11);
+        vst1q_u16(out16_bit_buffer + 48, concat12);
+        vst1q_u16(out16_bit_buffer + 56, concat13);
+
+        in8_bit_buffer += 64;
+        inn_bit_buffer += 16;
+        out16_bit_buffer += 64;
+    }
+}
+
+void svt_compressed_packmsb_neon(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8_t *inn_bit_buffer,
+                                 uint32_t inn_stride, uint16_t *out16_bit_buffer, uint32_t out_stride, uint32_t width,
+                                 uint32_t height) {
+    if (width == 32) {
+        compressed_packmsb_32x2h(
+            in8_bit_buffer, in8_stride, inn_bit_buffer, inn_stride, out16_bit_buffer, out_stride, height);
+    } else if (width == 64) {
+        compressed_packmsb_64xh(
+            in8_bit_buffer, in8_stride, inn_bit_buffer, inn_stride, out16_bit_buffer, out_stride, height);
+    } else {
+        int32_t  leftover     = width;
+        uint32_t offset8b_16b = 0;
+        uint32_t offset2b     = 0;
+        if (leftover >= 64) {
+            uint32_t offset = width & 0xffffff40;
+            for (uint32_t y = 0; y < height; y++) {
+                compressed_packmsb_64(in8_bit_buffer + y * in8_stride,
+                                      inn_bit_buffer + y * inn_stride,
+                                      out16_bit_buffer + y * out_stride,
+                                      width >> 6);
+            }
+            offset8b_16b += offset;
+            offset2b += offset >> 2;
+            leftover -= offset;
+        }
+        if (leftover >= 32) {
+            compressed_packmsb_32x2h(in8_bit_buffer + offset8b_16b,
+                                     in8_stride,
+                                     inn_bit_buffer + offset2b,
+                                     inn_stride,
+                                     out16_bit_buffer + offset8b_16b,
+                                     out_stride,
+                                     height);
+            offset8b_16b += 32;
+            offset2b += 8;
+            leftover -= 32;
+        }
+        if (leftover) {
+            svt_compressed_packmsb_c(in8_bit_buffer + offset8b_16b,
+                                     in8_stride,
+                                     inn_bit_buffer + offset2b,
+                                     inn_stride,
+                                     out16_bit_buffer + offset8b_16b,
+                                     out_stride,
+                                     leftover,
+                                     height);
+        }
+    }
+}
+
+void svt_enc_msb_pack2d_neon(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8_t *inn_bit_buffer,
+                             uint16_t *out16_bit_buffer, uint32_t inn_stride, uint32_t out_stride, uint32_t width,
+                             uint32_t height) {
+    uint32_t count_width, count_height;
+
+    if (width == 4) {
+        for (count_height = 0; count_height < height; count_height += 2) {
+            vst1_u16(out16_bit_buffer,
+                     vshr_n_u16(
+                         vreinterpret_u16_u8(vzip1_u8(vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(inn_bit_buffer))),
+                                                      vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(in8_bit_buffer))))),
+                         6));
+            vst1_u16(out16_bit_buffer + out_stride,
+                     vshr_n_u16(vreinterpret_u16_u8(vzip1_u8(
+                                    vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(inn_bit_buffer + inn_stride))),
+                                    vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(in8_bit_buffer + in8_stride))))),
+                                6));
+
+            out16_bit_buffer += (out_stride << 1);
+            in8_bit_buffer += (in8_stride << 1);
+            inn_bit_buffer += (inn_stride << 1);
+        }
+    } else if (width == 8) {
+        for (count_height = 0; count_height < height; count_height += 2) {
+            vst1q_u16(out16_bit_buffer,
+                      vshrq_n_u16(
+                          vreinterpretq_u16_u8(vzip1q_u8(
+                              vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(inn_bit_buffer))), vdup_n_u8(0)),
+                              vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(in8_bit_buffer))), vdup_n_u8(0)))),
+                          6));
+            vst1q_u16(
+                out16_bit_buffer + out_stride,
+                vshrq_n_u16(vreinterpretq_u16_u8(vzip1q_u8(
+                                vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(inn_bit_buffer + inn_stride))),
+                                            vdup_n_u8(0)),
+                                vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(in8_bit_buffer + in8_stride))),
+                                            vdup_n_u8(0)))),
+                            6));
+
+            out16_bit_buffer += (out_stride << 1);
+            in8_bit_buffer += (in8_stride << 1);
+            inn_bit_buffer += (inn_stride << 1);
+        }
+    } else if (width == 16) {
+        for (count_height = 0; count_height < height; count_height += 2) {
+            const uint8x16_t inn_bit_buffer_lo = vld1q_u8(inn_bit_buffer);
+            const uint8x16_t inn_bit_buffer_hi = vld1q_u8(inn_bit_buffer + inn_stride);
+            const uint8x16_t in_8bit_buffer_lo = vld1q_u8(in8_bit_buffer);
+            const uint8x16_t in_8bit_buffer_hi = vld1q_u8(in8_bit_buffer + in8_stride);
+
+            const uint16x8_t out_pixel_1 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_lo, in_8bit_buffer_lo)), 6);
+            const uint16x8_t out_pixel_2 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_lo, in_8bit_buffer_lo)), 6);
+            const uint16x8_t out_pixel_3 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_hi, in_8bit_buffer_hi)), 6);
+            const uint16x8_t out_pixel_4 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_hi, in_8bit_buffer_hi)), 6);
+
+            vst1q_u16(out16_bit_buffer + 0, out_pixel_1);
+            vst1q_u16(out16_bit_buffer + 8, out_pixel_2);
+            vst1q_u16(out16_bit_buffer + out_stride + 0, out_pixel_3);
+            vst1q_u16(out16_bit_buffer + out_stride + 8, out_pixel_4);
+
+            in8_bit_buffer += (in8_stride << 1);
+            inn_bit_buffer += (inn_stride << 1);
+            out16_bit_buffer += (out_stride << 1);
+        }
+    } else if (width == 32) {
+        for (count_height = 0; count_height < height; count_height += 2) {
+            const uint8x16_t inn_bit_buffer_1 = vld1q_u8(inn_bit_buffer);
+            const uint8x16_t inn_bit_buffer_2 = vld1q_u8(inn_bit_buffer + 16);
+            const uint8x16_t inn_bit_buffer_3 = vld1q_u8(inn_bit_buffer + inn_stride);
+            const uint8x16_t inn_bit_buffer_4 = vld1q_u8(inn_bit_buffer + inn_stride + 16);
+
+            const uint8x16_t in_8bit_buffer1 = vld1q_u8(in8_bit_buffer);
+            const uint8x16_t in_8bit_buffer2 = vld1q_u8(in8_bit_buffer + 16);
+            const uint8x16_t in_8bit_buffer3 = vld1q_u8(in8_bit_buffer + in8_stride);
+            const uint8x16_t in_8bit_buffer4 = vld1q_u8(in8_bit_buffer + in8_stride + 16);
+
+            const uint16x8_t out_pixel_1 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_1, in_8bit_buffer1)), 6);
+            const uint16x8_t out_pixel_2 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_1, in_8bit_buffer1)), 6);
+            const uint16x8_t out_pixel_3 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_2, in_8bit_buffer2)), 6);
+            const uint16x8_t out_pixel_4 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_2, in_8bit_buffer2)), 6);
+            const uint16x8_t out_pixel_5 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_3, in_8bit_buffer3)), 6);
+            const uint16x8_t out_pixel_6 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_3, in_8bit_buffer3)), 6);
+            const uint16x8_t out_pixel_7 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_4, in_8bit_buffer4)), 6);
+            const uint16x8_t out_pixel_8 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_4, in_8bit_buffer4)), 6);
+
+            vst1q_u16(out16_bit_buffer + 0, out_pixel_1);
+            vst1q_u16(out16_bit_buffer + 8, out_pixel_2);
+            vst1q_u16(out16_bit_buffer + 16, out_pixel_3);
+            vst1q_u16(out16_bit_buffer + 24, out_pixel_4);
+            vst1q_u16(out16_bit_buffer + out_stride + 0, out_pixel_5);
+            vst1q_u16(out16_bit_buffer + out_stride + 8, out_pixel_6);
+            vst1q_u16(out16_bit_buffer + out_stride + 16, out_pixel_7);
+            vst1q_u16(out16_bit_buffer + out_stride + 24, out_pixel_8);
+
+            in8_bit_buffer += (in8_stride << 1);
+            inn_bit_buffer += (inn_stride << 1);
+            out16_bit_buffer += (out_stride << 1);
+        }
+    } else if (width == 64) {
+        for (count_height = 0; count_height < height; ++count_height) {
+            const uint8x16_t inn_bit_buffer_1 = vld1q_u8(inn_bit_buffer);
+            const uint8x16_t inn_bit_buffer_2 = vld1q_u8(inn_bit_buffer + 16);
+            const uint8x16_t inn_bit_buffer_3 = vld1q_u8(inn_bit_buffer + 32);
+            const uint8x16_t inn_bit_buffer_4 = vld1q_u8(inn_bit_buffer + 48);
+
+            const uint8x16_t in_8bit_buffer1 = vld1q_u8(in8_bit_buffer);
+            const uint8x16_t in_8bit_buffer2 = vld1q_u8(in8_bit_buffer + 16);
+            const uint8x16_t in_8bit_buffer3 = vld1q_u8(in8_bit_buffer + 32);
+            const uint8x16_t in_8bit_buffer4 = vld1q_u8(in8_bit_buffer + 48);
+
+            const uint16x8_t out_pixel_1 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_1, in_8bit_buffer1)), 6);
+            const uint16x8_t out_pixel_2 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_1, in_8bit_buffer1)), 6);
+            const uint16x8_t out_pixel_3 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_2, in_8bit_buffer2)), 6);
+            const uint16x8_t out_pixel_4 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_2, in_8bit_buffer2)), 6);
+            const uint16x8_t out_pixel_5 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_3, in_8bit_buffer3)), 6);
+            const uint16x8_t out_pixel_6 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_3, in_8bit_buffer3)), 6);
+            const uint16x8_t out_pixel_7 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip1q_u8(inn_bit_buffer_4, in_8bit_buffer4)), 6);
+            const uint16x8_t out_pixel_8 = vshrq_n_u16(
+                vreinterpretq_u16_u8(vzip2q_u8(inn_bit_buffer_4, in_8bit_buffer4)), 6);
+
+            vst1q_u16(out16_bit_buffer + 0, out_pixel_1);
+            vst1q_u16(out16_bit_buffer + 8, out_pixel_2);
+            vst1q_u16(out16_bit_buffer + 16, out_pixel_3);
+            vst1q_u16(out16_bit_buffer + 24, out_pixel_4);
+            vst1q_u16(out16_bit_buffer + 32, out_pixel_5);
+            vst1q_u16(out16_bit_buffer + 40, out_pixel_6);
+            vst1q_u16(out16_bit_buffer + 48, out_pixel_7);
+            vst1q_u16(out16_bit_buffer + 56, out_pixel_8);
+
+            in8_bit_buffer += in8_stride;
+            inn_bit_buffer += inn_stride;
+            out16_bit_buffer += out_stride;
+        }
+    } else {
+        uint32_t in_n_stride_diff = (inn_stride << 1) - width;
+        uint32_t in_8_stride_diff = (in8_stride << 1) - width;
+        uint32_t out_stride_diff  = (out_stride << 1) - width;
+
+        if (!(width & 7)) {
+            for (count_height = 0; count_height < height; count_height += 2) {
+                for (count_width = 0; count_width < width; count_width += 8) {
+                    vst1q_u16(
+                        out16_bit_buffer,
+                        vshrq_n_u16(
+                            vreinterpretq_u16_u8(vzip1q_u8(
+                                vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(inn_bit_buffer))), vdup_n_u8(0)),
+                                vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(in8_bit_buffer))),
+                                            vdup_n_u8(0)))),
+                            6));
+                    vst1q_u16(
+                        out16_bit_buffer + out_stride,
+                        vshrq_n_u16(
+                            vreinterpretq_u16_u8(vzip1q_u8(
+                                vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(inn_bit_buffer + inn_stride))),
+                                            vdup_n_u8(0)),
+                                vcombine_u8(vreinterpret_u8_u64(vld1_u64((uint64_t *)(in8_bit_buffer + in8_stride))),
+                                            vdup_n_u8(0)))),
+                            6));
+
+                    out16_bit_buffer += 8;
+                    in8_bit_buffer += 8;
+                    inn_bit_buffer += 8;
+                }
+                in8_bit_buffer += in_8_stride_diff;
+                inn_bit_buffer += in_n_stride_diff;
+                out16_bit_buffer += out_stride_diff;
+            }
+        } else {
+            for (count_height = 0; count_height < height; count_height += 2) {
+                for (count_width = 0; count_width < width; count_width += 4) {
+                    vst1_u16(out16_bit_buffer,
+                             vshr_n_u16(vreinterpret_u16_u8(
+                                            vzip1_u8(vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(inn_bit_buffer))),
+                                                     vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(in8_bit_buffer))))),
+                                        6));
+                    vst1_u16(
+                        out16_bit_buffer + out_stride,
+                        vshr_n_u16(vreinterpret_u16_u8(vzip1_u8(
+                                       vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(inn_bit_buffer + inn_stride))),
+                                       vreinterpret_u8_u32(vdup_n_u32(*(uint32_t *)(in8_bit_buffer + in8_stride))))),
+                                   6));
+
+                    out16_bit_buffer += 4;
+                    in8_bit_buffer += 4;
+                    inn_bit_buffer += 4;
+                }
+                in8_bit_buffer += in_8_stride_diff;
+                inn_bit_buffer += in_n_stride_diff;
+                out16_bit_buffer += out_stride_diff;
+            }
+        }
+    }
+}
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 16e13849c..37e8316ec 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -866,7 +866,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_av1_inv_txfm2d_add_64x64, svt_av1_inv_txfm2d_add_64x64_c, svt_av1_inv_txfm2d_add_64x64_neon);
     SET_NEON(svt_av1_inv_txfm_add, svt_av1_inv_txfm_add_c, svt_dav1d_inv_txfm_add_neon);
 
-    SET_ONLY_C(svt_compressed_packmsb, svt_compressed_packmsb_c);
+    SET_NEON(svt_compressed_packmsb, svt_compressed_packmsb_c, svt_compressed_packmsb_neon);
     SET_ONLY_C(svt_c_pack, svt_c_pack_c);
     SET_ONLY_C(svt_unpack_avg, svt_unpack_avg_c);
     SET_ONLY_C(svt_unpack_avg_safe_sub, svt_unpack_avg_safe_sub_c);
@@ -875,7 +875,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_cfl_luma_subsampling_420_hbd, svt_cfl_luma_subsampling_420_hbd_c);
     SET_ONLY_C(svt_convert_8bit_to_16bit, svt_convert_8bit_to_16bit_c);
     SET_ONLY_C(svt_convert_16bit_to_8bit, svt_convert_16bit_to_8bit_c);
-    SET_ONLY_C(svt_pack2d_16_bit_src_mul4, svt_enc_msb_pack2_d);
+    SET_NEON(svt_pack2d_16_bit_src_mul4, svt_enc_msb_pack2_d, svt_enc_msb_pack2d_neon);
     SET_ONLY_C(svt_aom_un_pack2d_16_bit_src_mul4, svt_enc_msb_un_pack2_d);
     SET_ONLY_C(svt_full_distortion_kernel_cbf_zero32_bits, svt_full_distortion_kernel_cbf_zero32_bits_c);
     SET_NEON(svt_full_distortion_kernel32_bits, svt_full_distortion_kernel32_bits_c, svt_full_distortion_kernel32_bits_neon);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index 2f9dc8e67..d0fd46c8e 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1377,6 +1377,13 @@ extern "C" {
 
     void svt_av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
+    void svt_compressed_packmsb_neon(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8_t *inn_bit_buffer,
+                                     uint32_t inn_stride, uint16_t *out16_bit_buffer, uint32_t out_stride,
+                                     uint32_t width, uint32_t height);
+
+    void svt_enc_msb_pack2d_neon(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8_t *inn_bit_buffer,
+                                 uint16_t *out16_bit_buffer, uint32_t inn_stride, uint32_t out_stride, uint32_t width,
+                                 uint32_t height);
 #endif
 
 #ifdef ARCH_X86_64
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
index 156b7ea66..cf9f6337f 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
@@ -856,7 +856,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_av1_calc_indices_dim2, svt_av1_calc_indices_dim2_c);
     SET_ONLY_C(variance_highbd, svt_aom_variance_highbd_c);
     SET_ONLY_C(svt_av1_haar_ac_sad_8x8_uint8_input, svt_av1_haar_ac_sad_8x8_uint8_input_c);
-    SET_ONLY_C(svt_unpack_and_2bcompress, svt_unpack_and_2bcompress_c);
+    SET_NEON(svt_unpack_and_2bcompress, svt_unpack_and_2bcompress_c, svt_unpack_and_2bcompress_neon);
     SET_ONLY_C(svt_estimate_noise_fp16, svt_estimate_noise_fp16_c);
     SET_ONLY_C(svt_estimate_noise_highbd_fp16, svt_estimate_noise_highbd_fp16_c);
     SET_ONLY_C(svt_copy_mi_map_grid, svt_copy_mi_map_grid_c);
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
index f99955076..927c6935d 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
@@ -1134,6 +1134,10 @@ extern "C" {
     void svt_av1_compute_stats_highbd_neon(int32_t wiener_win, const uint8_t *dgd8, const uint8_t *src8, int32_t h_start, int32_t h_end, int32_t v_start, int32_t v_end, int32_t dgd_stride, int32_t src_stride, int64_t *M, int64_t *H, EbBitDepth bit_depth);
     uint64_t svt_aom_compute_cdef_dist_16bit_neon(const uint16_t *dst, int32_t dstride, const uint16_t *src, const CdefList *dlist, int32_t cdef_count, BlockSize bsize, int32_t coeff_shift, int32_t pli, uint8_t subsampling_factor);
 
+    void svt_unpack_and_2bcompress_neon(uint16_t *in16b_buffer, uint32_t in16b_stride, uint8_t *out8b_buffer,
+                                    uint32_t out8b_stride, uint8_t *out2b_buffer, uint32_t out2b_stride, uint32_t width,
+                                    uint32_t height);
+
 #endif
 
 #ifdef ARCH_X86_64
diff --git a/test/CMakeLists.txt b/test/CMakeLists.txt
index d533be7c4..a3ee3c65c 100644
--- a/test/CMakeLists.txt
+++ b/test/CMakeLists.txt
@@ -87,62 +87,65 @@ set(arch_neutral_files
     ../third_party/aom_dsp/src/entdec.c)
 
 set(multi_arch_files
+    CdefTest.cc
     FwdTxfm2dAsmTest.cc
     InvTxfm2dAsmTest.cc
+    PackUnPackTest.cc
     PictureOperatorTest.cc
+    RestorationPickTest.cc
     SadTest.cc
-    selfguided_filter_test.cc
-    CdefTest.cc
     SpatialFullDistortionTest.cc
     TemporalFilterTestPlanewise.cc
     convolve_2d_test.cc
-    RestorationPickTest.cc)
+    RestorationPickTest.cc
+    selfguided_filter_test.cc
+)
 
 if(HAVE_X86_PLATFORM)
-  set(x86_arch_files
-      AdaptiveScanTest.cc
-      CompoundUtilTest.cc
-      DeblockTest.cc
-      EbNoiseModel_test.cc
-      EbBlend_a64_mask_1d_test.cc
-      EbBlend_a64_mask_test.cc
-      EbHighbdIntraPredictionTests.cc
-      EbHighbdIntraPredictionTests.h
-      EncodeTxbAsmTest.cc
-      FFTTest.cc
-      FilterIntraPredTest.cc
-      ForwardtransformTests.cc
-      FwdTxfm1dTest.cc
-      InvTxfm1dTest.cc
-      FwdTxfm2dTest.cc
-      HbdVarianceTest.cc
-      MotionEstimationTest.cc
-      OBMCSadTest.cc
-      OBMCVarianceTest.cc
-      PackUnPackTest.cc
-      PaletteModeUtilTest.cc
-      PsnrTest.cc
-      QuantAsmTest.cc
-      ResidualTest.cc
-      SelfGuidedUtilTest.cc
-      VarianceTest.cc
-      WedgeUtilTest.cc
-      av1_convolve_scale_test.cc
-      compute_mean_test.cc
-      corner_match_test.cc
-      dwt_test.cc
-      frame_error_test.cc
-      hadamard_test.cc
-      intrapred_cfl_test.cc
-      intrapred_dr_test.cc
-      intrapred_edge_filter_test.cc
-      intrapred_test.cc
-      quantize_func_test.cc
-      subtract_avg_cfl_test.cc
-      warp_filter_test.cc
-      warp_filter_test_util.cc
-      warp_filter_test_util.h
-      wiener_convolve_test.cc)
+    set(x86_arch_files
+        AdaptiveScanTest.cc
+        CompoundUtilTest.cc
+        DeblockTest.cc
+        EbNoiseModel_test.cc
+        EbBlend_a64_mask_1d_test.cc
+        EbBlend_a64_mask_test.cc
+        EbHighbdIntraPredictionTests.cc
+        EbHighbdIntraPredictionTests.h
+        EncodeTxbAsmTest.cc
+        FFTTest.cc
+        FilterIntraPredTest.cc
+        ForwardtransformTests.cc
+        FwdTxfm1dTest.cc
+        InvTxfm1dTest.cc
+        FwdTxfm2dTest.cc
+        HbdVarianceTest.cc
+        MotionEstimationTest.cc
+        OBMCSadTest.cc
+        OBMCVarianceTest.cc
+        PaletteModeUtilTest.cc
+        PsnrTest.cc
+        QuantAsmTest.cc
+        ResidualTest.cc
+        SelfGuidedUtilTest.cc
+        VarianceTest.cc
+        WedgeUtilTest.cc
+        av1_convolve_scale_test.cc
+        compute_mean_test.cc
+        corner_match_test.cc
+        dwt_test.cc
+        frame_error_test.cc
+        hadamard_test.cc
+        intrapred_cfl_test.cc
+        intrapred_dr_test.cc
+        intrapred_edge_filter_test.cc
+        intrapred_test.cc
+        quantize_func_test.cc
+        subtract_avg_cfl_test.cc
+        warp_filter_test.cc
+        warp_filter_test_util.cc
+        warp_filter_test_util.h
+        wiener_convolve_test.cc
+    )
 endif()
 
 if(HAVE_ARM_PLATFORM)
diff --git a/test/PackUnPackTest.cc b/test/PackUnPackTest.cc
index e69878ed5..006c49c8f 100644
--- a/test/PackUnPackTest.cc
+++ b/test/PackUnPackTest.cc
@@ -23,6 +23,9 @@
  * - svt_unpack_avg_avx2_intrin
  * - svt_unpack_avg_sse2_intrin
  * - svt_unpack_avg_safe_sub_avx2_intrin
+ * - svt_unpack_and_2bcompress_neon
+ * - svt_compressed_packmsb_neon
+ * - svt_enc_msb_pack2d_neon
  *
  * @author Cidana-Ivy, Cidana-Wenyao
  *
@@ -47,6 +50,7 @@ using svt_av1_test_tool::SVTRandom;  // to generate the random
 namespace {
 const int RANDOM_TIME = 8;
 typedef std::tuple<uint32_t, uint32_t> AreaSize;
+
 /*
  * TEST_PACK_SIZES: containing width of 32, 64.
  */
@@ -79,6 +83,8 @@ AreaSize TEST_PACK_SIZES_EXTEND[] = {AreaSize(128, 128),
                                      AreaSize(800, 600),
                                      AreaSize(640, 480)};
 
+#ifdef ARCH_X86_64
+
 // test svt_c_pack_avx2_intrin, which only support width of 32 and 64;
 class PackTest : public ::testing::TestWithParam<AreaSize> {
   public:
@@ -185,6 +191,8 @@ TEST_P(PackTest, PackTest) {
 
 INSTANTIATE_TEST_SUITE_P(PACK, PackTest, ::testing::ValuesIn(TEST_PACK_SIZES));
 
+#endif  // ARCH_X86_64
+
 // test svt_compressed_packmsb_sse4_1_intrin
 // test svt_compressed_packmsb_avx2_intrin
 typedef void (*svt_compressed_packmsb_fn)(
@@ -296,7 +304,9 @@ TEST_P(PackMsbTest, PackMsbTest) {
     run_test();
 };
 
-INSTANTIATE_TEST_SUITE_P(
+#ifdef ARCH_X86_64
+
+INSTANTIATE_TEST_CASE_P(
     PACKMSB, PackMsbTest,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
                        ::testing::Values(svt_compressed_packmsb_sse4_1_intrin,
@@ -308,6 +318,22 @@ INSTANTIATE_TEST_SUITE_P(
                        ::testing::Values(svt_compressed_packmsb_sse4_1_intrin,
                                          svt_compressed_packmsb_avx2_intrin)));
 
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+
+INSTANTIATE_TEST_CASE_P(
+    PACKMSB, PackMsbTest,
+    ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
+                       ::testing::Values(svt_compressed_packmsb_neon)));
+
+INSTANTIATE_TEST_CASE_P(
+    PACKMSB_EXTEND, PackMsbTest,
+    ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES_EXTEND),
+                       ::testing::Values(svt_compressed_packmsb_neon)));
+
+#endif  // ARCH_AARCH64
+
 // test svt_unpack_and_2bcompress_sse4_1
 // test svt_unpack_and_2bcompress_avx2
 typedef void (*svt_unpack_and_2bcompress_fn)(
@@ -438,7 +464,9 @@ TEST_P(Unpack2bCompress, Unpack2bCompress) {
     run_test();
 };
 
-INSTANTIATE_TEST_SUITE_P(
+#ifdef ARCH_X86_64
+
+INSTANTIATE_TEST_CASE_P(
     UNPACK2BCOMPRESS, Unpack2bCompress,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
                        ::testing::Values(svt_unpack_and_2bcompress_sse4_1,
@@ -461,6 +489,32 @@ INSTANTIATE_TEST_SUITE_P(
                        ::testing::Values(svt_unpack_and_2bcompress_sse4_1,
                                          svt_unpack_and_2bcompress_avx2)));
 
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+
+INSTANTIATE_TEST_CASE_P(
+    UNPACK2BCOMPRESS, Unpack2bCompress,
+    ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
+                       ::testing::Values(svt_unpack_and_2bcompress_neon)));
+
+INSTANTIATE_TEST_CASE_P(
+    UNPACK2BCOMPRESS_EXTEND, Unpack2bCompress,
+    ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES_EXTEND),
+                       ::testing::Values(svt_unpack_and_2bcompress_neon)));
+
+INSTANTIATE_TEST_CASE_P(
+    UNPACK2BCOMPRESS_EXTEND2, Unpack2bCompress,
+    ::testing::Combine(::testing::Values(AreaSize(32, 1), AreaSize(32, 2),
+                                         AreaSize(32, 3), AreaSize(32, 5),
+                                         AreaSize(64, 1), AreaSize(64, 2),
+                                         AreaSize(64, 3), AreaSize(64, 5),
+                                         AreaSize(65, 3), AreaSize(66, 5),
+                                         AreaSize(129, 3), AreaSize(129, 5)),
+                       ::testing::Values(svt_unpack_and_2bcompress_neon)));
+
+#endif  // ARCH_AARCH64
+
 // test svt_enc_msb_pack2d_avx2_intrin_al and svt_enc_msb_pack2d_sse2_intrin.
 // There is an implicit assumption that the width should be multiple of 4.
 // Also there are special snippet to handle width of {4, 8, 16, 32, 64}, so use
@@ -483,9 +537,16 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
         test_size_ = MAX_SB_SQUARE;
         in_8bit_buffer_ = nullptr;
         inn_bit_buffer_ = nullptr;
-        out_16bit_buffer_avx2_ = nullptr;
         out_16bit_buffer_c_ = nullptr;
+
+#ifdef ARCH_X86_64
+        out_16bit_buffer_avx2_ = nullptr;
         out_16bit_buffer_sse2_ = nullptr;
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+        out_16bit_buffer_neon_ = nullptr;
+#endif  // ARCH_AARCH64
     }
 
     void SetUp() override {
@@ -493,15 +554,25 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
             reinterpret_cast<uint8_t *>(svt_aom_memalign(32, test_size_));
         inn_bit_buffer_ =
             reinterpret_cast<uint8_t *>(svt_aom_memalign(32, test_size_));
-        out_16bit_buffer_avx2_ = reinterpret_cast<uint16_t *>(
-            svt_aom_memalign(32, sizeof(uint16_t) * test_size_));
+
         out_16bit_buffer_c_ = reinterpret_cast<uint16_t *>(
             svt_aom_memalign(32, sizeof(uint16_t) * test_size_));
-        out_16bit_buffer_sse2_ = reinterpret_cast<uint16_t *>(
+        memset(out_16bit_buffer_c_, 0, sizeof(uint16_t) * test_size_);
+
+#ifdef ARCH_X86_64
+        out_16bit_buffer_avx2_ = reinterpret_cast<uint16_t *>(
             svt_aom_memalign(32, sizeof(uint16_t) * test_size_));
         memset(out_16bit_buffer_avx2_, 0, sizeof(uint16_t) * test_size_);
-        memset(out_16bit_buffer_c_, 0, sizeof(uint16_t) * test_size_);
+        out_16bit_buffer_sse2_ = reinterpret_cast<uint16_t *>(
+            svt_aom_memalign(32, sizeof(uint16_t) * test_size_));
         memset(out_16bit_buffer_sse2_, 0, sizeof(uint16_t) * test_size_);
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+        out_16bit_buffer_neon_ = reinterpret_cast<uint16_t *>(
+            svt_aom_memalign(32, sizeof(uint16_t) * test_size_));
+        memset(out_16bit_buffer_neon_, 0, sizeof(uint16_t) * test_size_);
+#endif  // ARCH_AARCH64
     }
 
     void TearDown() override {
@@ -509,12 +580,22 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
             svt_aom_free(in_8bit_buffer_);
         if (inn_bit_buffer_)
             svt_aom_free(inn_bit_buffer_);
-        if (out_16bit_buffer_avx2_)
-            svt_aom_free(out_16bit_buffer_avx2_);
+
         if (out_16bit_buffer_c_)
             svt_aom_free(out_16bit_buffer_c_);
+
+#ifdef ARCH_X86_64
+        if (out_16bit_buffer_avx2_)
+            svt_aom_free(out_16bit_buffer_avx2_);
         if (out_16bit_buffer_sse2_)
             svt_aom_free(out_16bit_buffer_sse2_);
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+        if (out_16bit_buffer_neon_)
+            svt_aom_free(out_16bit_buffer_neon_);
+#endif  // ARCH_AARCH64
+
         aom_clear_system_state();
     }
 
@@ -538,14 +619,6 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
             svt_buf_random_u8(in_8bit_buffer_, test_size_);
             svt_buf_random_u8(inn_bit_buffer_, test_size_);
 
-            svt_enc_msb_pack2d_avx2_intrin_al(in_8bit_buffer_,
-                                              in_stride_,
-                                              inn_bit_buffer_,
-                                              out_16bit_buffer_avx2_,
-                                              out_stride_,
-                                              out_stride_,
-                                              area_width_,
-                                              area_height_);
             svt_enc_msb_pack2_d(in_8bit_buffer_,
                                 in_stride_,
                                 inn_bit_buffer_,
@@ -554,6 +627,22 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
                                 out_stride_,
                                 area_width_,
                                 area_height_);
+
+#ifdef ARCH_X86_64
+
+            svt_enc_msb_pack2d_avx2_intrin_al(in_8bit_buffer_,
+                                              in_stride_,
+                                              inn_bit_buffer_,
+                                              out_16bit_buffer_avx2_,
+                                              out_stride_,
+                                              out_stride_,
+                                              area_width_,
+                                              area_height_);
+            check_output(area_width_,
+                         area_height_,
+                         out_16bit_buffer_avx2_,
+                         out_16bit_buffer_c_);
+
             svt_enc_msb_pack2d_sse2_intrin(in_8bit_buffer_,
                                            in_stride_,
                                            inn_bit_buffer_,
@@ -565,13 +654,28 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
 
             check_output(area_width_,
                          area_height_,
-                         out_16bit_buffer_avx2_,
+                         out_16bit_buffer_sse2_,
                          out_16bit_buffer_c_);
+
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+
+            svt_enc_msb_pack2d_neon(in_8bit_buffer_,
+                                    in_stride_,
+                                    inn_bit_buffer_,
+                                    out_16bit_buffer_neon_,
+                                    out_stride_,
+                                    out_stride_,
+                                    area_width_,
+                                    area_height_);
             check_output(area_width_,
                          area_height_,
-                         out_16bit_buffer_sse2_,
+                         out_16bit_buffer_neon_,
                          out_16bit_buffer_c_);
 
+#endif  // ARCH_AARCH64
+
             EXPECT_FALSE(HasFailure())
                 << "svt_enc_msb_pack2d_{sse2,avx2}_intrin failed at " << i
                 << "th test with size (" << area_width_ << "," << area_height_
@@ -581,8 +685,18 @@ class Pack2dTest : public ::testing::TestWithParam<AreaSize> {
 
     uint8_t *in_8bit_buffer_, *inn_bit_buffer_;
     uint32_t in_stride_, out_stride_;
-    uint16_t *out_16bit_buffer_avx2_, *out_16bit_buffer_c_,
-        *out_16bit_buffer_sse2_;
+
+    uint16_t *out_16bit_buffer_c_;
+
+#ifdef ARCH_X86_64
+    uint16_t *out_16bit_buffer_avx2_;
+    uint16_t *out_16bit_buffer_sse2_;
+#endif
+
+#ifdef ARCH_AARCH64
+    uint16_t *out_16bit_buffer_neon_;
+#endif
+
     uint32_t area_width_, area_height_;
     uint32_t test_size_;
 };
@@ -594,6 +708,8 @@ TEST_P(Pack2dTest, Pack2dTest) {
 INSTANTIATE_TEST_SUITE_P(PACK2D, Pack2dTest,
                          ::testing::ValuesIn(TEST_COMMON_SIZES));
 
+#ifdef ARCH_X86_64
+
 // test svt_enc_un_pack8_bit_data_avx2_intrin
 // Similar assumption that the width is multiple of 4, using
 // TEST_COMMON_SIZES to cover all the special width.
@@ -774,6 +890,10 @@ TEST_P(UnPackTest, UnPack2dTest) {
 INSTANTIATE_TEST_SUITE_P(UNPACK, UnPackTest,
                          ::testing::ValuesIn(TEST_COMMON_SIZES));
 
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_X86_64
+
 // test svt_unpack_avg_avx2_intrin
 // only width of {4, 8, 16, 32, 64} are implemented in
 // svt_unpack_avg_avx2_intrin. only width of {8, 16, 32, 64} are implemented in
@@ -967,4 +1087,6 @@ TEST_P(UnPackAvgTest, UnPackSubAvgTest) {
 INSTANTIATE_TEST_SUITE_P(UNPACKAVG, UnPackAvgTest,
                          ::testing::ValuesIn(TEST_AVG_SIZES));
 
+#endif  // ARCH_X86_64
+
 }  // namespace
-- 
GitLab


From 98d54ef9fc8a6262244f4f9adb189c92f083817c Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Wed, 17 Apr 2024 19:07:54 +0000
Subject: [PATCH 03/13] NEON port of svt_estimate_noise_fp16_c

---
 .../ASM_NEON/EbTemporalFiltering_neon.c       | 147 ++++++++++++++++++
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.c       |   2 +-
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.h       |   2 +
 test/TemporalFilterTestPlanewise.cc           |  29 ++++
 4 files changed, 179 insertions(+), 1 deletion(-)

diff --git a/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c b/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c
index 0eee80477..c1156a12e 100644
--- a/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c
@@ -11,6 +11,7 @@
 
 #include <assert.h>
 #include <arm_neon.h>
+
 #include "EbDefinitions.h"
 #include "EbTemporalFiltering_constants.h"
 #include "EbUtility.h"
@@ -442,3 +443,149 @@ void svt_aom_get_final_filtered_pixels_neon(MeContext *me_ctx, EbByte *src_cente
         }
     }
 }
+
+int32_t svt_estimate_noise_fp16_neon(const uint8_t *src, uint16_t width, uint16_t height, uint16_t stride_y) {
+    int64_t sum = 0;
+    int64_t num = 0;
+
+    //  A | B | C
+    //  D | E | F
+    //  G | H | I
+    // g_x = (A - I) + (G - C) + 2*(D - F)
+    // g_y = (A - I) - (G - C) + 2*(B - H)
+    // v   = 4*E - 2*(D+F+B+H) + (A+C+G+I)
+
+    const int16x8_t zero               = vdupq_n_s16(0);
+    const int16x8_t edge_treshold      = vdupq_n_s16(EDGE_THRESHOLD);
+    int32x4_t       num_accumulator_lo = vdupq_n_s32(0);
+    int32x4_t       num_accumulator_hi = vdupq_n_s32(0);
+    int32x4_t       sum_accumulator_lo = vdupq_n_s32(0);
+    int32x4_t       sum_accumulator_hi = vdupq_n_s32(0);
+
+    for (int i = 1; i < height - 1; ++i) {
+        const int k_stride = i * stride_y;
+
+        int j = 1;
+
+        int16x8_t rowAC = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(&src[k_stride + j - stride_y - 1 + 0])));
+        int16x8_t rowBC = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(&src[k_stride + j - 1 + 0])));
+        int16x8_t rowCC = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(&src[k_stride + j + stride_y - 1 + 0])));
+
+        for (; j + 16 < width - 1; j += 16) {
+            const uint8x16_t rowABC = vld1q_u8(&src[k_stride + j - stride_y - 1 + 8]);
+            const uint8x16_t rowBBC = vld1q_u8(&src[k_stride + j - 1 + 8]);
+            const uint8x16_t rowCBC = vld1q_u8(&src[k_stride + j + stride_y - 1 + 8]);
+
+            const int16x8_t rowAA = rowAC;
+            const int16x8_t rowAB = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(rowABC)));
+            rowAC                 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(rowABC)));
+            const int16x8_t rowBA = rowBC;
+            const int16x8_t rowBB = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(rowBBC)));
+            rowBC                 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(rowBBC)));
+            const int16x8_t rowCA = rowCC;
+            const int16x8_t rowCB = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(rowCBC)));
+            rowCC                 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(rowCBC)));
+
+            const int16x8_t A_lo = rowAA;
+            const int16x8_t A_hi = rowAB;
+            const int16x8_t B_lo = vextq_s16(rowAA, rowAB, 1);
+            const int16x8_t B_hi = vextq_s16(rowAB, rowAC, 1);
+            const int16x8_t C_lo = vextq_s16(rowAA, rowAB, 2);
+            const int16x8_t C_hi = vextq_s16(rowAB, rowAC, 2);
+
+            const int16x8_t D_lo = rowBA;
+            const int16x8_t D_hi = rowBB;
+            const int16x8_t E_lo = vextq_s16(rowBA, rowBB, 1);
+            const int16x8_t E_hi = vextq_s16(rowBB, rowBC, 1);
+            const int16x8_t F_lo = vextq_s16(rowBA, rowBB, 2);
+            const int16x8_t F_hi = vextq_s16(rowBB, rowBC, 2);
+
+            const int16x8_t G_lo = rowCA;
+            const int16x8_t G_hi = rowCB;
+            const int16x8_t H_lo = vextq_s16(rowCA, rowCB, 1);
+            const int16x8_t H_hi = vextq_s16(rowCB, rowCC, 1);
+            const int16x8_t I_lo = vextq_s16(rowCA, rowCB, 2);
+            const int16x8_t I_hi = vextq_s16(rowCB, rowCC, 2);
+
+            const int16x8_t A_m_I_lo = vsubq_s16(A_lo, I_lo);
+            const int16x8_t A_m_I_hi = vsubq_s16(A_hi, I_hi);
+            const int16x8_t G_m_C_lo = vsubq_s16(G_lo, C_lo);
+            const int16x8_t G_m_C_hi = vsubq_s16(G_hi, C_hi);
+
+            const int16x8_t D_m_Fx2_lo = vshlq_n_s16(vsubq_s16(D_lo, F_lo), 1);
+            const int16x8_t D_m_Fx2_hi = vshlq_n_s16(vsubq_s16(D_hi, F_hi), 1);
+            const int16x8_t B_m_Hx2_lo = vshlq_n_s16(vsubq_s16(B_lo, H_lo), 1);
+            const int16x8_t B_m_Hx2_hi = vshlq_n_s16(vsubq_s16(B_hi, H_hi), 1);
+
+            const int16x8_t gx_256_lo = vqabsq_s16(vaddq_s16(vaddq_s16(A_m_I_lo, G_m_C_lo), D_m_Fx2_lo));
+            const int16x8_t gx_256_hi = vqabsq_s16(vaddq_s16(vaddq_s16(A_m_I_hi, G_m_C_hi), D_m_Fx2_hi));
+
+            const int16x8_t gy_256_lo = vqabsq_s16(vaddq_s16(vsubq_s16(A_m_I_lo, G_m_C_lo), B_m_Hx2_lo));
+            const int16x8_t gy_256_hi = vqabsq_s16(vaddq_s16(vsubq_s16(A_m_I_hi, G_m_C_hi), B_m_Hx2_hi));
+
+            const int16x8_t ga_256_lo = vaddq_s16(gx_256_lo, gy_256_lo);
+            const int16x8_t ga_256_hi = vaddq_s16(gx_256_hi, gy_256_hi);
+
+            const int16x8_t D_F_B_Hx2_lo = vshlq_n_s16(vaddq_s16(vaddq_s16(D_lo, F_lo), vaddq_s16(B_lo, H_lo)), 1);
+            const int16x8_t D_F_B_Hx2_hi = vshlq_n_s16(vaddq_s16(vaddq_s16(D_hi, F_hi), vaddq_s16(B_hi, H_hi)), 1);
+            const int16x8_t A_C_G_I_lo   = vaddq_s16(vaddq_s16(A_lo, C_lo), vaddq_s16(G_lo, I_lo));
+            const int16x8_t A_C_G_I_hi   = vaddq_s16(vaddq_s16(A_hi, C_hi), vaddq_s16(G_hi, I_hi));
+            int16x8_t       v_256_lo = vqabsq_s16(vaddq_s16(vsubq_s16(vshlq_n_s16(E_lo, 2), D_F_B_Hx2_lo), A_C_G_I_lo));
+            int16x8_t       v_256_hi = vqabsq_s16(vaddq_s16(vsubq_s16(vshlq_n_s16(E_hi, 2), D_F_B_Hx2_hi), A_C_G_I_hi));
+
+            //if (ga < EDGE_THRESHOLD)
+            const int16x8_t cmp_lo = vreinterpretq_s16_u16(vshrq_n_u16(vcgtq_s16(edge_treshold, ga_256_lo), 15));
+            const int16x8_t cmp_hi = vreinterpretq_s16_u16(vshrq_n_u16(vcgtq_s16(edge_treshold, ga_256_hi), 15));
+            v_256_lo               = vmulq_s16(v_256_lo, cmp_lo);
+            v_256_hi               = vmulq_s16(v_256_hi, cmp_hi);
+
+            //num_accumulator and sum_accumulator have 32bit values
+            num_accumulator_lo = vaddq_s32(num_accumulator_lo, vreinterpretq_s32_s16(vzip1q_s16(cmp_lo, zero)));
+            num_accumulator_hi = vaddq_s32(num_accumulator_hi, vreinterpretq_s32_s16(vzip1q_s16(cmp_hi, zero)));
+            num_accumulator_lo = vaddq_s32(num_accumulator_lo, vreinterpretq_s32_s16(vzip2q_s16(cmp_lo, zero)));
+            num_accumulator_hi = vaddq_s32(num_accumulator_hi, vreinterpretq_s32_s16(vzip2q_s16(cmp_hi, zero)));
+
+            sum_accumulator_lo = vaddq_s32(sum_accumulator_lo, vreinterpretq_s32_s16(vzip1q_s16(v_256_lo, zero)));
+            sum_accumulator_hi = vaddq_s32(sum_accumulator_hi, vreinterpretq_s32_s16(vzip1q_s16(v_256_hi, zero)));
+            sum_accumulator_lo = vaddq_s32(sum_accumulator_lo, vreinterpretq_s32_s16(vzip2q_s16(v_256_lo, zero)));
+            sum_accumulator_hi = vaddq_s32(sum_accumulator_hi, vreinterpretq_s32_s16(vzip2q_s16(v_256_hi, zero)));
+        }
+
+        for (; j < width - 1; ++j) {
+            const int k = i * stride_y + j;
+
+            // Sobel gradients
+            const int g_x = (src[k - stride_y - 1] - src[k - stride_y + 1]) +
+                (src[k + stride_y - 1] - src[k + stride_y + 1]) + 2 * (src[k - 1] - src[k + 1]);
+            const int g_y = (src[k - stride_y - 1] - src[k + stride_y - 1]) +
+                (src[k - stride_y + 1] - src[k + stride_y + 1]) + 2 * (src[k - stride_y] - src[k + stride_y]);
+            const int ga = abs(g_x) + abs(g_y);
+
+            if (ga < EDGE_THRESHOLD) { // Do not consider edge pixels to estimate the noise
+                // Find Laplacian
+                const int v = 4 * src[k] - 2 * (src[k - 1] + src[k + 1] + src[k - stride_y] + src[k + stride_y]) +
+                    (src[k - stride_y - 1] + src[k - stride_y + 1] + src[k + stride_y - 1] + src[k + stride_y + 1]);
+                sum += abs(v);
+                ++num;
+            }
+        }
+    }
+
+    int32x4_t sum_256_lo = vpaddq_s32(sum_accumulator_lo, num_accumulator_lo);
+    int32x4_t sum_256_hi = vpaddq_s32(sum_accumulator_hi, num_accumulator_hi);
+    sum_256_lo           = vpaddq_s32(sum_256_lo, sum_256_lo);
+    sum_256_hi           = vpaddq_s32(sum_256_hi, sum_256_hi);
+
+    int32x4_t sum_128 = vaddq_s32(sum_256_lo, sum_256_hi);
+
+    sum += vgetq_lane_s32(sum_128, 0);
+    num += vgetq_lane_s32(vextq_s32(sum_128, vdupq_n_s32(0), 1), 0);
+
+    // If very few smooth pels, return -1 since the estimate is unreliable
+    if (num < SMOOTH_THRESHOLD) {
+        return -65536 /*-1:fp16*/;
+    }
+
+    FP_ASSERT((((int64_t)sum * SQRT_PI_BY_2_FP16) / (6 * num)) < ((int64_t)1 << 31));
+    return (int32_t)((sum * SQRT_PI_BY_2_FP16) / (6 * num));
+}
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
index cf9f6337f..c271ce9ae 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
@@ -857,7 +857,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(variance_highbd, svt_aom_variance_highbd_c);
     SET_ONLY_C(svt_av1_haar_ac_sad_8x8_uint8_input, svt_av1_haar_ac_sad_8x8_uint8_input_c);
     SET_NEON(svt_unpack_and_2bcompress, svt_unpack_and_2bcompress_c, svt_unpack_and_2bcompress_neon);
-    SET_ONLY_C(svt_estimate_noise_fp16, svt_estimate_noise_fp16_c);
+    SET_NEON(svt_estimate_noise_fp16, svt_estimate_noise_fp16_c, svt_estimate_noise_fp16_neon);
     SET_ONLY_C(svt_estimate_noise_highbd_fp16, svt_estimate_noise_highbd_fp16_c);
     SET_ONLY_C(svt_copy_mi_map_grid, svt_copy_mi_map_grid_c);
     SET_ONLY_C(svt_av1_add_block_observations_internal, svt_av1_add_block_observations_internal_c);
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
index 927c6935d..1b8517969 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
@@ -1116,6 +1116,8 @@ extern "C" {
 
     void svt_av1_get_nz_map_contexts_neon(const uint8_t *const levels, const int16_t *const scan, const uint16_t eob, TxSize tx_size, const TxClass tx_class, int8_t *const coeff_contexts);
 
+    int32_t svt_estimate_noise_fp16_neon(const uint8_t *src, uint16_t width, uint16_t height, uint16_t stride_y);
+
     void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t input_stride, TxType transform_type, uint8_t  bit_depth);
     void svt_av1_fwd_txfm2d_32x32_N4_neon(int16_t *input, int32_t *output, uint32_t stride, TxType tx_type, uint8_t bd);
     void svt_av1_fwd_txfm2d_64x64_N4_neon(int16_t *input, int32_t *output, uint32_t stride, TxType tx_type, uint8_t bd);
diff --git a/test/TemporalFilterTestPlanewise.cc b/test/TemporalFilterTestPlanewise.cc
index 24ef331fa..5cf6aefa2 100644
--- a/test/TemporalFilterTestPlanewise.cc
+++ b/test/TemporalFilterTestPlanewise.cc
@@ -1331,6 +1331,23 @@ int32_t estimate_noise_fp16_avx2_wrapper(const uint16_t *src, int width,
 
 #endif
 
+#ifdef ARCH_AARCH64
+
+int32_t estimate_noise_fp16_c_wrapper(const uint16_t *src, int width,
+                                      int height, int stride, int bd) {
+    UNUSED(bd);
+    return svt_estimate_noise_fp16_c(
+        (const uint8_t *)src, width, height, stride);
+}
+int32_t estimate_noise_fp16_neon_wrapper(const uint16_t *src, int width,
+                                         int height, int stride, int bd) {
+    UNUSED(bd);
+    return svt_estimate_noise_fp16_neon(
+        (const uint8_t *)src, width, height, stride);
+}
+
+#endif
+
 typedef int32_t (*EstimateNoiseFuncFP)(const uint16_t *src, int width,
                                        int height, int stride, int bd);
 
@@ -1414,6 +1431,18 @@ INSTANTIATE_TEST_SUITE_P(
 
 #endif
 
+#ifdef ARCH_AARCH64
+
+INSTANTIATE_TEST_CASE_P(
+    NEON_lbd, EstimateNoiseTestFP,
+    ::testing::Combine(::testing::Values(estimate_noise_fp16_c_wrapper),
+                       ::testing::Values(estimate_noise_fp16_neon_wrapper),
+                       ::testing::Values(3840, 1920, 1280, 800, 640, 360),
+                       ::testing::Values(2160, 1080, 720, 600, 480, 240),
+                       ::testing::Values(8)));
+
+#endif
+
 typedef double (*EstimateNoiseFuncDbl)(const uint16_t *src, int width,
                                        int height, int stride, int bd);
 
-- 
GitLab


From 5e214fbcbee9b98ba95b98db831fd70ec112ca22 Mon Sep 17 00:00:00 2001
From: Gerardo Puga <glpuga@gmail.com>
Date: Mon, 22 Apr 2024 17:26:11 +0000
Subject: [PATCH 04/13] Port svt_handle_transform64x64_c and others

---
 .../EbPictureOperators_Intrinsic_neon.c       |  32 ++++
 Source/Lib/Common/ASM_NEON/convolve_neon.c    |  61 +++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   4 +
 Source/Lib/Encoder/ASM_NEON/CMakeLists.txt    |   1 +
 .../Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c  |  80 ++++++++++
 .../ASM_NEON/EbTemporalFiltering_neon.c       |  80 ++++++++++
 .../Lib/Encoder/ASM_NEON/EbTransforms_neon.c  | 150 ++++++++++++++++++
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.c       |  28 ++--
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.h       |  25 +++
 test/CMakeLists.txt                           |   7 +-
 test/InvTxfm2dAsmTest.cc                      |  66 ++++++--
 test/SadTest.cc                               |   5 +
 test/SpatialFullDistortionTest.cc             |   8 +
 test/TemporalFilterTestPlanewise.cc           |  14 +-
 test/quantize_func_test.cc                    |  19 ++-
 16 files changed, 548 insertions(+), 34 deletions(-)
 create mode 100644 Source/Lib/Encoder/ASM_NEON/EbTransforms_neon.c

diff --git a/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c b/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
index 73d1fbe93..98af0b0fe 100644
--- a/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
+++ b/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
@@ -930,3 +930,35 @@ void svt_enc_msb_pack2d_neon(uint8_t *in8_bit_buffer, uint32_t in8_stride, uint8
         }
     }
 }
+
+void svt_full_distortion_kernel_cbf_zero32_bits_neon(int32_t *coeff, uint32_t coeff_stride,
+                                                     uint64_t distortion_result[DIST_CALC_TOTAL], uint32_t area_width,
+                                                     uint32_t area_height) {
+    uint64x2_t sum = vdupq_n_u64(0);
+
+    uint32_t row_count = area_height;
+    do {
+        int32_t *coeff_temp = coeff;
+
+        uint32_t col_count = area_width / 4;
+        do {
+            const int32x2_t x_lo = vld1_s32(coeff_temp + 0);
+            const int32x2_t x_hi = vld1_s32(coeff_temp + 2);
+            coeff_temp += 4;
+
+            const uint64x2_t y_lo = vreinterpretq_u64_s64(vmull_s32(x_lo, x_lo));
+            const uint64x2_t y_hi = vreinterpretq_u64_s64(vmull_s32(x_hi, x_hi));
+
+            sum = vaddq_u64(sum, y_lo);
+            sum = vaddq_u64(sum, y_hi);
+
+        } while (--col_count);
+
+        coeff += coeff_stride;
+        row_count -= 1;
+    } while (row_count > 0);
+
+    const uint64x2_t temp2 = vextq_u64(sum, sum, 1);
+    const uint64x2_t temp1 = vaddq_u64(sum, temp2);
+    vst1q_u64(distortion_result, temp1);
+}
diff --git a/Source/Lib/Common/ASM_NEON/convolve_neon.c b/Source/Lib/Common/ASM_NEON/convolve_neon.c
index ac212b73a..9394a3ac4 100644
--- a/Source/Lib/Common/ASM_NEON/convolve_neon.c
+++ b/Source/Lib/Common/ASM_NEON/convolve_neon.c
@@ -15,6 +15,8 @@
 #include "mem_neon.h"
 #include "transpose_neon.h"
 #include "filter.h"
+#include "EbInterPrediction.h"
+#include "EbUtility.h"
 
 #define ROUND0_BITS 3
 
@@ -2124,3 +2126,62 @@ int8_t svt_av1_wedge_sign_from_residuals_neon(const int16_t *ds, const uint8_t *
 
     return (horizontal_add_s64x2(sum) > limit);
 }
+
+uint8_t svt_av1_compute_cul_level_neon(const int16_t *const scan, const int32_t *const quant_coeff, uint16_t *eob) {
+    if (*eob == 1) {
+        if (quant_coeff[0] > 0)
+            return (AOMMIN(COEFF_CONTEXT_MASK, quant_coeff[0]) + (2 << COEFF_CONTEXT_BITS));
+        if (quant_coeff[0] < 0) {
+            return (AOMMIN(COEFF_CONTEXT_MASK, ABS(quant_coeff[0])) | (1 << COEFF_CONTEXT_BITS));
+        }
+        return 0;
+    }
+
+    int32x4_t sum_256_0 = vdupq_n_s32(0);
+    int32x4_t sum_256_1 = vdupq_n_s32(0);
+
+    for (int32_t c = 0; c < *eob; c += 8) {
+        const int16x4_t scan_64_0 = vld1_s16(scan + c + 0);
+        const int16x4_t scan_64_1 = vld1_s16(scan + c + 4);
+
+        const int32x4_t scan_128_0 = vmovl_s16(scan_64_0);
+        const int32x4_t scan_128_1 = vmovl_s16(scan_64_1);
+
+        // no gather operation, unfortunately
+        const int32_t quant_coeff_0 = *(quant_coeff + vgetq_lane_s32(scan_128_0, 0) + 4 * 8);
+        const int32_t quant_coeff_1 = *(quant_coeff + vgetq_lane_s32(scan_128_0, 1) + 4 * 8);
+        const int32_t quant_coeff_2 = *(quant_coeff + vgetq_lane_s32(scan_128_0, 2) + 4 * 8);
+        const int32_t quant_coeff_3 = *(quant_coeff + vgetq_lane_s32(scan_128_0, 3) + 4 * 8);
+
+        const int32_t quant_coeff_4 = *(quant_coeff + vgetq_lane_s32(scan_128_1, 0) + 4 * 8);
+        const int32_t quant_coeff_5 = *(quant_coeff + vgetq_lane_s32(scan_128_1, 1) + 4 * 8);
+        const int32_t quant_coeff_6 = *(quant_coeff + vgetq_lane_s32(scan_128_1, 2) + 4 * 8);
+        const int32_t quant_coeff_7 = *(quant_coeff + vgetq_lane_s32(scan_128_1, 3) + 4 * 8);
+
+        int32x4_t quant_coeff_128_0 = vcombine_s32(
+            vcreate_s32((((uint64_t)quant_coeff_1) << 32) + (uint64_t)quant_coeff_0),
+            vcreate_s32((((uint64_t)quant_coeff_3) << 32) + (uint64_t)quant_coeff_2));
+        int32x4_t quant_coeff_128_1 = vcombine_s32(
+            vcreate_s32((((uint64_t)quant_coeff_5) << 32) + (uint64_t)quant_coeff_4),
+            vcreate_s32((((uint64_t)quant_coeff_7) << 32) + (uint64_t)quant_coeff_6));
+
+        quant_coeff_128_0 = vabsq_s32(quant_coeff_128_0);
+        quant_coeff_128_1 = vabsq_s32(quant_coeff_128_1);
+
+        sum_256_0 = vaddq_s32(sum_256_0, quant_coeff_128_0);
+        sum_256_1 = vaddq_s32(sum_256_1, quant_coeff_128_1);
+    }
+
+    int32x4_t partial_sums = vaddq_s32(sum_256_0, sum_256_1);
+    partial_sums           = vpaddq_s32(partial_sums, partial_sums);
+    partial_sums           = vpaddq_s32(partial_sums, partial_sums);
+
+    const int32_t cul_level = AOMMIN(COEFF_CONTEXT_MASK, vgetq_lane_s32(partial_sums, 0));
+
+    // DC value, calculation from set_dc_sign()
+    if (quant_coeff[0] < 0)
+        return (cul_level | (1 << COEFF_CONTEXT_BITS));
+    if (quant_coeff[0] > 0)
+        return (cul_level + (2 << COEFF_CONTEXT_BITS));
+    return (uint8_t)cul_level;
+}
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 37e8316ec..599861a83 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -877,7 +877,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_convert_16bit_to_8bit, svt_convert_16bit_to_8bit_c);
     SET_NEON(svt_pack2d_16_bit_src_mul4, svt_enc_msb_pack2_d, svt_enc_msb_pack2d_neon);
     SET_ONLY_C(svt_aom_un_pack2d_16_bit_src_mul4, svt_enc_msb_un_pack2_d);
-    SET_ONLY_C(svt_full_distortion_kernel_cbf_zero32_bits, svt_full_distortion_kernel_cbf_zero32_bits_c);
+    SET_NEON(svt_full_distortion_kernel_cbf_zero32_bits, svt_full_distortion_kernel_cbf_zero32_bits_c, svt_full_distortion_kernel_cbf_zero32_bits_neon);
     SET_NEON(svt_full_distortion_kernel32_bits, svt_full_distortion_kernel32_bits_c, svt_full_distortion_kernel32_bits_neon);
     SET_NEON(svt_spatial_full_distortion_kernel, svt_spatial_full_distortion_kernel_c, svt_spatial_full_distortion_kernel_neon);
     SET_ONLY_C(svt_full_distortion_kernel16_bits, svt_full_distortion_kernel16_bits_c);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index d0fd46c8e..e7031106f 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1366,6 +1366,10 @@ extern "C" {
 
     void svt_full_distortion_kernel32_bits_neon(int32_t *coeff, uint32_t coeff_stride, int32_t *recon_coeff, uint32_t recon_coeff_stride, uint64_t distortion_result[DIST_CALC_TOTAL], uint32_t area_width, uint32_t area_height);
 
+    void svt_full_distortion_kernel_cbf_zero32_bits_neon(int32_t *coeff, uint32_t coeff_stride,
+                                                         uint64_t distortion_result[DIST_CALC_TOTAL], uint32_t area_width,
+                                                         uint32_t area_height);
+
     void svt_av1_inv_txfm2d_add_16x16_neon(const int32_t *input, uint16_t *output_r, int32_t stride_r, uint16_t *output_w, int32_t stride_w, TxType tx_type, int32_t bd);
     void svt_av1_inv_txfm2d_add_32x32_neon(const int32_t *input, uint16_t *output_r, int32_t stride_r, uint16_t *output_w, int32_t stride_w, TxType tx_type, int32_t bd);
     void svt_av1_inv_txfm2d_add_64x64_neon(const int32_t *input, uint16_t *output_r, int32_t stride_r, uint16_t *output_w, int32_t stride_w, TxType tx_type, int32_t bd);
diff --git a/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt b/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt
index 079e4e22d..d9c1d8e87 100644
--- a/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt
@@ -20,6 +20,7 @@ target_sources(
   PUBLIC EbComputeSAD_neon.c
   PUBLIC EbCdef_neon.c
   PUBLIC EbTemporalFiltering_neon.c
+  PUBLIC EbTransforms_neon.c
   PUBLIC hadmard_path_neon.c
   PUBLIC highbd_fwd_txfm_neon.c
   PUBLIC obmc_sad_neon.c
diff --git a/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c
index 3ae761c7a..4865fcab3 100644
--- a/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c
@@ -1703,3 +1703,83 @@ uint32_t svt_nxm_sad_kernel_helper_neon(const uint8_t *src, uint32_t src_stride,
     }
     return res;
 }
+
+void svt_ext_eight_sad_calculation_32x32_64x64_neon(uint32_t p_sad16x16[16][8], uint32_t *p_best_sad_32x32,
+                                                    uint32_t *p_best_sad_64x64, uint32_t *p_best_mv32x32,
+                                                    uint32_t *p_best_mv64x64, uint32_t mv, uint32_t p_sad32x32[4][8]) {
+    uint32x4_t       tmp0     = vaddq_u32(vld1q_u32(p_sad16x16[0]), vld1q_u32(p_sad16x16[1]));
+    uint32x4_t       tmp1     = vaddq_u32(vld1q_u32(p_sad16x16[2]), vld1q_u32(p_sad16x16[3]));
+    const uint32x4_t sad32_a1 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32(p_sad32x32[0], sad32_a1);
+
+    tmp0                      = vaddq_u32(vld1q_u32((p_sad16x16[0] + 4)), vld1q_u32((p_sad16x16[1] + 4)));
+    tmp1                      = vaddq_u32(vld1q_u32((p_sad16x16[2] + 4)), vld1q_u32((p_sad16x16[3] + 4)));
+    const uint32x4_t sad32_a2 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32((p_sad32x32[0] + 4), sad32_a2);
+
+    tmp0                      = vaddq_u32(vld1q_u32(p_sad16x16[4]), vld1q_u32(p_sad16x16[5]));
+    tmp1                      = vaddq_u32(vld1q_u32(p_sad16x16[6]), vld1q_u32(p_sad16x16[7]));
+    const uint32x4_t sad32_b1 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32(p_sad32x32[1], sad32_b1);
+
+    tmp0                      = vaddq_u32(vld1q_u32((p_sad16x16[4] + 4)), vld1q_u32((p_sad16x16[5] + 4)));
+    tmp1                      = vaddq_u32(vld1q_u32((p_sad16x16[6] + 4)), vld1q_u32((p_sad16x16[7] + 4)));
+    const uint32x4_t sad32_b2 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32((p_sad32x32[1] + 4), sad32_b2);
+
+    tmp0                      = vaddq_u32(vld1q_u32(p_sad16x16[8]), vld1q_u32(p_sad16x16[9]));
+    tmp1                      = vaddq_u32(vld1q_u32(p_sad16x16[10]), vld1q_u32(p_sad16x16[11]));
+    const uint32x4_t sad32_c1 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32(p_sad32x32[2], sad32_c1);
+
+    tmp0                      = vaddq_u32(vld1q_u32((p_sad16x16[8] + 4)), vld1q_u32((p_sad16x16[9] + 4)));
+    tmp1                      = vaddq_u32(vld1q_u32((p_sad16x16[10] + 4)), vld1q_u32((p_sad16x16[11] + 4)));
+    const uint32x4_t sad32_c2 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32((p_sad32x32[2] + 4), sad32_c2);
+
+    tmp0                      = vaddq_u32(vld1q_u32(p_sad16x16[12]), vld1q_u32(p_sad16x16[13]));
+    tmp1                      = vaddq_u32(vld1q_u32(p_sad16x16[14]), vld1q_u32(p_sad16x16[15]));
+    const uint32x4_t sad32_d1 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32(p_sad32x32[3], sad32_d1);
+
+    tmp0                      = vaddq_u32(vld1q_u32((p_sad16x16[12] + 4)), vld1q_u32((p_sad16x16[13] + 4)));
+    tmp1                      = vaddq_u32(vld1q_u32((p_sad16x16[14] + 4)), vld1q_u32((p_sad16x16[15] + 4)));
+    const uint32x4_t sad32_d2 = vaddq_u32(tmp0, tmp1);
+    vst1q_u32((p_sad32x32[3] + 4), sad32_d2);
+
+    DECLARE_ALIGNED(32, uint32_t, p_sad64x64[8]);
+    tmp0 = vaddq_u32(sad32_a1, sad32_b1);
+    tmp1 = vaddq_u32(sad32_c1, sad32_d1);
+    vst1q_u32(p_sad64x64, vaddq_u32(tmp0, tmp1));
+    tmp0 = vaddq_u32(sad32_a2, sad32_b2);
+    tmp1 = vaddq_u32(sad32_c2, sad32_d2);
+    vst1q_u32((p_sad64x64 + 4), vaddq_u32(tmp0, tmp1));
+
+    DECLARE_ALIGNED(32, uint32_t, computed_idx[8]);
+    uint32x4_t       search_idx = vmovl_u16(vcreate_u16(0x0003000200010000));
+    const uint32x4_t mv_sse     = vdupq_n_u32(mv);
+    uint32x4_t       new_mv_sse = vaddq_u32(search_idx, mv_sse);
+    new_mv_sse                  = vandq_u32(new_mv_sse, vdupq_n_u32(0xffff));
+    vst1q_u32(computed_idx, vorrq_u32(new_mv_sse, vandq_u32(mv_sse, vdupq_n_u32(0xffff0000))));
+
+    search_idx = vmovl_u16(vcreate_u16(0x0007000600050004));
+    new_mv_sse = vaddq_u32(search_idx, mv_sse);
+    new_mv_sse = vandq_u32(new_mv_sse, vdupq_n_u32(0xffff));
+    vst1q_u32((computed_idx + 4), vorrq_u32(new_mv_sse, vandq_u32(mv_sse, vdupq_n_u32(0xffff0000))));
+
+    for (int i = 0; i < 4; i++) {
+        for (int j = 0; j < 8; j++) {
+            if (p_sad32x32[i][j] < p_best_sad_32x32[i]) {
+                p_best_sad_32x32[i] = p_sad32x32[i][j];
+                p_best_mv32x32[i]   = computed_idx[j];
+            }
+        }
+    }
+
+    for (int j = 0; j < 8; j++) {
+        if (p_sad64x64[j] < p_best_sad_64x64[0]) {
+            p_best_sad_64x64[0] = p_sad64x64[j];
+            p_best_mv64x64[0]   = computed_idx[j];
+        }
+    }
+}
diff --git a/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c b/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c
index c1156a12e..cb32de521 100644
--- a/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/EbTemporalFiltering_neon.c
@@ -589,3 +589,83 @@ int32_t svt_estimate_noise_fp16_neon(const uint8_t *src, uint16_t width, uint16_
     FP_ASSERT((((int64_t)sum * SQRT_PI_BY_2_FP16) / (6 * num)) < ((int64_t)1 << 31));
     return (int32_t)((sum * SQRT_PI_BY_2_FP16) / (6 * num));
 }
+
+static void apply_filtering_central_loop_lbd(uint16_t w, uint16_t h, uint8_t *src, uint16_t src_stride, uint32_t *accum,
+                                             uint16_t *count) {
+    assert(w % 8 == 0);
+
+    uint32x4_t modifier       = vdupq_n_u32(TF_PLANEWISE_FILTER_WEIGHT_SCALE);
+    uint16x8_t modifier_epi16 = vdupq_n_u16(TF_PLANEWISE_FILTER_WEIGHT_SCALE);
+
+    for (uint16_t k = 0, i = 0; i < h; i++) {
+        for (uint16_t j = 0; j < w; j += 8) {
+            const uint16x8_t src_16 = vmovl_u8(vld1_u8(src + i * src_stride + j));
+
+            vst1q_u32(accum + k + 0, vmulq_u32(modifier, vmovl_u16(vget_low_u16(src_16))));
+            vst1q_u32(accum + k + 4, vmulq_u32(modifier, vmovl_u16(vget_high_u16(src_16))));
+            vst1q_u16(count + k, modifier_epi16);
+
+            k += 8;
+        }
+    }
+}
+
+static void apply_filtering_central_loop_hbd(uint16_t w, uint16_t h, uint16_t *src, uint16_t src_stride,
+                                             uint32_t *accum, uint16_t *count) {
+    assert(w % 8 == 0);
+
+    uint32x4_t modifier       = vdupq_n_u32(TF_PLANEWISE_FILTER_WEIGHT_SCALE);
+    uint16x8_t modifier_epi16 = vdupq_n_u16(TF_PLANEWISE_FILTER_WEIGHT_SCALE);
+
+    for (uint16_t k = 0, i = 0; i < h; i++) {
+        for (uint16_t j = 0; j < w; j += 8) {
+            const uint32x4_t src_1 = vmovl_u16(vld1_u16(src + i * src_stride + j + 0));
+            const uint32x4_t src_2 = vmovl_u16(vld1_u16(src + i * src_stride + j + 4));
+
+            vst1q_u32(accum + k + 0, vmulq_u32(modifier, src_1));
+            vst1q_u32(accum + k + 4, vmulq_u32(modifier, src_2));
+            vst1q_u16(count + k, modifier_epi16);
+
+            k += 8;
+        }
+    }
+}
+
+void svt_aom_apply_filtering_central_neon(struct MeContext *me_ctx, EbPictureBufferDesc *input_picture_ptr_central,
+                                          EbByte *src, uint32_t **accum, uint16_t **count, uint16_t blk_width,
+                                          uint16_t blk_height, uint32_t ss_x, uint32_t ss_y) {
+    uint16_t src_stride_y = input_picture_ptr_central->stride_y;
+
+    // Luma
+    apply_filtering_central_loop_lbd(blk_width, blk_height, src[C_Y], src_stride_y, accum[C_Y], count[C_Y]);
+
+    // Chroma
+    if (me_ctx->tf_chroma) {
+        uint16_t blk_height_ch = blk_height >> ss_y;
+        uint16_t blk_width_ch  = blk_width >> ss_x;
+        uint16_t src_stride_ch = src_stride_y >> ss_x;
+        apply_filtering_central_loop_lbd(blk_width_ch, blk_height_ch, src[C_U], src_stride_ch, accum[C_U], count[C_U]);
+        apply_filtering_central_loop_lbd(blk_width_ch, blk_height_ch, src[C_V], src_stride_ch, accum[C_V], count[C_V]);
+    }
+}
+
+void svt_aom_apply_filtering_central_highbd_neon(struct MeContext    *me_ctx,
+                                                 EbPictureBufferDesc *input_picture_ptr_central, uint16_t **src_16bit,
+                                                 uint32_t **accum, uint16_t **count, uint16_t blk_width,
+                                                 uint16_t blk_height, uint32_t ss_x, uint32_t ss_y) {
+    uint16_t src_stride_y = input_picture_ptr_central->stride_y;
+
+    // Luma
+    apply_filtering_central_loop_hbd(blk_width, blk_height, src_16bit[C_Y], src_stride_y, accum[C_Y], count[C_Y]);
+
+    // Chroma
+    if (me_ctx->tf_chroma) {
+        uint16_t blk_height_ch = blk_height >> ss_y;
+        uint16_t blk_width_ch  = blk_width >> ss_x;
+        uint16_t src_stride_ch = src_stride_y >> ss_x;
+        apply_filtering_central_loop_hbd(
+            blk_width_ch, blk_height_ch, src_16bit[C_U], src_stride_ch, accum[C_U], count[C_U]);
+        apply_filtering_central_loop_hbd(
+            blk_width_ch, blk_height_ch, src_16bit[C_V], src_stride_ch, accum[C_V], count[C_V]);
+    }
+}
diff --git a/Source/Lib/Encoder/ASM_NEON/EbTransforms_neon.c b/Source/Lib/Encoder/ASM_NEON/EbTransforms_neon.c
new file mode 100644
index 000000000..51a72f039
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/EbTransforms_neon.c
@@ -0,0 +1,150 @@
+/*
+ * Copyright(c) 2024 Intel Corporation
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at https://www.aomedia.org/license/software-license. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
+ */
+
+#include "EbDefinitions.h"
+#include "EbMotionEstimation.h"
+
+#include <arm_neon.h>
+
+static INLINE void energy_computation_kernel_neon(const int32_t *const in, uint64x2_t *const sum256) {
+    const int32x4_t  input     = vld1q_s32(in);
+    const int32x2_t  in_lo     = vget_low_s32(input);
+    const int32x2_t  in_hi     = vget_high_s32(input);
+    const uint64x2_t energy_lo = vreinterpretq_u64_s64(vmull_s32(in_lo, in_lo));
+    const uint64x2_t energy_hi = vreinterpretq_u64_s64(vmull_s32(in_hi, in_hi));
+    *sum256                    = vaddq_u64(*sum256, vaddq_u64(energy_lo, energy_hi));
+}
+
+static INLINE uint64_t hadd64_neon(const uint64x2_t sum256) {
+    const uint64x2_t partial_sum = vpaddq_u64(sum256, sum256);
+    return vgetq_lane_u64(partial_sum, 0);
+}
+
+static INLINE uint64_t energy_computation_neon(const int32_t *const in, const uint32_t size) {
+    uint64x2_t sum = vdupq_n_u64(0);
+    uint32_t   i   = 0;
+
+    do {
+        energy_computation_kernel_neon(in + i + 0, &sum);
+        energy_computation_kernel_neon(in + i + 4, &sum);
+        i += 8;
+    } while (i < size);
+
+    return hadd64_neon(sum);
+}
+
+static INLINE uint64_t energy_computation_64_neon(const int32_t *in, const uint32_t height) {
+    uint64x2_t sum = vdupq_n_u64(0);
+    uint32_t   i   = height;
+
+    do {
+        energy_computation_kernel_neon(in + 0 * 8 + 0, &sum);
+        energy_computation_kernel_neon(in + 0 * 8 + 4, &sum);
+        energy_computation_kernel_neon(in + 1 * 8 + 0, &sum);
+        energy_computation_kernel_neon(in + 1 * 8 + 4, &sum);
+        energy_computation_kernel_neon(in + 2 * 8 + 0, &sum);
+        energy_computation_kernel_neon(in + 2 * 8 + 4, &sum);
+        energy_computation_kernel_neon(in + 3 * 8 + 0, &sum);
+        energy_computation_kernel_neon(in + 3 * 8 + 4, &sum);
+        in += 64;
+    } while (--i);
+
+    return hadd64_neon(sum);
+}
+
+static INLINE void copy_32_bytes_neon(const int32_t *src, int32_t *dst) {
+    const int32x4x2_t val = vld2q_s32(src + 0 * 8);
+    vst2q_s32(dst + 0 * 8, val);
+}
+
+static INLINE void copy_256x_bytes_neon(const int32_t *src, int32_t *dst, const uint32_t height) {
+    uint32_t h = height;
+
+    do {
+        copy_32_bytes_neon(src + 0 * 8, dst + 0 * 8);
+        copy_32_bytes_neon(src + 1 * 8, dst + 1 * 8);
+        copy_32_bytes_neon(src + 2 * 8, dst + 2 * 8);
+        copy_32_bytes_neon(src + 3 * 8, dst + 3 * 8);
+        src += 64;
+        dst += 32;
+    } while (--h);
+}
+
+uint64_t svt_handle_transform16x64_neon(int32_t *output) {
+    //bottom 16x32 area.
+    const uint64_t three_quad_energy = energy_computation_neon(output + 16 * 32, 16 * 32);
+    return three_quad_energy;
+}
+
+uint64_t svt_handle_transform32x64_neon(int32_t *output) {
+    //bottom 32x32 area.
+    const uint64_t three_quad_energy = energy_computation_neon(output + 32 * 32, 32 * 32);
+    return three_quad_energy;
+}
+
+uint64_t svt_handle_transform64x16_neon(int32_t *output) {
+    // top - right 32x16 area.
+    const uint64_t three_quad_energy = energy_computation_64_neon(output + 32, 16);
+    // Re-pack non-zero coeffs in the first 32x16 indices.
+    copy_256x_bytes_neon(output + 64, output + 32, 15);
+
+    return three_quad_energy;
+}
+
+uint64_t svt_handle_transform64x32_neon(int32_t *output) {
+    // top - right 32x32 area.
+    const uint64_t three_quad_energy = energy_computation_64_neon(output + 32, 32);
+    // Re-pack non-zero coeffs in the first 32x32 indices.
+    copy_256x_bytes_neon(output + 64, output + 32, 31);
+
+    return three_quad_energy;
+}
+
+uint64_t svt_handle_transform64x64_neon(int32_t *output) {
+    uint64_t three_quad_energy;
+
+    // top - right 32x32 area.
+    three_quad_energy = energy_computation_64_neon(output + 32, 32);
+    //bottom 64x32 area.
+    three_quad_energy += energy_computation_neon(output + 32 * 64, 64 * 32);
+    // Re-pack non-zero coeffs in the first 32x32 indices.
+    copy_256x_bytes_neon(output + 64, output + 32, 31);
+
+    return three_quad_energy;
+}
+
+uint64_t svt_handle_transform16x64_N2_N4_neon(int32_t *output) {
+    (void)output;
+    return 0;
+}
+
+uint64_t svt_handle_transform32x64_N2_N4_neon(int32_t *output) {
+    (void)output;
+    return 0;
+}
+
+uint64_t svt_handle_transform64x16_N2_N4_neon(int32_t *output) {
+    // Re-pack non-zero coeffs in the first 32x16 indices.
+    copy_256x_bytes_neon(output + 64, output + 32, 15);
+    return 0;
+}
+
+uint64_t svt_handle_transform64x32_N2_N4_neon(int32_t *output) {
+    // Re-pack non-zero coeffs in the first 32x32 indices.
+    copy_256x_bytes_neon(output + 64, output + 32, 31);
+    return 0;
+}
+
+uint64_t svt_handle_transform64x64_N2_N4_neon(int32_t *output) {
+    // Re-pack non-zero coeffs in the first 32x32 indices.
+    copy_256x_bytes_neon(output + 64, output + 32, 31);
+    return 0;
+}
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
index c271ce9ae..11e04818c 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
@@ -766,16 +766,16 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_av1_fwd_txfm2d_64x32, svt_av1_fwd_txfm2d_64x32_c, svt_av1_fwd_txfm2d_64x32_neon);
     SET_NEON(svt_av1_fwd_txfm2d_64x64, svt_av1_transform_two_d_64x64_c, svt_av1_fwd_txfm2d_64x64_neon);
 
-    SET_ONLY_C(svt_handle_transform16x64, svt_handle_transform16x64_c);
-    SET_ONLY_C(svt_handle_transform32x64, svt_handle_transform32x64_c);
-    SET_ONLY_C(svt_handle_transform64x16, svt_handle_transform64x16_c);
-    SET_ONLY_C(svt_handle_transform64x32, svt_handle_transform64x32_c);
-    SET_ONLY_C(svt_handle_transform64x64, svt_handle_transform64x64_c);
-    SET_ONLY_C(svt_handle_transform16x64_N2_N4, svt_handle_transform16x64_N2_N4_c);
-    SET_ONLY_C(svt_handle_transform32x64_N2_N4, svt_handle_transform32x64_N2_N4_c);
-    SET_ONLY_C(svt_handle_transform64x16_N2_N4, svt_handle_transform64x16_N2_N4_c);
-    SET_ONLY_C(svt_handle_transform64x32_N2_N4, svt_handle_transform64x32_N2_N4_c);
-    SET_ONLY_C(svt_handle_transform64x64_N2_N4, svt_handle_transform64x64_N2_N4_c);
+    SET_NEON(svt_handle_transform16x64, svt_handle_transform16x64_c, svt_handle_transform16x64_neon);
+    SET_NEON(svt_handle_transform32x64, svt_handle_transform32x64_c, svt_handle_transform32x64_neon);
+    SET_NEON(svt_handle_transform64x16, svt_handle_transform64x16_c, svt_handle_transform64x16_neon);
+    SET_NEON(svt_handle_transform64x32, svt_handle_transform64x32_c, svt_handle_transform64x32_neon);
+    SET_NEON(svt_handle_transform64x64, svt_handle_transform64x64_c, svt_handle_transform64x64_neon);
+    SET_NEON(svt_handle_transform16x64_N2_N4, svt_handle_transform16x64_N2_N4_c, svt_handle_transform16x64_N2_N4_neon);
+    SET_NEON(svt_handle_transform32x64_N2_N4, svt_handle_transform32x64_N2_N4_c, svt_handle_transform32x64_N2_N4_neon);
+    SET_NEON(svt_handle_transform64x16_N2_N4, svt_handle_transform64x16_N2_N4_c, svt_handle_transform64x16_N2_N4_neon);
+    SET_NEON(svt_handle_transform64x32_N2_N4, svt_handle_transform64x32_N2_N4_c, svt_handle_transform64x32_N2_N4_neon);
+    SET_NEON(svt_handle_transform64x64_N2_N4, svt_handle_transform64x64_N2_N4_c, svt_handle_transform64x64_N2_N4_neon);
     SET_ONLY_C(svt_av1_fwd_txfm2d_4x4_N2, svt_aom_transform_two_d_4x4_N2_c);
     SET_ONLY_C(svt_av1_fwd_txfm2d_4x8_N2, svt_av1_fwd_txfm2d_4x8_N2_c);
     SET_ONLY_C(svt_av1_fwd_txfm2d_4x16_N2, svt_av1_fwd_txfm2d_4x16_N2_c);
@@ -834,13 +834,13 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_av1_apply_temporal_filter_planewise_medium, svt_av1_apply_temporal_filter_planewise_medium_c, svt_av1_apply_temporal_filter_planewise_medium_neon);
     SET_ONLY_C(svt_av1_apply_temporal_filter_planewise_medium_hbd, svt_av1_apply_temporal_filter_planewise_medium_hbd_c);
     SET_NEON(get_final_filtered_pixels, svt_aom_get_final_filtered_pixels_c, svt_aom_get_final_filtered_pixels_neon);
-    SET_ONLY_C(apply_filtering_central, svt_aom_apply_filtering_central_c);
-    SET_ONLY_C(apply_filtering_central_highbd, svt_aom_apply_filtering_central_highbd_c);
+    SET_NEON(apply_filtering_central, svt_aom_apply_filtering_central_c, svt_aom_apply_filtering_central_neon);
+    SET_NEON(apply_filtering_central_highbd, svt_aom_apply_filtering_central_highbd_c, svt_aom_apply_filtering_central_highbd_neon);
     SET_NEON(downsample_2d, svt_aom_downsample_2d_c, svt_aom_downsample_2d_neon);
     SET_NEON(svt_ext_sad_calculation_8x8_16x16, svt_ext_sad_calculation_8x8_16x16_c, svt_ext_sad_calculation_8x8_16x16_neon_intrin);
     SET_ONLY_C(svt_ext_sad_calculation_32x32_64x64, svt_ext_sad_calculation_32x32_64x64_c);
     SET_NEON(svt_ext_all_sad_calculation_8x8_16x16, svt_ext_all_sad_calculation_8x8_16x16_c, svt_ext_all_sad_calculation_8x8_16x16_neon);
-    SET_ONLY_C(svt_ext_eight_sad_calculation_32x32_64x64, svt_ext_eight_sad_calculation_32x32_64x64_c);
+    SET_NEON(svt_ext_eight_sad_calculation_32x32_64x64, svt_ext_eight_sad_calculation_32x32_64x64_c, svt_ext_eight_sad_calculation_32x32_64x64_neon);
     SET_ONLY_C(svt_initialize_buffer_32bits, svt_initialize_buffer_32bits_c);
     SET_NEON(svt_nxm_sad_kernel_sub_sampled, svt_nxm_sad_kernel_helper_c, svt_nxm_sad_kernel_helper_neon);
     SET_NEON(svt_nxm_sad_kernel, svt_nxm_sad_kernel_helper_c, svt_nxm_sad_kernel_helper_neon);
@@ -873,7 +873,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_av1_highbd_down2_symeven, svt_av1_highbd_down2_symeven_c);
     SET_ONLY_C(svt_av1_highbd_resize_plane, svt_av1_highbd_resize_plane_c);
     SET_ONLY_C(svt_av1_resize_plane, svt_av1_resize_plane_c);
-    SET_ONLY_C(svt_av1_compute_cul_level, svt_av1_compute_cul_level_c);
+    SET_NEON(svt_av1_compute_cul_level, svt_av1_compute_cul_level_c, svt_av1_compute_cul_level_neon);
     SET_ONLY_C(svt_ssim_8x8, svt_ssim_8x8_c);
     SET_ONLY_C(svt_ssim_4x4, svt_ssim_4x4_c);
     SET_ONLY_C(svt_ssim_8x8_hbd, svt_ssim_8x8_hbd_c);
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
index 1b8517969..de7a46870 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
@@ -1079,6 +1079,17 @@ extern "C" {
 
     uint32_t svt_nxm_sad_kernel_helper_neon(const uint8_t *src, uint32_t src_stride, const uint8_t *ref, uint32_t ref_stride, uint32_t height, uint32_t width);
 
+    uint64_t svt_handle_transform16x64_neon(int32_t *output);
+    uint64_t svt_handle_transform32x64_neon(int32_t *output);
+    uint64_t svt_handle_transform64x16_neon(int32_t *output);
+    uint64_t svt_handle_transform64x32_neon(int32_t *output);
+    uint64_t svt_handle_transform64x64_neon(int32_t *output);
+    uint64_t svt_handle_transform16x64_N2_N4_neon(int32_t *output);
+    uint64_t svt_handle_transform32x64_N2_N4_neon(int32_t *output);
+    uint64_t svt_handle_transform64x16_N2_N4_neon(int32_t *output);
+    uint64_t svt_handle_transform64x32_N2_N4_neon(int32_t *output);
+    uint64_t svt_handle_transform64x64_N2_N4_neon(int32_t *output);
+
     void svt_av1_fwd_txfm2d_4x4_neon(int16_t *input, int32_t *coeff, uint32_t stride, TxType tx_type, uint8_t bd);
     void svt_av1_fwd_txfm2d_4x8_neon(int16_t *input, int32_t *coeff, uint32_t stride, TxType tx_type, uint8_t bd);
     void svt_av1_fwd_txfm2d_4x16_neon(int16_t *input, int32_t *coeff, uint32_t stride, TxType tx_type, uint8_t bd);
@@ -1125,6 +1136,20 @@ extern "C" {
 
     void svt_av1_apply_temporal_filter_planewise_medium_neon(struct MeContext *me_ctx, const uint8_t *y_src, int y_src_stride, const uint8_t *y_pre, int y_pre_stride, const uint8_t *u_src, const uint8_t *v_src, int uv_src_stride, const uint8_t *u_pre, const uint8_t *v_pre, int uv_pre_stride, unsigned int block_width, unsigned int block_height, int ss_x, int ss_y, uint32_t *y_accum, uint16_t *y_count, uint32_t *u_accum, uint16_t *u_count, uint32_t *v_accum, uint16_t *v_count);
 
+    void svt_ext_eight_sad_calculation_32x32_64x64_neon(uint32_t p_sad16x16[16][8], uint32_t *p_best_sad_32x32,
+                                                        uint32_t *p_best_sad_64x64, uint32_t *p_best_mv32x32,
+                                                        uint32_t *p_best_mv64x64, uint32_t mv, uint32_t p_sad32x32[4][8]);
+
+    uint8_t svt_av1_compute_cul_level_neon(const int16_t *const scan, const int32_t *const quant_coeff, uint16_t *eob);
+
+    void svt_aom_apply_filtering_central_neon(struct MeContext *me_ctx, EbPictureBufferDesc *input_picture_ptr_central,
+                                              EbByte *src, uint32_t **accum, uint16_t **count, uint16_t blk_width,
+                                              uint16_t blk_height, uint32_t ss_x, uint32_t ss_y);
+    void svt_aom_apply_filtering_central_highbd_neon(struct MeContext *me_ctx, EbPictureBufferDesc *input_picture_ptr_central,
+                                                     uint16_t **src_16bit, uint32_t **accum, uint16_t **count,
+                                                     uint16_t blk_width, uint16_t blk_height, uint32_t ss_x,
+                                                     uint32_t ss_y);
+
     int svt_aom_satd_neon(const TranLow *coeff, int length);
 
     int64_t svt_aom_sse_neon(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride, int width, int height);
diff --git a/test/CMakeLists.txt b/test/CMakeLists.txt
index a3ee3c65c..9cad777c9 100644
--- a/test/CMakeLists.txt
+++ b/test/CMakeLists.txt
@@ -87,18 +87,20 @@ set(arch_neutral_files
     ../third_party/aom_dsp/src/entdec.c)
 
 set(multi_arch_files
-    CdefTest.cc
     FwdTxfm2dAsmTest.cc
     InvTxfm2dAsmTest.cc
     PackUnPackTest.cc
     PictureOperatorTest.cc
-    RestorationPickTest.cc
     SadTest.cc
+    selfguided_filter_test.cc
+    CdefTest.cc
+    RestorationPickTest.cc
     SpatialFullDistortionTest.cc
     TemporalFilterTestPlanewise.cc
     convolve_2d_test.cc
     RestorationPickTest.cc
     selfguided_filter_test.cc
+    quantize_func_test.cc
 )
 
 if(HAVE_X86_PLATFORM)
@@ -139,7 +141,6 @@ if(HAVE_X86_PLATFORM)
         intrapred_dr_test.cc
         intrapred_edge_filter_test.cc
         intrapred_test.cc
-        quantize_func_test.cc
         subtract_avg_cfl_test.cc
         warp_filter_test.cc
         warp_filter_test_util.cc
diff --git a/test/InvTxfm2dAsmTest.cc b/test/InvTxfm2dAsmTest.cc
index c1a8d827f..25b1235fd 100644
--- a/test/InvTxfm2dAsmTest.cc
+++ b/test/InvTxfm2dAsmTest.cc
@@ -713,6 +713,8 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
         }
     }
 
+#endif  // ARCH_X86_64
+
     void run_HandleTransform_match_test() {
         using HandleTxfmFunc = uint64_t (*)(int32_t * output);
         const int num_htf_sizes = 10;
@@ -727,6 +729,8 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
             svt_handle_transform64x16_N2_N4_c,
             svt_handle_transform64x32_N2_N4_c,
             svt_handle_transform64x64_N2_N4_c};
+
+#ifdef ARCH_X86_64
         const HandleTxfmFunc htf_asm_funcs[num_htf_sizes] = {
             svt_handle_transform16x64_avx2,
             svt_handle_transform32x64_avx2,
@@ -738,6 +742,22 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
             svt_handle_transform64x16_N2_N4_avx2,
             svt_handle_transform64x32_N2_N4_avx2,
             svt_handle_transform64x64_N2_N4_avx2};
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+        const HandleTxfmFunc htf_asm_funcs[num_htf_sizes] = {
+            svt_handle_transform16x64_neon,
+            svt_handle_transform32x64_neon,
+            svt_handle_transform64x16_neon,
+            svt_handle_transform64x32_neon,
+            svt_handle_transform64x64_neon,
+            svt_handle_transform16x64_N2_N4_neon,
+            svt_handle_transform32x64_N2_N4_neon,
+            svt_handle_transform64x16_N2_N4_neon,
+            svt_handle_transform64x32_N2_N4_neon,
+            svt_handle_transform64x64_N2_N4_neon};
+#endif  // ARCH_AARCH64
+
         DECLARE_ALIGNED(32, int32_t, input[MAX_TX_SQUARE]);
 
         for (int idx = 0; idx < num_htf_sizes; ++idx) {
@@ -772,6 +792,9 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
             svt_handle_transform64x16_N2_N4_c,
             svt_handle_transform64x32_N2_N4_c,
             svt_handle_transform64x64_N2_N4_c};
+
+#ifdef ARCH_X86_64
+        char const *const intrinsic_set_name = "avx2";
         const HandleTxfmFunc htf_asm_funcs[num_htf_sizes] = {
             svt_handle_transform16x64_avx2,
             svt_handle_transform32x64_avx2,
@@ -783,6 +806,23 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
             svt_handle_transform64x16_N2_N4_avx2,
             svt_handle_transform64x32_N2_N4_avx2,
             svt_handle_transform64x64_N2_N4_avx2};
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+        char const *const intrinsic_set_name = "neon";
+        const HandleTxfmFunc htf_asm_funcs[num_htf_sizes] = {
+            svt_handle_transform16x64_neon,
+            svt_handle_transform32x64_neon,
+            svt_handle_transform64x16_neon,
+            svt_handle_transform64x32_neon,
+            svt_handle_transform64x64_neon,
+            svt_handle_transform16x64_N2_N4_neon,
+            svt_handle_transform32x64_N2_N4_neon,
+            svt_handle_transform64x16_N2_N4_neon,
+            svt_handle_transform64x32_N2_N4_neon,
+            svt_handle_transform64x64_N2_N4_neon};
+#endif  // ARCH_AARCH64
+
         DECLARE_ALIGNED(32, int32_t, input[MAX_TX_SQUARE]);
         double time_c, time_o;
         uint64_t start_time_seconds, start_time_useconds;
@@ -790,6 +830,9 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
         uint64_t finish_time_seconds, finish_time_useconds;
 
         for (int idx = 0; idx < num_htf_sizes; ++idx) {
+            if (!htf_asm_funcs[idx]) {
+                continue;
+            }
             const TxSize tx_size = htf_tx_size[idx];
             const uint64_t num_loop = 10000000;
             uint64_t energy_ref, energy_asm;
@@ -830,22 +873,21 @@ class InvTxfm2dAsmTest : public ::testing::TestWithParam<InvTxfm2dParam> {
             }
 
             printf("Average Nanoseconds per Function Call\n");
-            printf("    HandleTransform%dx%d_c     : %6.2f\n",
+            printf("    HandleTransform%dx%d_c    : %6.2f\n",
                    widths[idx],
                    heights[idx],
                    1000000 * time_c / num_loop);
             printf(
-                "    HandleTransform%dx%d_avx2) : %6.2f   (Comparison: "
+                "    HandleTransform%dx%d_%s : %6.2f   (Comparison: "
                 "%5.2fx)\n",
                 widths[idx],
                 heights[idx],
+                intrinsic_set_name,
                 1000000 * time_o / num_loop,
                 time_c / time_o);
         }
     }
 
-#endif  // ARCH_X86_64
-
   private:
     // clear the coeffs according to eob position, note the coeffs are
     // linear.
@@ -944,6 +986,14 @@ TEST_P(InvTxfm2dAsmTest, sqr_txfm_match_test) {
     }
 }
 
+TEST_P(InvTxfm2dAsmTest, HandleTransform_match_test) {
+    run_HandleTransform_match_test();
+}
+
+TEST_P(InvTxfm2dAsmTest, DISABLED_HandleTransform_speed_test) {
+    run_handle_transform_speed_test();
+}
+
 #ifdef ARCH_X86_64
 
 TEST_P(InvTxfm2dAsmTest, rect_type1_txfm_match_test) {
@@ -992,14 +1042,6 @@ TEST_P(InvTxfm2dAsmTest, lowbd_txfm_match_test) {
     }
 }
 
-TEST_P(InvTxfm2dAsmTest, HandleTransform_match_test) {
-    run_HandleTransform_match_test();
-}
-
-TEST_P(InvTxfm2dAsmTest, DISABLED_HandleTransform_speed_test) {
-    run_handle_transform_speed_test();
-}
-
 extern "C" void svt_av1_lowbd_inv_txfm2d_add_ssse3(
     const int32_t *input, uint8_t *output_r, int32_t stride_r,
     uint8_t *output_w, int32_t stride_w, TxType tx_type, TxSize tx_size,
diff --git a/test/SadTest.cc b/test/SadTest.cc
index 1d2c1e45e..0dad22643 100644
--- a/test/SadTest.cc
+++ b/test/SadTest.cc
@@ -1067,6 +1067,7 @@ TEST_P(Allsad_CalculationTest, 8x8_16x16_Test_sse4_1) {
 TEST_P(Allsad_CalculationTest, 32x32_64x64_Test_sse4_1) {
     check_get_32x32_sad(svt_ext_eight_sad_calculation_32x32_64x64_sse4_1);
 }
+
 TEST_P(Allsad_CalculationTest, 32x32_64x64_Test_avx2) {
     check_get_32x32_sad(svt_ext_eight_sad_calculation_32x32_64x64_avx2);
 }
@@ -1079,6 +1080,10 @@ TEST_P(Allsad_CalculationTest, 8x8_16x16_Test_neon) {
     check_get_8x8_sad(svt_ext_all_sad_calculation_8x8_16x16_neon);
 }
 
+TEST_P(Allsad_CalculationTest, 32x32_64x64_Test_neon) {
+    check_get_32x32_sad(svt_ext_eight_sad_calculation_32x32_64x64_neon);
+}
+
 #endif
 
 INSTANTIATE_TEST_SUITE_P(
diff --git a/test/SpatialFullDistortionTest.cc b/test/SpatialFullDistortionTest.cc
index 48f241ab1..cfe58b01f 100644
--- a/test/SpatialFullDistortionTest.cc
+++ b/test/SpatialFullDistortionTest.cc
@@ -744,4 +744,12 @@ INSTANTIATE_TEST_SUITE_P(
 
 #endif
 
+#ifdef ARCH_AARCH64
+
+INSTANTIATE_TEST_CASE_P(
+    NEON, fullDistortionKernelCbfZero32Bits,
+    ::testing::Values(svt_full_distortion_kernel_cbf_zero32_bits_neon));
+
+#endif  //  ARCH_AARCH64
+
 }  // namespace
diff --git a/test/TemporalFilterTestPlanewise.cc b/test/TemporalFilterTestPlanewise.cc
index 5cf6aefa2..5a8916bac 100644
--- a/test/TemporalFilterTestPlanewise.cc
+++ b/test/TemporalFilterTestPlanewise.cc
@@ -1333,6 +1333,18 @@ int32_t estimate_noise_fp16_avx2_wrapper(const uint16_t *src, int width,
 
 #ifdef ARCH_AARCH64
 
+TEST_F(TemporalFilterTestApplyFilteringCentral, test_lbd_neon) {
+    for (int i = 0; i < 100; ++i) {
+        RunTest(false, svt_aom_apply_filtering_central_neon, NULL);
+    }
+}
+
+TEST_F(TemporalFilterTestApplyFilteringCentral, test_hbd_neon) {
+    for (int i = 0; i < 100; ++i) {
+        RunTest(true, NULL, svt_aom_apply_filtering_central_highbd_neon);
+    }
+}
+
 int32_t estimate_noise_fp16_c_wrapper(const uint16_t *src, int width,
                                       int height, int stride, int bd) {
     UNUSED(bd);
@@ -1346,7 +1358,7 @@ int32_t estimate_noise_fp16_neon_wrapper(const uint16_t *src, int width,
         (const uint8_t *)src, width, height, stride);
 }
 
-#endif
+#endif  // ARCH_AARCH64
 
 typedef int32_t (*EstimateNoiseFuncFP)(const uint16_t *src, int width,
                                        int height, int stride, int bd);
diff --git a/test/quantize_func_test.cc b/test/quantize_func_test.cc
index 07e913263..1d17af762 100644
--- a/test/quantize_func_test.cc
+++ b/test/quantize_func_test.cc
@@ -796,6 +796,8 @@ TEST_P(QuantizeQmHbdTest, CoeffHalfDequant) {
 
 using std::make_tuple;
 
+#ifdef ARCH_X86_64
+
 #if HAS_AVX2
 const QuantizeParam kQParamArrayAvx2[] = {
     make_tuple(&svt_av1_quantize_fp_c, &svt_av1_quantize_fp_sse4_1,
@@ -969,7 +971,10 @@ INSTANTIATE_TEST_SUITE_P(AVX2, QuantizeQmTest,
 INSTANTIATE_TEST_SUITE_P(AVX2, QuantizeQmHbdTest,
                          ::testing::ValuesIn(kQmParamHbdArrayAvx2));
 
-TEST(ComputeCulLevel, avx2) {
+#endif  // HAS_AVX2
+#endif  // ARCH_X86_64
+
+TEST(ComputeCulLevel, ComputeCulLevelIntrinsic) {
     SVTRandom rnd(0, (1 << 10) - 1);
     const int max_size = 100 * 100;
     // scan[] is a set of indexes for quant_coeff[]
@@ -991,13 +996,21 @@ TEST(ComputeCulLevel, avx2) {
         eob_ref = eob_mod = rnd.random() % max_size;
 
         int32_t ref = svt_av1_compute_cul_level_c(scan, quant_coeff, &eob_ref);
+
+#ifdef ARCH_X86_64
+#if HAS_AVX2
         int32_t mod =
             svt_av1_compute_cul_level_avx2(scan, quant_coeff, &eob_mod);
+#endif  // HAS_AVX2
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+        int32_t mod =
+            svt_av1_compute_cul_level_neon(scan, quant_coeff, &eob_mod);
+#endif  // ARCH_AARCH64
 
         EXPECT_TRUE(ref == mod);
         EXPECT_TRUE(eob_ref == eob_mod);
     }
 }
-
-#endif  // HAS_AVX2
 }  // namespace
-- 
GitLab


From 2b273d88990ee55492e8baca7c942dd68c8ed6ec Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Fri, 23 Feb 2024 11:46:36 +0000
Subject: [PATCH 05/13] NEON port of svt_av1_highbd_jnt_convolve_y_c

---
 Source/Lib/Common/ASM_NEON/CMakeLists.txt     |   1 +
 .../ASM_NEON/highbd_jnt_convolve_neon.c       | 284 ++++++++++++++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   1 +
 test/convolve_2d_test.cc                      |   7 +-
 5 files changed, 290 insertions(+), 5 deletions(-)
 create mode 100644 Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c

diff --git a/Source/Lib/Common/ASM_NEON/CMakeLists.txt b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
index 380f8fb4c..75dfbb4e6 100644
--- a/Source/Lib/Common/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
@@ -32,6 +32,7 @@ target_sources(
   PUBLIC EbIntraPrediction_neon.c
   PUBLIC EbPictureOperators_Intrinsic_neon.c
   PUBLIC highbd_convolve_2d_neon.c
+  PUBLIC highbd_jnt_convolve_neon.c
   PUBLIC selfguided_neon.c
   PUBLIC sse_neon.c
   PUBLIC warp_plane_neon.c
diff --git a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
new file mode 100644
index 000000000..01f3313ca
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
@@ -0,0 +1,284 @@
+/*
+ * Copyright (c) 2024, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <arm_neon.h>
+
+#include <assert.h>
+
+#include "EbDefinitions.h"
+#include "common_dsp_rtcd.h"
+#include "convolve.h"
+
+static INLINE int32x4_t svt_aom_convolve(const uint16x8_t *const s, const int16x8_t *const coeffs) {
+    const int32x4_t res_0 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[0])), vget_low_s16(coeffs[0])),
+                                       vmull_high_s16(vreinterpretq_s16_u16(s[0]), coeffs[0]));
+    const int32x4_t res_1 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[1])), vget_low_s16(coeffs[1])),
+                                       vmull_high_s16(vreinterpretq_s16_u16(s[1]), coeffs[1]));
+    const int32x4_t res_2 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[2])), vget_low_s16(coeffs[2])),
+                                       vmull_high_s16(vreinterpretq_s16_u16(s[2]), coeffs[2]));
+    const int32x4_t res_3 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[3])), vget_low_s16(coeffs[3])),
+                                       vmull_high_s16(vreinterpretq_s16_u16(s[3]), coeffs[3]));
+
+    return vaddq_s32(vaddq_s32(res_0, res_1), vaddq_s32(res_2, res_3));
+}
+
+static INLINE int32x4_t highbd_convolve_rounding_neon(const int32x4_t *const res_unsigned,
+                                                      const int32x4_t *const offset_const, const int round_shift) {
+    const int32x4_t res_signed = vsubq_s32(*res_unsigned, *offset_const);
+
+    return vrshlq_s32(res_signed, vdupq_n_s32(-round_shift));
+}
+
+static INLINE int32x4_t highbd_comp_avg_neon(const uint16x8_t *const data_ref_0, const int32x4_t *const res_unsigned,
+                                             const int32x4_t *const wt0, const int32x4_t *const wt1,
+                                             const int use_dist_wtd_avg) {
+    int32x4_t res;
+    if (use_dist_wtd_avg) {
+        const int32x4_t wt0_res = vmulq_s32(vreinterpretq_s32_u16(*data_ref_0), *wt0);
+        const int32x4_t wt1_res = vmulq_s32(*res_unsigned, *wt1);
+
+        const int32x4_t wt_res = vaddq_s32(wt0_res, wt1_res);
+        res                    = vshrq_n_s32(wt_res, DIST_PRECISION_BITS);
+    } else {
+        const int32x4_t wt_res = vaddq_s32(vreinterpretq_s32_u16(*data_ref_0), *res_unsigned);
+        res                    = vshrq_n_s32(wt_res, 1);
+    }
+    return res;
+}
+
+static INLINE void prepare_coeffs(const InterpFilterParams *const filter_params, const int subpel_q4,
+                                  int16x8_t *const coeffs /* [4] */) {
+    const int16_t  *filter = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
+    const int32x4_t coeff  = vld1q_s32((const int32_t *)filter);
+
+    // coeffs 0 1 0 1 0 1 0 1
+    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 0)));
+    // coeffs 2 3 2 3 2 3 2 3
+    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 1)));
+    // coeffs 4 5 4 5 4 5 4 5
+    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 2)));
+    // coeffs 6 7 6 7 6 7 6 7
+    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 3)));
+}
+
+void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst0, int32_t dst_stride0,
+                                        int32_t w, int32_t h, const InterpFilterParams *filter_params_x,
+                                        const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
+                                        const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd) {
+    if (w <= 4) {
+        svt_av1_highbd_jnt_convolve_y_c(src,
+                                        src_stride,
+                                        dst0,
+                                        dst_stride0,
+                                        w,
+                                        h,
+                                        filter_params_x,
+                                        filter_params_y,
+                                        subpel_x_q4,
+                                        subpel_y_q4,
+                                        conv_params,
+                                        bd);
+        return;
+    }
+    CONV_BUF_TYPE        *dst        = conv_params->dst;
+    const int             dst_stride = conv_params->dst_stride;
+    const int             fo_vert    = filter_params_y->taps / 2 - 1;
+    const uint16_t *const src_ptr    = src - fo_vert * src_stride;
+
+    assert(bits >= 0);
+    int       i, j;
+    const int do_average       = conv_params->do_average;
+    const int use_jnt_comp_avg = conv_params->use_jnt_comp_avg;
+
+    const int       w0            = conv_params->fwd_offset;
+    const int       w1            = conv_params->bck_offset;
+    const int32x4_t wt0           = vdupq_n_s32(w0);
+    const int32x4_t wt1           = vdupq_n_s32(w1);
+    const int32x4_t round_const_y = vdupq_n_s32(((1 << conv_params->round_1) >> 1));
+
+    const int        offset_0         = bd + 2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+    const int        offset           = (1 << offset_0) + (1 << (offset_0 - 1));
+    const int32x4_t  offset_const     = vdupq_n_s32(offset);
+    const int        rounding_shift   = 2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+    const uint16x8_t clip_pixel_to_bd = vdupq_n_u16(bd == 10 ? 1023 : (bd == 12 ? 4095 : 255));
+    const uint16x8_t zero             = vdupq_n_u16(0);
+    uint16x8_t       s[16];
+    int16x8_t        coeffs_y[4];
+
+    prepare_coeffs(filter_params_y, subpel_y_q4, coeffs_y);
+
+    for (j = 0; j < w; j += 8) {
+        const uint16_t *data = &src_ptr[j];
+        /* Vertical filter */
+        {
+            uint16x8_t s0 = vld1q_u16(data + 0 * src_stride);
+            uint16x8_t s1 = vld1q_u16(data + 1 * src_stride);
+            uint16x8_t s2 = vld1q_u16(data + 2 * src_stride);
+            uint16x8_t s3 = vld1q_u16(data + 3 * src_stride);
+            uint16x8_t s4 = vld1q_u16(data + 4 * src_stride);
+            uint16x8_t s5 = vld1q_u16(data + 5 * src_stride);
+            uint16x8_t s6 = vld1q_u16(data + 6 * src_stride);
+
+            s[0] = vzip1q_u16(s0, s1);
+            s[1] = vzip1q_u16(s2, s3);
+            s[2] = vzip1q_u16(s4, s5);
+
+            s[4] = vzip2q_u16(s0, s1);
+            s[5] = vzip2q_u16(s2, s3);
+            s[6] = vzip2q_u16(s4, s5);
+
+            s[0 + 8] = vzip1q_u16(s1, s2);
+            s[1 + 8] = vzip1q_u16(s3, s4);
+            s[2 + 8] = vzip1q_u16(s5, s6);
+
+            s[4 + 8] = vzip2q_u16(s1, s2);
+            s[5 + 8] = vzip2q_u16(s3, s4);
+            s[6 + 8] = vzip2q_u16(s5, s6);
+
+            for (i = 0; i < h; i += 2) {
+                data = &src_ptr[i * src_stride + j];
+
+                const uint16x8_t s7 = vld1q_u16(data + 7 * src_stride);
+                const uint16x8_t s8 = vld1q_u16(data + 8 * src_stride);
+
+                s[3] = vzip1q_u16(s6, s7);
+                s[7] = vzip2q_u16(s6, s7);
+
+                s[3 + 8] = vzip1q_u16(s7, s8);
+                s[7 + 8] = vzip2q_u16(s7, s8);
+
+                const int32x4_t res_a0       = svt_aom_convolve(s, coeffs_y);
+                int32x4_t       res_a_round0 = vshlq_n_s32(res_a0, FILTER_BITS - conv_params->round_0);
+                res_a_round0 = vshrq_n_s32(vaddq_s32(res_a_round0, round_const_y), conv_params->round_1);
+
+                const int32x4_t res_a1       = svt_aom_convolve(s + 8, coeffs_y);
+                int32x4_t       res_a_round1 = vshlq_n_s32(res_a1, FILTER_BITS - conv_params->round_0);
+                res_a_round1 = vshrq_n_s32(vaddq_s32(res_a_round1, round_const_y), conv_params->round_1);
+
+                const int32x4_t res_unsigned_lo_0 = vaddq_s32(res_a_round0, offset_const);
+                const int32x4_t res_unsigned_lo_1 = vaddq_s32(res_a_round1, offset_const);
+
+                if (w - j < 8) {
+                    if (do_average) {
+                        const uint16x8_t data_0 = vcombine_u16(vld1_u16(&dst[i * dst_stride + j]), vdup_n_u16(0));
+                        const uint16x8_t data_1 = vcombine_u16(vld1_u16(&dst[i * dst_stride + j + dst_stride]),
+                                                               vdup_n_u16(0));
+
+                        const uint16x8_t data_ref_0 = vzip1q_u16(data_0, zero);
+                        const uint16x8_t data_ref_1 = vzip1q_u16(data_1, zero);
+
+                        const int32x4_t comp_avg_res_0 = highbd_comp_avg_neon(
+                            &data_ref_0, &res_unsigned_lo_0, &wt0, &wt1, use_jnt_comp_avg);
+                        const int32x4_t comp_avg_res_1 = highbd_comp_avg_neon(
+                            &data_ref_1, &res_unsigned_lo_1, &wt0, &wt1, use_jnt_comp_avg);
+
+                        const int32x4_t round_result_0 = highbd_convolve_rounding_neon(
+                            &comp_avg_res_0, &offset_const, rounding_shift);
+                        const int32x4_t round_result_1 = highbd_convolve_rounding_neon(
+                            &comp_avg_res_1, &offset_const, rounding_shift);
+
+                        const uint16x8_t res_16b_0  = vqmovun_high_s32(vqmovun_s32(round_result_0), round_result_0);
+                        const uint16x8_t res_clip_0 = vminq_u16(res_16b_0, clip_pixel_to_bd);
+                        const uint16x8_t res_16b_1  = vqmovun_high_s32(vqmovun_s32(round_result_1), round_result_1);
+                        const uint16x8_t res_clip_1 = vminq_u16(res_16b_1, clip_pixel_to_bd);
+
+                        vst1_u16(&dst0[i * dst_stride0 + j], vget_low_u16(res_clip_0));
+                        vst1_u16(&dst0[i * dst_stride0 + j + dst_stride0], vget_low_u16(res_clip_1));
+
+                    } else {
+                        const uint16x8_t res_16b_0 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_0),
+                                                                      res_unsigned_lo_0);
+
+                        const uint16x8_t res_16b_1 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_1),
+                                                                      res_unsigned_lo_1);
+
+                        vst1_u16(&dst[i * dst_stride + j], vget_low_u16(res_16b_0));
+                        vst1_u16(&dst[i * dst_stride + j + dst_stride], vget_low_u16(res_16b_1));
+                    }
+                } else {
+                    const int32x4_t res_b0       = svt_aom_convolve(s + 4, coeffs_y);
+                    int32x4_t       res_b_round0 = vshlq_n_s32(res_b0, FILTER_BITS - conv_params->round_0);
+                    res_b_round0 = vshrq_n_s32(vaddq_s32(res_b_round0, round_const_y), conv_params->round_1);
+
+                    const int32x4_t res_b1       = svt_aom_convolve(s + 4 + 8, coeffs_y);
+                    int32x4_t       res_b_round1 = vshlq_n_s32(res_b1, FILTER_BITS - conv_params->round_0);
+                    res_b_round1 = vshrq_n_s32(vaddq_s32(res_b_round1, round_const_y), conv_params->round_1);
+
+                    int32x4_t res_unsigned_hi_0 = vaddq_s32(res_b_round0, offset_const);
+                    int32x4_t res_unsigned_hi_1 = vaddq_s32(res_b_round1, offset_const);
+
+                    if (do_average) {
+                        const uint16x8_t data_0          = vld1q_u16(&dst[i * dst_stride + j]);
+                        const uint16x8_t data_1          = vld1q_u16(&dst[i * dst_stride + j + dst_stride]);
+                        const uint16x8_t data_ref_0_lo_0 = vzip1q_u16(data_0, zero);
+                        const uint16x8_t data_ref_0_lo_1 = vzip1q_u16(data_1, zero);
+
+                        const uint16x8_t data_ref_0_hi_0 = vzip2q_u16(data_0, zero);
+                        const uint16x8_t data_ref_0_hi_1 = vzip2q_u16(data_1, zero);
+
+                        const int32x4_t comp_avg_res_lo_0 = highbd_comp_avg_neon(
+                            &data_ref_0_lo_0, &res_unsigned_lo_0, &wt0, &wt1, use_jnt_comp_avg);
+                        const int32x4_t comp_avg_res_lo_1 = highbd_comp_avg_neon(
+                            &data_ref_0_lo_1, &res_unsigned_lo_1, &wt0, &wt1, use_jnt_comp_avg);
+                        const int32x4_t comp_avg_res_hi_0 = highbd_comp_avg_neon(
+                            &data_ref_0_hi_0, &res_unsigned_hi_0, &wt0, &wt1, use_jnt_comp_avg);
+                        const int32x4_t comp_avg_res_hi_1 = highbd_comp_avg_neon(
+                            &data_ref_0_hi_1, &res_unsigned_hi_1, &wt0, &wt1, use_jnt_comp_avg);
+
+                        const int32x4_t round_result_lo_0 = highbd_convolve_rounding_neon(
+                            &comp_avg_res_lo_0, &offset_const, rounding_shift);
+                        const int32x4_t round_result_lo_1 = highbd_convolve_rounding_neon(
+                            &comp_avg_res_lo_1, &offset_const, rounding_shift);
+                        const int32x4_t round_result_hi_0 = highbd_convolve_rounding_neon(
+                            &comp_avg_res_hi_0, &offset_const, rounding_shift);
+                        const int32x4_t round_result_hi_1 = highbd_convolve_rounding_neon(
+                            &comp_avg_res_hi_1, &offset_const, rounding_shift);
+
+                        const uint16x8_t res_16b_0  = vqmovun_high_s32(vqmovun_s32(round_result_lo_0),
+                                                                      round_result_hi_0);
+                        const uint16x8_t res_clip_0 = vminq_u16(res_16b_0, clip_pixel_to_bd);
+
+                        const uint16x8_t res_16b_1  = vqmovun_high_s32(vqmovun_s32(round_result_lo_1),
+                                                                      round_result_hi_1);
+                        const uint16x8_t res_clip_1 = vminq_u16(res_16b_1, clip_pixel_to_bd);
+
+                        vst1q_u16(&dst0[i * dst_stride0 + j], res_clip_0);
+                        vst1q_u16(&dst0[i * dst_stride0 + j + dst_stride0], res_clip_1);
+                    } else {
+                        const uint16x8_t res_16bit0 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_0),
+                                                                       res_unsigned_hi_0);
+                        const uint16x8_t res_16bit1 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_1),
+                                                                       res_unsigned_hi_1);
+                        vst1q_u16(&dst[i * dst_stride + j], res_16bit0);
+                        vst1q_u16(&dst[i * dst_stride + j + dst_stride], res_16bit1);
+                    }
+                }
+                s[0] = s[1];
+                s[1] = s[2];
+                s[2] = s[3];
+
+                s[4] = s[5];
+                s[5] = s[6];
+                s[6] = s[7];
+
+                s[0 + 8] = s[1 + 8];
+                s[1 + 8] = s[2 + 8];
+                s[2 + 8] = s[3 + 8];
+
+                s[4 + 8] = s[5 + 8];
+                s[5 + 8] = s[6 + 8];
+                s[6 + 8] = s[7 + 8];
+
+                s6 = s8;
+            }
+        }
+    }
+}
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 599861a83..72a75e6a6 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -894,7 +894,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_av1_highbd_jnt_convolve_2d, svt_av1_highbd_jnt_convolve_2d_c, svt_av1_highbd_jnt_convolve_2d_neon);
     SET_ONLY_C(svt_av1_highbd_jnt_convolve_2d_copy, svt_av1_highbd_jnt_convolve_2d_copy_c);
     SET_ONLY_C(svt_av1_highbd_jnt_convolve_x, svt_av1_highbd_jnt_convolve_x_c);
-    SET_ONLY_C(svt_av1_highbd_jnt_convolve_y, svt_av1_highbd_jnt_convolve_y_c);
+    SET_NEON(svt_av1_highbd_jnt_convolve_y, svt_av1_highbd_jnt_convolve_y_c, svt_av1_highbd_jnt_convolve_y_neon);
     SET_ONLY_C(svt_av1_highbd_convolve_x_sr, svt_av1_highbd_convolve_x_sr_c);
     SET_NEON(svt_av1_convolve_2d_sr, svt_av1_convolve_2d_sr_c, svt_av1_convolve_2d_sr_neon);
     SET_NEON(svt_av1_convolve_2d_copy_sr, svt_av1_convolve_2d_copy_sr_c, svt_av1_convolve_2d_copy_sr_neon);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index e7031106f..dc1f1c970 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1378,6 +1378,7 @@ extern "C" {
                                            const InterpFilterParams *filter_params_x,
                                            const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
                                            const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
+    void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
     void svt_av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
diff --git a/test/convolve_2d_test.cc b/test/convolve_2d_test.cc
index d6e5b0ef7..847ff976e 100644
--- a/test/convolve_2d_test.cc
+++ b/test/convolve_2d_test.cc
@@ -1057,7 +1057,7 @@ class AV1HbdJntConvolve2DTest : public AV1HbdConvolve2DTest {
             } else if (has_subx == 1) {
                 func_tst_ = func_ref_;  // not yet ported
             } else if (has_suby == 1) {
-                func_tst_ = func_ref_;  // not yet ported
+                func_tst_ = svt_av1_highbd_jnt_convolve_y_neon;
             } else {
                 func_tst_ = func_ref_;  // not yet ported
             }
@@ -1111,9 +1111,8 @@ INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTest2D, AV1HbdJntConvolve2DTest,
 // not yet ported
 // INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestX, AV1HbdJntConvolve2DTest,
 //                         BuildParams(1, 0, 2, 1));
-// not yet ported
-// INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestY, AV1HbdJntConvolve2DTest,
-//                         BuildParams(0, 1, 2, 1));
+INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdJntConvolve2DTest,
+                        BuildParams(0, 1, 2, 1));
 
 #endif  // ARCH_AARCH64
 
-- 
GitLab


From 96052a9847e8f5d855172a103cb1d6cf725f9761 Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rjcausarano@gmail.com>
Date: Mon, 29 Apr 2024 16:36:28 -0300
Subject: [PATCH 06/13] NEON port of svt_av1_highbd_convolve_y_sr_c

---
 Source/Lib/Common/ASM_NEON/CMakeLists.txt     |   1 +
 .../Common/ASM_NEON/highbd_convolve_neon.c    | 349 ++++++++++++++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   1 +
 test/convolve_2d_test.cc                      |  37 +-
 5 files changed, 370 insertions(+), 20 deletions(-)
 create mode 100644 Source/Lib/Common/ASM_NEON/highbd_convolve_neon.c

diff --git a/Source/Lib/Common/ASM_NEON/CMakeLists.txt b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
index 75dfbb4e6..47472f455 100644
--- a/Source/Lib/Common/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
@@ -33,6 +33,7 @@ target_sources(
   PUBLIC EbPictureOperators_Intrinsic_neon.c
   PUBLIC highbd_convolve_2d_neon.c
   PUBLIC highbd_jnt_convolve_neon.c
+  PUBLIC highbd_convolve_neon.c
   PUBLIC selfguided_neon.c
   PUBLIC sse_neon.c
   PUBLIC warp_plane_neon.c
diff --git a/Source/Lib/Common/ASM_NEON/highbd_convolve_neon.c b/Source/Lib/Common/ASM_NEON/highbd_convolve_neon.c
new file mode 100644
index 000000000..4c4fade20
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/highbd_convolve_neon.c
@@ -0,0 +1,349 @@
+/*
+ * Copyright (c) 2024, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <assert.h>
+
+#include <arm_neon.h>
+
+#include "EbDefinitions.h"
+#include "common_dsp_rtcd.h"
+#include "convolve.h"
+
+static INLINE void svt_prepare_coeffs_12tap(const int16_t *const filter, int16x8_t *coeffs /* [6] */) {
+    int32x4_t coeffs_y  = vld1q_s32((int32_t const *)filter);
+    int32x4_t coeffs_y2 = vld1q_s32((int32_t const *)(filter + 8));
+
+    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 0))); // coeffs 0 1 0 1 0 1 0 1
+    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 1))); // coeffs 2 3 2 3 2 3 2 3
+    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 2))); // coeffs 4 5 4 5 4 5 4 5
+    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y, 3))); // coeffs 6 7 6 7 6 7 6 7
+
+    coeffs[4] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y2, 0))); // coeffs 8 9 8 9 8 9 8 9
+    coeffs[5] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeffs_y2, 1))); // coeffs 10 11 10 11 10 11 10 11
+}
+
+static INLINE void prepare_coeffs(const int16_t *const filter, int16x8_t *const coeffs /* [4] */) {
+    const int16x8_t coeff = vld1q_s16(filter);
+
+    // coeffs 0 1 0 1 0 1 0 1
+    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 0)));
+    // coeffs 2 3 2 3 2 3 2 3
+    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 1)));
+    // coeffs 4 5 4 5 4 5 4 5
+    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 2)));
+    // coeffs 6 7 6 7 6 7 6 7
+    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 3)));
+}
+
+static INLINE int32x4_t convolve_12tap(const uint16x8_t *s, const int16x8_t *coeffs) {
+    const int32x4_t d0     = vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[0])), vget_low_s16(coeffs[0]));
+    const int32x4_t d1     = vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[1])), vget_low_s16(coeffs[1]));
+    const int32x4_t d2     = vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[2])), vget_low_s16(coeffs[2]));
+    const int32x4_t d3     = vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[3])), vget_low_s16(coeffs[3]));
+    const int32x4_t d4     = vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[4])), vget_low_s16(coeffs[4]));
+    const int32x4_t d5     = vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[5])), vget_low_s16(coeffs[5]));
+    const int32x4_t d_0123 = vaddq_s32(vaddq_s32(d0, d1), vaddq_s32(d2, d3));
+    const int32x4_t d      = vaddq_s32(vaddq_s32(d4, d5), d_0123);
+    return d;
+}
+
+static INLINE int32x4_t svt_aom_convolve(const uint16x8_t *const s, const int16x8_t *const coeffs) {
+    const int32x4_t res_0 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[0])), vget_low_s16(coeffs[0])),
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[0])), vget_high_s16(coeffs[0])));
+    const int32x4_t res_1 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[1])), vget_low_s16(coeffs[1])),
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[1])), vget_high_s16(coeffs[1])));
+    const int32x4_t res_2 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[2])), vget_low_s16(coeffs[2])),
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[2])), vget_high_s16(coeffs[2])));
+    const int32x4_t res_3 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[3])), vget_low_s16(coeffs[3])),
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[3])), vget_high_s16(coeffs[3])));
+
+    const int32x4_t res = vaddq_s32(vaddq_s32(res_0, res_1), vaddq_s32(res_2, res_3));
+
+    return res;
+}
+
+void svt_av1_highbd_convolve_y_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride,
+                                       int32_t w, int32_t h, const InterpFilterParams *filter_params_x,
+                                       const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
+                                       const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd) {
+    (void)filter_params_x;
+    (void)subpel_x_q4;
+    (void)conv_params;
+    int                   i, j;
+    const int             fo_vert = filter_params_y->taps / 2 - 1;
+    const uint16_t *const src_ptr = src - fo_vert * src_stride;
+    const int             bits    = FILTER_BITS;
+
+    const int32x4_t round_shift_bits = vsetq_lane_s32(bits, vdupq_n_s32(0), 0);
+    const int32x4_t round_const_bits = vdupq_n_s32((1 << bits) >> 1);
+    const int16x8_t clip_pixel       = vdupq_n_s16(bd == 10 ? 1023 : (bd == 12 ? 4095 : 255));
+    const int16x8_t zero             = vdupq_n_s16(0);
+
+    if (filter_params_y->taps == 12) {
+        uint16x8_t           s[24];
+        int16x8_t            coeffs_y[6];
+        const int16_t *const y_filter = av1_get_interp_filter_subpel_kernel(*filter_params_y,
+                                                                            subpel_y_q4 & SUBPEL_MASK);
+        svt_prepare_coeffs_12tap(y_filter, coeffs_y);
+
+        for (j = 0; j < w; j += 8) {
+            const uint16_t *data = &src_ptr[j];
+            /* Vertical filter */
+            uint16x8_t s0  = vld1q_u16(data + 0 * src_stride);
+            uint16x8_t s1  = vld1q_u16(data + 1 * src_stride);
+            uint16x8_t s2  = vld1q_u16(data + 2 * src_stride);
+            uint16x8_t s3  = vld1q_u16(data + 3 * src_stride);
+            uint16x8_t s4  = vld1q_u16(data + 4 * src_stride);
+            uint16x8_t s5  = vld1q_u16(data + 5 * src_stride);
+            uint16x8_t s6  = vld1q_u16(data + 6 * src_stride);
+            uint16x8_t s7  = vld1q_u16(data + 7 * src_stride);
+            uint16x8_t s8  = vld1q_u16(data + 8 * src_stride);
+            uint16x8_t s9  = vld1q_u16(data + 9 * src_stride);
+            uint16x8_t s10 = vld1q_u16(data + 10 * src_stride);
+
+            s[0] = vzip1q_u16(s0, s1);
+            s[1] = vzip1q_u16(s2, s3);
+            s[2] = vzip1q_u16(s4, s5);
+            s[3] = vzip1q_u16(s6, s7);
+            s[4] = vzip1q_u16(s8, s9);
+
+            s[6]  = vzip2q_u16(s0, s1);
+            s[7]  = vzip2q_u16(s2, s3);
+            s[8]  = vzip2q_u16(s4, s5);
+            s[9]  = vzip2q_u16(s6, s7);
+            s[10] = vzip2q_u16(s8, s9);
+
+            s[12] = vzip1q_u16(s1, s2);
+            s[13] = vzip1q_u16(s3, s4);
+            s[14] = vzip1q_u16(s5, s6);
+            s[15] = vzip1q_u16(s7, s8);
+            s[16] = vzip1q_u16(s9, s10);
+
+            s[18] = vzip2q_u16(s1, s2);
+            s[19] = vzip2q_u16(s3, s4);
+            s[20] = vzip2q_u16(s5, s6);
+            s[21] = vzip2q_u16(s7, s8);
+            s[22] = vzip2q_u16(s9, s10);
+
+            for (i = 0; i < h; i += 2) {
+                data = &src_ptr[i * src_stride + j];
+
+                uint16x8_t s11 = vld1q_u16(data + 11 * src_stride);
+                uint16x8_t s12 = vld1q_u16(data + 12 * src_stride);
+
+                s[5]  = vzip1q_u16(s10, s11);
+                s[11] = vzip2q_u16(s10, s11);
+
+                s[17] = vzip1q_u16(s11, s12);
+                s[23] = vzip2q_u16(s11, s12);
+
+                const int32x4_t res_a0       = convolve_12tap(s, coeffs_y);
+                int32x4_t       res_a_round0 = vshlq_s32(vaddq_s32(res_a0, round_const_bits),
+                                                   vdupq_n_s32(-round_shift_bits[0]));
+
+                const int32x4_t res_a1       = convolve_12tap(s + 12, coeffs_y);
+                int32x4_t       res_a_round1 = vshlq_s32(vaddq_s32(res_a1, round_const_bits),
+                                                   vdupq_n_s32(-round_shift_bits[0]));
+
+                if (w - j > 4) {
+                    const int32x4_t res_b0       = convolve_12tap(s + 6, coeffs_y);
+                    int32x4_t       res_b_round0 = vshlq_s32(vaddq_s32(res_b0, round_const_bits),
+                                                       vdupq_n_s32(-round_shift_bits[0]));
+
+                    const int32x4_t res_b1       = convolve_12tap(s + 18, coeffs_y);
+                    int32x4_t       res_b_round1 = vshlq_s32(vaddq_s32(res_b1, round_const_bits),
+                                                       vdupq_n_s32(-round_shift_bits[0]));
+
+                    int16x8_t res_16bit0 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_b_round0);
+                    res_16bit0           = vminq_s16(res_16bit0, clip_pixel);
+                    res_16bit0           = vmaxq_s16(res_16bit0, zero);
+
+                    int16x8_t res_16bit1 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_b_round1);
+                    res_16bit1           = vminq_s16(res_16bit1, clip_pixel);
+                    res_16bit1           = vmaxq_s16(res_16bit1, zero);
+
+                    vst1q_u16(&dst[i * dst_stride + j], vreinterpretq_u16_s16(res_16bit0));
+                    vst1q_u16(&dst[i * dst_stride + j + dst_stride], vreinterpretq_u16_s16(res_16bit1));
+                } else if (w == 4) {
+                    int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                    res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                    res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                    int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                    res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                    res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                    vst1_u16(&dst[i * dst_stride + j], vget_low_u16(vreinterpretq_u16_s16(res_a_round0_s16)));
+                    vst1_u16(&dst[i * dst_stride + j + dst_stride],
+                             vget_low_u16(vreinterpretq_u16_s16(res_a_round1_s16)));
+                } else {
+                    int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                    res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                    res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                    int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                    res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                    res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                    *((uint32_t *)(&dst[i * dst_stride + j])) = vgetq_lane_u32(vreinterpretq_u32_s16(res_a_round0_s16),
+                                                                               0);
+                    *((uint32_t *)(&dst[i * dst_stride + j + dst_stride])) = vgetq_lane_u32(
+                        vreinterpretq_u32_s16(res_a_round1_s16), 0);
+                }
+
+                s[0] = s[1];
+                s[1] = s[2];
+                s[2] = s[3];
+                s[3] = s[4];
+                s[4] = s[5];
+
+                s[6]  = s[7];
+                s[7]  = s[8];
+                s[8]  = s[9];
+                s[9]  = s[10];
+                s[10] = s[11];
+
+                s[12] = s[13];
+                s[13] = s[14];
+                s[14] = s[15];
+                s[15] = s[16];
+                s[16] = s[17];
+
+                s[18] = s[19];
+                s[19] = s[20];
+                s[20] = s[21];
+                s[21] = s[22];
+                s[22] = s[23];
+
+                s10 = s12;
+            }
+        }
+    } else {
+        uint16x8_t     s[16];
+        int16x8_t      coeffs_y[4];
+        const int16_t *filter = av1_get_interp_filter_subpel_kernel(*filter_params_y, subpel_y_q4 & SUBPEL_MASK);
+        prepare_coeffs(filter, coeffs_y);
+
+        for (j = 0; j < w; j += 8) {
+            const uint16_t *data = &src_ptr[j];
+            /* Vertical filter */
+            {
+                uint16x8_t s0 = vld1q_u16(data + 0 * src_stride);
+                uint16x8_t s1 = vld1q_u16(data + 1 * src_stride);
+                uint16x8_t s2 = vld1q_u16(data + 2 * src_stride);
+                uint16x8_t s3 = vld1q_u16(data + 3 * src_stride);
+                uint16x8_t s4 = vld1q_u16(data + 4 * src_stride);
+                uint16x8_t s5 = vld1q_u16(data + 5 * src_stride);
+                uint16x8_t s6 = vld1q_u16(data + 6 * src_stride);
+
+                s[0] = vzip1q_u16(s0, s1);
+                s[1] = vzip1q_u16(s2, s3);
+                s[2] = vzip1q_u16(s4, s5);
+
+                s[4] = vzip2q_u16(s0, s1);
+                s[5] = vzip2q_u16(s2, s3);
+                s[6] = vzip2q_u16(s4, s5);
+
+                s[0 + 8] = vzip1q_u16(s1, s2);
+                s[1 + 8] = vzip1q_u16(s3, s4);
+                s[2 + 8] = vzip1q_u16(s5, s6);
+
+                s[4 + 8] = vzip2q_u16(s1, s2);
+                s[5 + 8] = vzip2q_u16(s3, s4);
+                s[6 + 8] = vzip2q_u16(s5, s6);
+
+                for (i = 0; i < h; i += 2) {
+                    data = &src_ptr[i * src_stride + j];
+
+                    uint16x8_t s7 = vld1q_u16(data + 7 * src_stride);
+                    uint16x8_t s8 = vld1q_u16(data + 8 * src_stride);
+
+                    s[3] = vzip1q_u16(s6, s7);
+                    s[7] = vzip2q_u16(s6, s7);
+
+                    s[3 + 8] = vzip1q_u16(s7, s8);
+                    s[7 + 8] = vzip2q_u16(s7, s8);
+
+                    const int32x4_t res_a0       = svt_aom_convolve(s, coeffs_y);
+                    int32x4_t       res_a_round0 = vshlq_s32(vaddq_s32(res_a0, round_const_bits),
+                                                       vdupq_n_s32(-round_shift_bits[0]));
+
+                    const int32x4_t res_a1       = svt_aom_convolve(s + 8, coeffs_y);
+                    int32x4_t       res_a_round1 = vshlq_s32(vaddq_s32(res_a1, round_const_bits),
+                                                       vdupq_n_s32(-round_shift_bits[0]));
+
+                    if (w - j > 4) {
+                        const int32x4_t res_b0       = svt_aom_convolve(s + 4, coeffs_y);
+                        int32x4_t       res_b_round0 = vshlq_s32(vaddq_s32(res_b0, round_const_bits),
+                                                           vdupq_n_s32(-round_shift_bits[0]));
+
+                        const int32x4_t res_b1       = svt_aom_convolve(s + 4 + 8, coeffs_y);
+                        int32x4_t       res_b_round1 = vshlq_s32(vaddq_s32(res_b1, round_const_bits),
+                                                           vdupq_n_s32(-round_shift_bits[0]));
+                        int16x8_t       res_16bit0   = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_b_round0);
+                        res_16bit0                   = vminq_s16(res_16bit0, clip_pixel);
+                        res_16bit0                   = vmaxq_s16(res_16bit0, zero);
+
+                        int16x8_t res_16bit1 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_b_round1);
+                        res_16bit1           = vminq_s16(res_16bit1, clip_pixel);
+                        res_16bit1           = vmaxq_s16(res_16bit1, zero);
+
+                        vst1q_u16(&dst[i * dst_stride + j], vreinterpretq_u16_s16(res_16bit0));
+                        vst1q_u16(&dst[i * dst_stride + j + dst_stride], vreinterpretq_u16_s16(res_16bit1));
+                    } else if (w == 4) {
+                        int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                        res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                        res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                        int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                        res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                        res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                        vst1_u16(&dst[i * dst_stride + j], vget_low_u16(vreinterpretq_u16_s16(res_a_round0_s16)));
+                        vst1_u16(&dst[i * dst_stride + j + dst_stride],
+                                 vget_low_u16(vreinterpretq_u16_s16(res_a_round1_s16)));
+                    } else {
+                        int16x8_t res_a_round0_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round0), res_a_round0);
+                        res_a_round0_s16           = vminq_s16(res_a_round0_s16, clip_pixel);
+                        res_a_round0_s16           = vmaxq_s16(res_a_round0_s16, zero);
+
+                        int16x8_t res_a_round1_s16 = vqmovn_high_s32(vqmovn_s32(res_a_round1), res_a_round1);
+                        res_a_round1_s16           = vminq_s16(res_a_round1_s16, clip_pixel);
+                        res_a_round1_s16           = vmaxq_s16(res_a_round1_s16, zero);
+
+                        *((uint32_t *)(&dst[i * dst_stride + j])) = vgetq_lane_u32(
+                            vreinterpretq_u32_s16(res_a_round0_s16), 0);
+                        *((uint32_t *)(&dst[i * dst_stride + j + dst_stride])) = vgetq_lane_u32(
+                            vreinterpretq_u32_s16(res_a_round1_s16), 0);
+                    }
+
+                    s[0] = s[1];
+                    s[1] = s[2];
+                    s[2] = s[3];
+
+                    s[4] = s[5];
+                    s[5] = s[6];
+                    s[6] = s[7];
+
+                    s[0 + 8] = s[1 + 8];
+                    s[1 + 8] = s[2 + 8];
+                    s[2 + 8] = s[3 + 8];
+
+                    s[4 + 8] = s[5 + 8];
+                    s[5 + 8] = s[6 + 8];
+                    s[6 + 8] = s[7 + 8];
+
+                    s6 = s8;
+                }
+            }
+        }
+    }
+}
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 72a75e6a6..bf6b865a5 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -887,8 +887,8 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_picture_average_kernel1_line, svt_picture_average_kernel1_line_c);
     SET_NEON(svt_av1_wiener_convolve_add_src, svt_av1_wiener_convolve_add_src_c, svt_av1_wiener_convolve_add_src_neon);
     SET_ONLY_C(svt_av1_convolve_2d_scale, svt_av1_convolve_2d_scale_c);
-    SET_ONLY_C(svt_av1_highbd_convolve_y_sr, svt_av1_highbd_convolve_y_sr_c);
     SET_NEON(svt_av1_highbd_convolve_2d_sr, svt_av1_highbd_convolve_2d_sr_c, svt_av1_highbd_convolve_2d_sr_neon);
+    SET_NEON(svt_av1_highbd_convolve_y_sr, svt_av1_highbd_convolve_y_sr_c, svt_av1_highbd_convolve_y_sr_neon);
     SET_ONLY_C(svt_av1_highbd_convolve_2d_scale, svt_av1_highbd_convolve_2d_scale_c);
     SET_ONLY_C(svt_av1_highbd_convolve_2d_copy_sr, svt_av1_highbd_convolve_2d_copy_sr_c);
     SET_NEON(svt_av1_highbd_jnt_convolve_2d, svt_av1_highbd_jnt_convolve_2d_c, svt_av1_highbd_jnt_convolve_2d_neon);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index dc1f1c970..6f0738d7c 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1379,6 +1379,7 @@ extern "C" {
                                            const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
                                            const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
     void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
+    void svt_av1_highbd_convolve_y_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
     void svt_av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
diff --git a/test/convolve_2d_test.cc b/test/convolve_2d_test.cc
index 847ff976e..9e4aa3f91 100644
--- a/test/convolve_2d_test.cc
+++ b/test/convolve_2d_test.cc
@@ -1125,7 +1125,7 @@ class AV1HbdSrConvolve2DTest : public AV1HbdConvolve2DTest {
         const int has_suby = TEST_GET_PARAM(2);
         const int fn_idx = TEST_GET_PARAM(3);
 
-#if defined(ARCH_X86_64)
+#ifdef ARCH_X86_64
 
         if (fn_idx == 0) {  // avx2
             if (has_subx == 1 && has_suby == 1)
@@ -1149,15 +1149,15 @@ class AV1HbdSrConvolve2DTest : public AV1HbdConvolve2DTest {
 
 #endif  // ARCH_X86_64
 
-#if defined(ARCH_AARCH64)
+#ifdef ARCH_AARCH64
 
-        if (fn_idx == 2) {  // neon
+        if (fn_idx == 2) {  // NEON
             if (has_subx == 1 && has_suby == 1)
                 func_tst_ = svt_av1_highbd_convolve_2d_sr_neon;
             // else if (has_subx == 1)                      Not yet implemented
             //     func_tst_ = svt_av1_highbd_convolve_x_sr_neon;
-            // else if (has_suby == 1)                      Not yet implemented
-            //     func_tst_ = svt_av1_highbd_convolve_y_sr_neon;
+            else if (has_suby == 1)
+                func_tst_ = svt_av1_highbd_convolve_y_sr_neon;
             // else                                         Not yet implemented
             //     func_tst_ = svt_av1_highbd_convolve_2d_copy_sr_neon;
         }
@@ -1178,16 +1178,7 @@ TEST_P(AV1HbdSrConvolve2DTest, DISABLED_SpeedTest) {
     speed_test();
 }
 
-#if defined(ARCH_X86_64)
-
-INSTANTIATE_TEST_SUITE_P(ConvolveTestX, AV1HbdSrConvolve2DTest,
-                         BuildParams(1, 0, 0, 1));
-INSTANTIATE_TEST_SUITE_P(ConvolveTest2D, AV1HbdSrConvolve2DTest,
-                         BuildParams(1, 1, 0, 1));
-INSTANTIATE_TEST_SUITE_P(ConvolveTestY, AV1HbdSrConvolve2DTest,
-                         BuildParams(0, 1, 0, 1));
-INSTANTIATE_TEST_SUITE_P(ConvolveTestCopy, AV1HbdSrConvolve2DTest,
-                         BuildParams(0, 0, 0, 1));
+#ifdef ARCH_X86_64
 
 INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestX, AV1HbdSrConvolve2DTest,
                         BuildParams(1, 0, 1, 1));
@@ -1198,18 +1189,26 @@ INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestY, AV1HbdSrConvolve2DTest,
 INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
                         BuildParams(0, 0, 1, 1));
 
+INSTANTIATE_TEST_CASE_P(ConvolveTestX, AV1HbdSrConvolve2DTest,
+                        BuildParams(1, 0, 0, 1));
+INSTANTIATE_TEST_CASE_P(ConvolveTest2D, AV1HbdSrConvolve2DTest,
+                        BuildParams(1, 1, 0, 1));
+INSTANTIATE_TEST_CASE_P(ConvolveTestY, AV1HbdSrConvolve2DTest,
+                        BuildParams(0, 1, 0, 1));
+INSTANTIATE_TEST_CASE_P(ConvolveTestCopy, AV1HbdSrConvolve2DTest,
+                        BuildParams(0, 0, 0, 1));
+
 #endif  // ARCH_X86_64
 
-#if defined(ARCH_AARCH64)
+#ifdef ARCH_AARCH64
 
 // INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestX, AV1HbdSrConvolve2DTest,
 //                         BuildParams(1, 0, 2, 1));            Not yet
 //                         implemented
 INSTANTIATE_TEST_CASE_P(NEON_ConvolveTest2D, AV1HbdSrConvolve2DTest,
                         BuildParams(1, 1, 2, 1));
-// INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdSrConvolve2DTest,
-//                         BuildParams(0, 1, 2, 1));            Not yet
-//                         implemented
+INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdSrConvolve2DTest,
+                        BuildParams(0, 1, 2, 1));
 // INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
 //                         BuildParams(0, 0, 2, 1));            Not yet
 //                         implemented
-- 
GitLab


From 6159fba5530c2322634cb8edf804f332c9095336 Mon Sep 17 00:00:00 2001
From: Gerardo Puga <glpuga@gmail.com>
Date: Fri, 17 May 2024 21:15:17 +0000
Subject: [PATCH 07/13] Break accum dependency in variance_large_neon

---
 Source/Lib/Encoder/ASM_NEON/variance_neon.c | 61 +++++++++++++------
 test/CMakeLists.txt                         |  6 +-
 test/SadTest.cc                             |  6 ++
 test/VarianceTest.cc                        | 65 +++++++++++++++++++--
 4 files changed, 112 insertions(+), 26 deletions(-)

diff --git a/Source/Lib/Encoder/ASM_NEON/variance_neon.c b/Source/Lib/Encoder/ASM_NEON/variance_neon.c
index f128e8011..946959f09 100644
--- a/Source/Lib/Encoder/ASM_NEON/variance_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/variance_neon.c
@@ -108,7 +108,7 @@ static INLINE void variance_16xh_neon(const uint8_t *src, int src_stride, const
 static INLINE void variance_large_neon(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride, int w,
                                        int h, int h_limit, uint32_t *sse, int *sum) {
     int32x4_t sum_s32    = vdupq_n_s32(0);
-    int32x4_t sse_s32[2] = {vdupq_n_s32(0), vdupq_n_s32(0)};
+    int32x4_t sse_s32[4] = {vdupq_n_s32(0), vdupq_n_s32(0), vdupq_n_s32(0), vdupq_n_s32(0)};
 
     // 'h_limit' is the number of 'w'-width rows we can process before our 16-bit
     // accumulator overflows. After hitting this limit we accumulate into 32-bit
@@ -116,27 +116,53 @@ static INLINE void variance_large_neon(const uint8_t *src, int src_stride, const
     int h_tmp = h > h_limit ? h_limit : h;
 
     int i = 0;
+
+    const bool odd_chunks = (w / 16) % 2;
+
     do {
         int16x8_t sum_s16[2] = {vdupq_n_s16(0), vdupq_n_s16(0)};
         do {
             int j = 0;
             do {
-                uint8x16_t s = vld1q_u8(src + j);
-                uint8x16_t r = vld1q_u8(ref + j);
-
-                int16x8_t diff_l = vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(s), vget_low_u8(r)));
-                int16x8_t diff_h = vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(s), vget_high_u8(r)));
+                {
+                    uint8x16_t s      = vld1q_u8(src + j);
+                    uint8x16_t r      = vld1q_u8(ref + j);
+                    int16x8_t  diff_l = vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(s), vget_low_u8(r)));
+                    int16x8_t  diff_h = vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(s), vget_high_u8(r)));
+                    sum_s16[0]        = vaddq_s16(sum_s16[0], diff_l);
+                    sum_s16[1]        = vaddq_s16(sum_s16[1], diff_h);
+                    sse_s32[0]        = vmlal_s16(sse_s32[0], vget_low_s16(diff_l), vget_low_s16(diff_l));
+                    sse_s32[1]        = vmlal_s16(sse_s32[1], vget_high_s16(diff_l), vget_high_s16(diff_l));
+                    sse_s32[2]        = vmlal_s16(sse_s32[2], vget_low_s16(diff_h), vget_low_s16(diff_h));
+                    sse_s32[3]        = vmlal_s16(sse_s32[3], vget_high_s16(diff_h), vget_high_s16(diff_h));
+                }
+                {
+                    uint8x16_t s      = vld1q_u8(src + j + 16);
+                    uint8x16_t r      = vld1q_u8(ref + j + 16);
+                    int16x8_t  diff_l = vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(s), vget_low_u8(r)));
+                    int16x8_t  diff_h = vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(s), vget_high_u8(r)));
+                    sum_s16[0]        = vaddq_s16(sum_s16[0], diff_l);
+                    sum_s16[1]        = vaddq_s16(sum_s16[1], diff_h);
+                    sse_s32[0]        = vmlal_s16(sse_s32[0], vget_low_s16(diff_l), vget_low_s16(diff_l));
+                    sse_s32[1]        = vmlal_s16(sse_s32[1], vget_high_s16(diff_l), vget_high_s16(diff_l));
+                    sse_s32[2]        = vmlal_s16(sse_s32[2], vget_low_s16(diff_h), vget_low_s16(diff_h));
+                    sse_s32[3]        = vmlal_s16(sse_s32[3], vget_high_s16(diff_h), vget_high_s16(diff_h));
+                }
+                j += 32;
+            } while (j < (w - 16));
 
-                sum_s16[0] = vaddq_s16(sum_s16[0], diff_l);
-                sum_s16[1] = vaddq_s16(sum_s16[1], diff_h);
-
-                sse_s32[0] = vmlal_s16(sse_s32[0], vget_low_s16(diff_l), vget_low_s16(diff_l));
-                sse_s32[1] = vmlal_s16(sse_s32[1], vget_high_s16(diff_l), vget_high_s16(diff_l));
-                sse_s32[0] = vmlal_s16(sse_s32[0], vget_low_s16(diff_h), vget_low_s16(diff_h));
-                sse_s32[1] = vmlal_s16(sse_s32[1], vget_high_s16(diff_h), vget_high_s16(diff_h));
-
-                j += 16;
-            } while (j < w);
+            if (odd_chunks) {
+                uint8x16_t s      = vld1q_u8(src + j);
+                uint8x16_t r      = vld1q_u8(ref + j);
+                int16x8_t  diff_l = vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(s), vget_low_u8(r)));
+                int16x8_t  diff_h = vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(s), vget_high_u8(r)));
+                sum_s16[0]        = vaddq_s16(sum_s16[0], diff_l);
+                sum_s16[1]        = vaddq_s16(sum_s16[1], diff_h);
+                sse_s32[0]        = vmlal_s16(sse_s32[0], vget_low_s16(diff_l), vget_low_s16(diff_l));
+                sse_s32[1]        = vmlal_s16(sse_s32[1], vget_high_s16(diff_l), vget_high_s16(diff_l));
+                sse_s32[2]        = vmlal_s16(sse_s32[2], vget_low_s16(diff_h), vget_low_s16(diff_h));
+                sse_s32[3]        = vmlal_s16(sse_s32[3], vget_high_s16(diff_h), vget_high_s16(diff_h));
+            }
 
             src += src_stride;
             ref += ref_stride;
@@ -150,7 +176,8 @@ static INLINE void variance_large_neon(const uint8_t *src, int src_stride, const
     } while (i < h);
 
     *sum = horizontal_add_s32x4(sum_s32);
-    *sse = (uint32_t)horizontal_add_s32x4(vaddq_s32(sse_s32[0], sse_s32[1]));
+    *sse = (uint32_t)horizontal_add_s32x4(
+        vaddq_s32(vaddq_s32(sse_s32[0], sse_s32[1]), vaddq_s32(sse_s32[2], sse_s32[3])));
 }
 
 static INLINE void variance_32xh_neon(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride, int h,
diff --git a/test/CMakeLists.txt b/test/CMakeLists.txt
index 9cad777c9..dc3b675a3 100644
--- a/test/CMakeLists.txt
+++ b/test/CMakeLists.txt
@@ -87,16 +87,17 @@ set(arch_neutral_files
     ../third_party/aom_dsp/src/entdec.c)
 
 set(multi_arch_files
+    CdefTest.cc
     FwdTxfm2dAsmTest.cc
     InvTxfm2dAsmTest.cc
     PackUnPackTest.cc
     PictureOperatorTest.cc
+    RestorationPickTest.cc
     SadTest.cc
     selfguided_filter_test.cc
-    CdefTest.cc
-    RestorationPickTest.cc
     SpatialFullDistortionTest.cc
     TemporalFilterTestPlanewise.cc
+    VarianceTest.cc
     convolve_2d_test.cc
     RestorationPickTest.cc
     selfguided_filter_test.cc
@@ -129,7 +130,6 @@ if(HAVE_X86_PLATFORM)
         QuantAsmTest.cc
         ResidualTest.cc
         SelfGuidedUtilTest.cc
-        VarianceTest.cc
         WedgeUtilTest.cc
         av1_convolve_scale_test.cc
         compute_mean_test.cc
diff --git a/test/SadTest.cc b/test/SadTest.cc
index 0dad22643..609f23966 100644
--- a/test/SadTest.cc
+++ b/test/SadTest.cc
@@ -614,6 +614,9 @@ class sad_LoopTest : public ::testing::WithParamInterface<sad_LoopTestParam>,
 #if EN_AVX512_SUPPORT
             svt_sad_loop_kernel_avx512_intrin
 #endif
+#endif
+#ifdef ARCH_AARCH64
+                svt_sad_loop_kernel_neon,
 #endif
         };
 
@@ -698,6 +701,9 @@ class sad_LoopTest : public ::testing::WithParamInterface<sad_LoopTestParam>,
 #if EN_AVX512_SUPPORT
             svt_sad_loop_kernel_avx512_intrin
 #endif
+#endif
+#ifdef ARCH_AARCH64
+                svt_sad_loop_kernel_neon,
 #endif
         };
 
diff --git a/test/VarianceTest.cc b/test/VarianceTest.cc
index b1be6458e..e28fa06ea 100644
--- a/test/VarianceTest.cc
+++ b/test/VarianceTest.cc
@@ -128,12 +128,16 @@ TEST_P(MseTest, MaxTest) {
     run_max_test();
 };
 
-INSTANTIATE_TEST_SUITE_P(
+#ifdef ARCH_X86_64
+
+INSTANTIATE_TEST_CASE_P(
     Variance, MseTest,
     ::testing::Values(
         TestMseParam(16, 16, &svt_aom_mse16x16_sse2, &svt_aom_mse16x16_c),
         TestMseParam(16, 16, &svt_aom_mse16x16_avx2, &svt_aom_mse16x16_c)));
 
+#endif
+
 class MseTestHighbd : public ::testing::TestWithParam<TestMseParamHighbd> {
   public:
     MseTestHighbd()
@@ -217,10 +221,14 @@ TEST_P(MseTestHighbd, MaxTest) {
     run_max_test();
 };
 
-INSTANTIATE_TEST_SUITE_P(Variance, MseTestHighbd,
-                         ::testing::Values(TestMseParamHighbd(
-                             16, 16, &svt_aom_highbd_8_mse16x16_sse2,
-                             &svt_aom_highbd_8_mse16x16_c)));
+#ifdef ARCH_X86_64
+
+INSTANTIATE_TEST_CASE_P(Variance, MseTestHighbd,
+                        ::testing::Values(TestMseParamHighbd(
+                            16, 16, &svt_aom_highbd_8_mse16x16_sse2,
+                            &svt_aom_highbd_8_mse16x16_c)));
+
+#endif  // ARCH_X86_64
 
 // sum of squares test
 static uint32_t mb_ss_ref(const int16_t *src) {
@@ -401,7 +409,9 @@ TEST_P(VarianceTest, OneQuarterTest) {
     run_one_quarter_test();
 };
 
-INSTANTIATE_TEST_SUITE_P(
+#ifdef ARCH_X86_64
+
+INSTANTIATE_TEST_CASE_P(
     Variance, VarianceTest,
     ::testing::Values(
         VarianceParam(4, 4, &svt_aom_variance4x4_c, &svt_aom_variance4x4_sse2),
@@ -503,6 +513,45 @@ INSTANTIATE_TEST_SUITE_P(
                                     &svt_aom_variance128x128_avx512)));
 #endif
 
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_AARCH64
+
+INSTANTIATE_TEST_CASE_P(
+    Variance_NEON, VarianceTest,
+    ::testing::Values(VarianceParam(16, 4, &svt_aom_variance16x4_c,
+                                    &svt_aom_variance16x4_neon),
+                      VarianceParam(16, 8, &svt_aom_variance16x8_c,
+                                    &svt_aom_variance16x8_neon),
+                      VarianceParam(16, 16, &svt_aom_variance16x16_c,
+                                    &svt_aom_variance16x16_neon),
+                      VarianceParam(16, 32, &svt_aom_variance16x32_c,
+                                    &svt_aom_variance16x32_neon),
+                      VarianceParam(16, 64, &svt_aom_variance16x64_c,
+                                    &svt_aom_variance16x64_neon),
+                      VarianceParam(32, 8, &svt_aom_variance32x8_c,
+                                    &svt_aom_variance32x8_neon),
+                      VarianceParam(32, 16, &svt_aom_variance32x16_c,
+                                    &svt_aom_variance32x16_neon),
+                      VarianceParam(32, 32, &svt_aom_variance32x32_c,
+                                    &svt_aom_variance32x32_neon),
+                      VarianceParam(32, 64, &svt_aom_variance32x64_c,
+                                    &svt_aom_variance32x64_neon),
+                      VarianceParam(64, 16, &svt_aom_variance64x16_c,
+                                    &svt_aom_variance64x16_neon),
+                      VarianceParam(64, 32, &svt_aom_variance64x32_c,
+                                    &svt_aom_variance64x32_neon),
+                      VarianceParam(64, 64, &svt_aom_variance64x64_c,
+                                    &svt_aom_variance64x64_neon),
+                      VarianceParam(64, 128, &svt_aom_variance64x128_c,
+                                    &svt_aom_variance64x128_neon),
+                      VarianceParam(128, 64, &svt_aom_variance128x64_c,
+                                    &svt_aom_variance128x64_neon),
+                      VarianceParam(128, 128, &svt_aom_variance128x128_c,
+                                    &svt_aom_variance128x128_neon)));
+
+#endif  // ARCH_AARCH64
+
 typedef unsigned int (*SubpixVarMxNFunc)(const uint8_t *a, int a_stride,
                                          int xoffset, int yoffset,
                                          const uint8_t *b, int b_stride,
@@ -639,6 +688,8 @@ void AvxSubpelVarianceTest::ExtremeRefTest() {
     }
 }
 
+#ifdef ARCH_X86_64
+
 const TestParams kArraySubpelVariance_sse2[] = {
     {7,
      7,
@@ -982,6 +1033,8 @@ INSTANTIATE_TEST_SUITE_P(AVX512, AvxSubpelVarianceTest,
                          ::testing::ValuesIn(kArraySubpelVariance_avx512));
 #endif
 
+#endif  // ARCH_X86_64
+
 TEST_P(AvxSubpelVarianceTest, Ref) {
     RefTest();
 }
-- 
GitLab


From 557ff85054d00150e599751e2be4038c069d2703 Mon Sep 17 00:00:00 2001
From: Gerardo Puga <glpuga@gmail.com>
Date: Wed, 22 May 2024 20:29:43 +0000
Subject: [PATCH 08/13] sad16_neon loop improvements

---
 .../ASM_NEON/highbd_jnt_convolve_neon.c       |  18 ++-
 .../Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c  | 141 ++++++++++--------
 test/SadTest.cc                               |   4 +-
 3 files changed, 88 insertions(+), 75 deletions(-)

diff --git a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
index 01f3313ca..8d7a7005d 100644
--- a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
+++ b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
@@ -156,12 +156,12 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
                 s[7 + 8] = vzip2q_u16(s7, s8);
 
                 const int32x4_t res_a0       = svt_aom_convolve(s, coeffs_y);
-                int32x4_t       res_a_round0 = vshlq_n_s32(res_a0, FILTER_BITS - conv_params->round_0);
-                res_a_round0 = vshrq_n_s32(vaddq_s32(res_a_round0, round_const_y), conv_params->round_1);
+                int32x4_t       res_a_round0 = vshlq_s32(res_a0, vdupq_n_s32(FILTER_BITS - conv_params->round_0));
+                res_a_round0 = vshlq_s32(vaddq_s32(res_a_round0, round_const_y), vdupq_n_s32(-conv_params->round_1));
 
                 const int32x4_t res_a1       = svt_aom_convolve(s + 8, coeffs_y);
-                int32x4_t       res_a_round1 = vshlq_n_s32(res_a1, FILTER_BITS - conv_params->round_0);
-                res_a_round1 = vshrq_n_s32(vaddq_s32(res_a_round1, round_const_y), conv_params->round_1);
+                int32x4_t       res_a_round1 = vshlq_s32(res_a1, vdupq_n_s32(FILTER_BITS - conv_params->round_0));
+                res_a_round1 = vshlq_s32(vaddq_s32(res_a_round1, round_const_y), vdupq_n_s32(-conv_params->round_1));
 
                 const int32x4_t res_unsigned_lo_0 = vaddq_s32(res_a_round0, offset_const);
                 const int32x4_t res_unsigned_lo_1 = vaddq_s32(res_a_round1, offset_const);
@@ -205,12 +205,14 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
                     }
                 } else {
                     const int32x4_t res_b0       = svt_aom_convolve(s + 4, coeffs_y);
-                    int32x4_t       res_b_round0 = vshlq_n_s32(res_b0, FILTER_BITS - conv_params->round_0);
-                    res_b_round0 = vshrq_n_s32(vaddq_s32(res_b_round0, round_const_y), conv_params->round_1);
+                    int32x4_t       res_b_round0 = vshlq_s32(res_b0, vdupq_n_s32(FILTER_BITS - conv_params->round_0));
+                    res_b_round0                 = vshlq_s32(vaddq_s32(res_b_round0, round_const_y),
+                                             vdupq_n_s32(-conv_params->round_1));
 
                     const int32x4_t res_b1       = svt_aom_convolve(s + 4 + 8, coeffs_y);
-                    int32x4_t       res_b_round1 = vshlq_n_s32(res_b1, FILTER_BITS - conv_params->round_0);
-                    res_b_round1 = vshrq_n_s32(vaddq_s32(res_b_round1, round_const_y), conv_params->round_1);
+                    int32x4_t       res_b_round1 = vshlq_s32(res_b1, vdupq_n_s32(FILTER_BITS - conv_params->round_0));
+                    res_b_round1                 = vshlq_s32(vaddq_s32(res_b_round1, round_const_y),
+                                             vdupq_n_s32(-conv_params->round_1));
 
                     int32x4_t res_unsigned_hi_0 = vaddq_s32(res_b_round0, offset_const);
                     int32x4_t res_unsigned_hi_1 = vaddq_s32(res_b_round1, offset_const);
diff --git a/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c
index 4865fcab3..2090fb57b 100644
--- a/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_neon.c
@@ -18,10 +18,14 @@
 #include "EbUtility.h"
 #include "mcomp.h"
 
+static INLINE uint16x8_t sad16_neon_init(uint8x16_t src, uint8x16_t ref) {
+    const uint8x16_t abs_diff = vabdq_u8(src, ref);
+    return vpaddlq_u8(abs_diff);
+}
+
 static INLINE void sad16_neon(uint8x16_t src, uint8x16_t ref, uint16x8_t *const sad_sum) {
-    uint8x16_t abs_diff;
-    abs_diff = vabdq_u8(src, ref);
-    *sad_sum = vpadalq_u8(*sad_sum, abs_diff);
+    const uint8x16_t abs_diff = vabdq_u8(src, ref);
+    *sad_sum                  = vpadalq_u8(*sad_sum, abs_diff);
 }
 
 /* Find the position of the first occurrence of 'value' in the vector 'x'.
@@ -381,8 +385,7 @@ static INLINE uint32x4_t sadwxhx4d_large_neon(const uint8_t *src, uint32_t src_s
                                               uint32_t ref_stride, uint32_t w, uint32_t h, uint32_t h_overflow) {
     uint32x4_t sum[4];
     uint16x8_t sum_lo[4], sum_hi[4];
-    uint8x16_t s0, s1;
-    uint32_t   i, j, ref_offset, h_limit;
+    uint32_t   h_limit;
 
     sum[0] = vdupq_n_u32(0);
     sum[1] = vdupq_n_u32(0);
@@ -391,8 +394,7 @@ static INLINE uint32x4_t sadwxhx4d_large_neon(const uint8_t *src, uint32_t src_s
 
     h_limit = h > h_overflow ? h_overflow : h;
 
-    ref_offset = 0;
-    i          = 0;
+    uint32_t i = 0;
     do {
         sum_lo[0] = vdupq_n_u16(0);
         sum_lo[1] = vdupq_n_u16(0);
@@ -403,25 +405,28 @@ static INLINE uint32x4_t sadwxhx4d_large_neon(const uint8_t *src, uint32_t src_s
         sum_hi[2] = vdupq_n_u16(0);
         sum_hi[3] = vdupq_n_u16(0);
         do {
-            j = 0;
+            const uint8_t       *loop_src       = src;
+            const uint8_t       *loop_ref       = ref;
+            const uint8_t *const loop_src_limit = loop_src + w;
             do {
-                s0 = vld1q_u8(src + j);
-                sad16_neon(s0, vld1q_u8(ref + 0 + ref_offset + j), &sum_lo[0]);
-                sad16_neon(s0, vld1q_u8(ref + 1 + ref_offset + j), &sum_lo[1]);
-                sad16_neon(s0, vld1q_u8(ref + 2 + ref_offset + j), &sum_lo[2]);
-                sad16_neon(s0, vld1q_u8(ref + 3 + ref_offset + j), &sum_lo[3]);
-
-                s1 = vld1q_u8(src + j + 16);
-                sad16_neon(s1, vld1q_u8(ref + 0 + ref_offset + j + 16), &sum_hi[0]);
-                sad16_neon(s1, vld1q_u8(ref + 1 + ref_offset + j + 16), &sum_hi[1]);
-                sad16_neon(s1, vld1q_u8(ref + 2 + ref_offset + j + 16), &sum_hi[2]);
-                sad16_neon(s1, vld1q_u8(ref + 3 + ref_offset + j + 16), &sum_hi[3]);
-
-                j += 32;
-            } while (j < w);
+                const uint8x16_t s0 = vld1q_u8(loop_src);
+                sad16_neon(s0, vld1q_u8(loop_ref + 0), &sum_lo[0]);
+                sad16_neon(s0, vld1q_u8(loop_ref + 1), &sum_lo[1]);
+                sad16_neon(s0, vld1q_u8(loop_ref + 2), &sum_lo[2]);
+                sad16_neon(s0, vld1q_u8(loop_ref + 3), &sum_lo[3]);
+
+                const uint8x16_t s1 = vld1q_u8(loop_src + 16);
+                sad16_neon(s1, vld1q_u8(loop_ref + 0 + 16), &sum_hi[0]);
+                sad16_neon(s1, vld1q_u8(loop_ref + 1 + 16), &sum_hi[1]);
+                sad16_neon(s1, vld1q_u8(loop_ref + 2 + 16), &sum_hi[2]);
+                sad16_neon(s1, vld1q_u8(loop_ref + 3 + 16), &sum_hi[3]);
+
+                loop_src += 32;
+                loop_ref += 32;
+            } while (loop_src < loop_src_limit);
 
             src += src_stride;
-            ref_offset += ref_stride;
+            ref += ref_stride;
         } while (++i < h_limit);
 
         sum[0] = vpadalq_u16(sum[0], sum_lo[0]);
@@ -452,36 +457,40 @@ static INLINE uint32x4_t sad64xhx4d_neon(const uint8_t *src, uint32_t src_stride
 static INLINE uint32x4_t sad32xhx4d_neon(const uint8_t *src, uint32_t src_stride, const uint8_t *ref,
                                          uint32_t ref_stride, uint32_t h) {
     uint16x8_t sum_lo[4], sum_hi[4];
-    uint8x16_t s0, s1;
-    uint32_t   i, ref_offset;
 
-    sum_lo[0] = vdupq_n_u16(0);
-    sum_lo[1] = vdupq_n_u16(0);
-    sum_lo[2] = vdupq_n_u16(0);
-    sum_lo[3] = vdupq_n_u16(0);
-    sum_hi[0] = vdupq_n_u16(0);
-    sum_hi[1] = vdupq_n_u16(0);
-    sum_hi[2] = vdupq_n_u16(0);
-    sum_hi[3] = vdupq_n_u16(0);
+    const uint8x16_t s0_init = vld1q_u8(src);
+    sum_lo[0]                = sad16_neon_init(s0_init, vld1q_u8(ref + 0));
+    sum_lo[1]                = sad16_neon_init(s0_init, vld1q_u8(ref + 1));
+    sum_lo[2]                = sad16_neon_init(s0_init, vld1q_u8(ref + 2));
+    sum_lo[3]                = sad16_neon_init(s0_init, vld1q_u8(ref + 3));
 
-    ref_offset = 0;
-    i          = h;
-    do {
-        s0 = vld1q_u8(src);
-        sad16_neon(s0, vld1q_u8(ref + 0 + ref_offset), &sum_lo[0]);
-        sad16_neon(s0, vld1q_u8(ref + 1 + ref_offset), &sum_lo[1]);
-        sad16_neon(s0, vld1q_u8(ref + 2 + ref_offset), &sum_lo[2]);
-        sad16_neon(s0, vld1q_u8(ref + 3 + ref_offset), &sum_lo[3]);
-
-        s1 = vld1q_u8(src + 16);
-        sad16_neon(s1, vld1q_u8(ref + 0 + ref_offset + 16), &sum_hi[0]);
-        sad16_neon(s1, vld1q_u8(ref + 1 + ref_offset + 16), &sum_hi[1]);
-        sad16_neon(s1, vld1q_u8(ref + 2 + ref_offset + 16), &sum_hi[2]);
-        sad16_neon(s1, vld1q_u8(ref + 3 + ref_offset + 16), &sum_hi[3]);
+    const uint8x16_t s1_init = vld1q_u8(src + 16);
+    sum_hi[0]                = sad16_neon_init(s1_init, vld1q_u8(ref + 0 + 16));
+    sum_hi[1]                = sad16_neon_init(s1_init, vld1q_u8(ref + 1 + 16));
+    sum_hi[2]                = sad16_neon_init(s1_init, vld1q_u8(ref + 2 + 16));
+    sum_hi[3]                = sad16_neon_init(s1_init, vld1q_u8(ref + 3 + 16));
+
+    const uint8_t *const src_limit = src + src_stride * h;
+
+    src += src_stride;
+    ref += ref_stride;
+
+    while (src < src_limit) {
+        const uint8x16_t s0 = vld1q_u8(src);
+        sad16_neon(s0, vld1q_u8(ref + 0), &sum_lo[0]);
+        sad16_neon(s0, vld1q_u8(ref + 1), &sum_lo[1]);
+        sad16_neon(s0, vld1q_u8(ref + 2), &sum_lo[2]);
+        sad16_neon(s0, vld1q_u8(ref + 3), &sum_lo[3]);
+
+        const uint8x16_t s1 = vld1q_u8(src + 16);
+        sad16_neon(s1, vld1q_u8(ref + 0 + 16), &sum_hi[0]);
+        sad16_neon(s1, vld1q_u8(ref + 1 + 16), &sum_hi[1]);
+        sad16_neon(s1, vld1q_u8(ref + 2 + 16), &sum_hi[2]);
+        sad16_neon(s1, vld1q_u8(ref + 3 + 16), &sum_hi[3]);
 
         src += src_stride;
-        ref_offset += ref_stride;
-    } while (--i != 0);
+        ref += ref_stride;
+    }
 
     return horizontal_long_add_4d_u16x8(sum_lo, sum_hi);
 }
@@ -490,26 +499,28 @@ static INLINE uint32x4_t sad16xhx4d_neon(const uint8_t *src, uint32_t src_stride
                                          uint32_t ref_stride, uint32_t h) {
     uint16x8_t sum_u16[4];
     uint32x4_t sum_u32[4];
-    uint8x16_t s;
-    uint32_t   ref_offset, i;
 
-    sum_u16[0] = vdupq_n_u16(0);
-    sum_u16[1] = vdupq_n_u16(0);
-    sum_u16[2] = vdupq_n_u16(0);
-    sum_u16[3] = vdupq_n_u16(0);
+    const uint8x16_t s_init = vld1q_u8(src);
+    sum_u16[0]              = sad16_neon_init(s_init, vld1q_u8(ref + 0));
+    sum_u16[1]              = sad16_neon_init(s_init, vld1q_u8(ref + 1));
+    sum_u16[2]              = sad16_neon_init(s_init, vld1q_u8(ref + 2));
+    sum_u16[3]              = sad16_neon_init(s_init, vld1q_u8(ref + 3));
 
-    ref_offset = 0;
-    i          = h;
-    do {
-        s = vld1q_u8(src);
-        sad16_neon(s, vld1q_u8(ref + 0 + ref_offset), &sum_u16[0]);
-        sad16_neon(s, vld1q_u8(ref + 1 + ref_offset), &sum_u16[1]);
-        sad16_neon(s, vld1q_u8(ref + 2 + ref_offset), &sum_u16[2]);
-        sad16_neon(s, vld1q_u8(ref + 3 + ref_offset), &sum_u16[3]);
+    const uint8_t *const src_limit = src + src_stride * h;
+
+    src += src_stride;
+    ref += ref_stride;
+
+    while (src < src_limit) {
+        const uint8x16_t s = vld1q_u8(src);
+        sad16_neon(s, vld1q_u8(ref + 0), &sum_u16[0]);
+        sad16_neon(s, vld1q_u8(ref + 1), &sum_u16[1]);
+        sad16_neon(s, vld1q_u8(ref + 2), &sum_u16[2]);
+        sad16_neon(s, vld1q_u8(ref + 3), &sum_u16[3]);
 
         src += src_stride;
-        ref_offset += ref_stride;
-    } while (--i != 0);
+        ref += ref_stride;
+    }
 
     sum_u32[0] = vpaddlq_u16(sum_u16[0]);
     sum_u32[1] = vpaddlq_u16(sum_u16[1]);
diff --git a/test/SadTest.cc b/test/SadTest.cc
index 609f23966..85e2020eb 100644
--- a/test/SadTest.cc
+++ b/test/SadTest.cc
@@ -616,7 +616,7 @@ class sad_LoopTest : public ::testing::WithParamInterface<sad_LoopTestParam>,
 #endif
 #endif
 #ifdef ARCH_AARCH64
-                svt_sad_loop_kernel_neon,
+                svt_sad_loop_kernel_neon
 #endif
         };
 
@@ -703,7 +703,7 @@ class sad_LoopTest : public ::testing::WithParamInterface<sad_LoopTestParam>,
 #endif
 #endif
 #ifdef ARCH_AARCH64
-                svt_sad_loop_kernel_neon,
+                svt_sad_loop_kernel_neon
 #endif
         };
 
-- 
GitLab


From 7cc33a12c0fcb46c3cf2c55ee6db37c4001157c3 Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Tue, 21 May 2024 15:26:44 +0000
Subject: [PATCH 09/13] NEON port of svt_full_distortion_kernel16_bits_neon and
 svt_av1_apply_temporal_filter_planewise_medium_hbd_c

---
 .../EbPictureOperators_Intrinsic_neon.c       | 110 ++++++++++++++++++
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   1 +
 test/SpatialFullDistortionTest.cc             |  11 ++
 4 files changed, 123 insertions(+), 1 deletion(-)

diff --git a/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c b/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
index 98af0b0fe..3dd8b8c6c 100644
--- a/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
+++ b/Source/Lib/Common/ASM_NEON/EbPictureOperators_Intrinsic_neon.c
@@ -287,6 +287,116 @@ void svt_aom_hadamard_32x32_neon(const int16_t *src_diff, ptrdiff_t src_stride,
     }
 }
 
+static INLINE uint32x4_t full_distortion_kernel4_neon_intrin(const uint16_t *const input, const uint16_t *const recon) {
+    const uint16x4_t in      = vld1_u16(input);
+    const uint16x4_t re      = vld1_u16(recon);
+    const uint16x4_t max     = vmax_u16(in, re);
+    const uint16x4_t min     = vmin_u16(in, re);
+    const uint16x4_t diff    = vsub_u16(max, min);
+    const uint32x4_t diff_32 = vmull_u16(diff, diff);
+    return diff_32;
+}
+
+static INLINE uint32x4_t full_distortion_kernel8_neon_intrin(uint16x8_t in, uint16x8_t re) {
+    const uint16x8_t max     = vmaxq_u16(in, re);
+    const uint16x8_t min     = vminq_u16(in, re);
+    const uint16x8_t diff    = vsubq_u16(max, min);
+    const uint32x4_t diff_32 = vpaddq_u32(vmull_u16(vget_low_u16(diff), vget_low_u16(diff)),
+                                          vmull_high_u16(diff, diff));
+    return diff_32;
+}
+
+uint64_t svt_full_distortion_kernel16_bits_neon(uint8_t *input, uint32_t input_offset, uint32_t input_stride,
+                                                uint8_t *recon, int32_t recon_offset, uint32_t recon_stride,
+                                                uint32_t area_width, uint32_t area_height) {
+    const uint32_t leftover    = area_width & 7; // area_width % 8
+    int64x2_t      sum64       = vdupq_n_s64(0);
+    uint16_t      *input_16bit = (uint16_t *)input;
+    uint16_t      *recon_16bit = (uint16_t *)recon;
+    input_16bit += input_offset;
+    recon_16bit += recon_offset;
+
+    const uint32x4_t zero = vdupq_n_u32(0);
+
+    if (leftover) { // (leftover == 4)
+        const uint16_t       *inp     = input_16bit + area_width - leftover;
+        const uint16_t       *rec     = recon_16bit + area_width - leftover;
+        const uint16_t *const inp_end = inp + area_height * input_stride;
+
+        do {
+            const uint32x4_t sum32 = full_distortion_kernel4_neon_intrin(inp, rec);
+            inp += input_stride;
+            rec += recon_stride;
+
+            sum64 = vaddq_s64(sum64,
+                              vaddq_s64(vreinterpretq_s64_u32(vzip1q_u32(sum32, zero)),
+                                        vreinterpretq_s64_u32(vzip2q_u32(sum32, zero))));
+        } while (inp < inp_end);
+    }
+
+    area_width -= leftover; // area width will be now a multiple of 8
+
+    if (area_width) {
+        const uint16_t *inp = input_16bit;
+        const uint16_t *rec = recon_16bit;
+
+        if (area_width == 8) {
+            const uint16_t *const inp_end = inp + area_height * input_stride;
+            do {
+                const uint32x4_t sum32 = full_distortion_kernel8_neon_intrin(vld1q_u16(inp), vld1q_u16(rec));
+                inp += input_stride;
+                rec += recon_stride;
+
+                sum64 = vaddq_s64(sum64,
+                                  vaddq_s64(vreinterpretq_s64_u32(vzip1q_u32(sum32, zero)),
+                                            vreinterpretq_s64_u32(vzip2q_u32(sum32, zero))));
+            } while (inp < inp_end);
+
+        } else if (area_width == 16) {
+            const uint16_t *const inp_end = inp + area_height * input_stride;
+            do {
+                const uint32x4_t sum32 = vaddq_u32(
+                    full_distortion_kernel8_neon_intrin(vld1q_u16(inp), vld1q_u16(rec)),
+                    full_distortion_kernel8_neon_intrin(vld1q_u16(inp + 8), vld1q_u16(rec + 8)));
+                inp += input_stride;
+                rec += recon_stride;
+                sum64 = vaddq_s64(sum64,
+                                  vaddq_s64(vreinterpretq_s64_u32(vzip1q_u32(sum32, zero)),
+                                            vreinterpretq_s64_u32(vzip2q_u32(sum32, zero))));
+            } while (inp < inp_end);
+        } else {
+            for (uint32_t h = 0; h < area_height; h++) {
+                uint32_t w = 0;
+
+                for (; w + 16 < area_width; w += 16) {
+                    const uint32x4_t sum32_0 = full_distortion_kernel8_neon_intrin(vld1q_u16(inp + w),
+                                                                                   vld1q_u16(rec + w));
+                    const uint32x4_t sum32_1 = full_distortion_kernel8_neon_intrin(vld1q_u16(inp + w + 8),
+                                                                                   vld1q_u16(rec + w + 8));
+
+                    sum64 = vaddq_s64(sum64,
+                                      vaddq_s64(vaddq_s64(vreinterpretq_s64_u32(vzip1q_u32(sum32_0, zero)),
+                                                          vreinterpretq_s64_u32(vzip2q_u32(sum32_0, zero))),
+                                                vaddq_s64(vreinterpretq_s64_u32(vzip1q_u32(sum32_1, zero)),
+                                                          vreinterpretq_s64_u32(vzip2q_u32(sum32_1, zero)))));
+                }
+
+                for (; w < area_width; w += 8) {
+                    const uint32x4_t sum32 = full_distortion_kernel8_neon_intrin(vld1q_u16(inp + w),
+                                                                                 vld1q_u16(rec + w));
+                    sum64                  = vaddq_s64(sum64,
+                                      vaddq_s64(vreinterpretq_s64_u32(vzip1q_u32(sum32, zero)),
+                                                vreinterpretq_s64_u32(vzip2q_u32(sum32, zero))));
+                }
+                inp += input_stride;
+                rec += recon_stride;
+            }
+        }
+    }
+
+    return vgetq_lane_s64(sum64, 0) + vgetq_lane_s64(sum64, 1);
+}
+
 void svt_full_distortion_kernel32_bits_neon(int32_t *coeff, uint32_t coeff_stride, int32_t *recon_coeff,
                                             uint32_t recon_coeff_stride, uint64_t distortion_result[DIST_CALC_TOTAL],
                                             uint32_t area_width, uint32_t area_height) {
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index bf6b865a5..cdf031e53 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -880,7 +880,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_full_distortion_kernel_cbf_zero32_bits, svt_full_distortion_kernel_cbf_zero32_bits_c, svt_full_distortion_kernel_cbf_zero32_bits_neon);
     SET_NEON(svt_full_distortion_kernel32_bits, svt_full_distortion_kernel32_bits_c, svt_full_distortion_kernel32_bits_neon);
     SET_NEON(svt_spatial_full_distortion_kernel, svt_spatial_full_distortion_kernel_c, svt_spatial_full_distortion_kernel_neon);
-    SET_ONLY_C(svt_full_distortion_kernel16_bits, svt_full_distortion_kernel16_bits_c);
+    SET_NEON(svt_full_distortion_kernel16_bits, svt_full_distortion_kernel16_bits_c, svt_full_distortion_kernel16_bits_neon);
     SET_NEON(svt_residual_kernel8bit, svt_residual_kernel8bit_c, svt_residual_kernel8bit_neon);
     SET_ONLY_C(svt_residual_kernel16bit, svt_residual_kernel16bit_c);
     SET_ONLY_C(svt_picture_average_kernel, svt_picture_average_kernel_c);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index 6f0738d7c..0bafc751a 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1364,6 +1364,7 @@ extern "C" {
     void svt_aom_paeth_predictor_64x32_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
     void svt_aom_paeth_predictor_64x64_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
 
+    uint64_t svt_full_distortion_kernel16_bits_neon(uint8_t* input, uint32_t input_offset, uint32_t input_stride, uint8_t* recon, int32_t recon_offset, uint32_t recon_stride, uint32_t area_width, uint32_t area_height);
     void svt_full_distortion_kernel32_bits_neon(int32_t *coeff, uint32_t coeff_stride, int32_t *recon_coeff, uint32_t recon_coeff_stride, uint64_t distortion_result[DIST_CALC_TOTAL], uint32_t area_width, uint32_t area_height);
 
     void svt_full_distortion_kernel_cbf_zero32_bits_neon(int32_t *coeff, uint32_t coeff_stride,
diff --git a/test/SpatialFullDistortionTest.cc b/test/SpatialFullDistortionTest.cc
index cfe58b01f..6f864993e 100644
--- a/test/SpatialFullDistortionTest.cc
+++ b/test/SpatialFullDistortionTest.cc
@@ -572,6 +572,17 @@ INSTANTIATE_TEST_SUITE_P(
 
 #endif
 
+#ifdef ARCH_AARCH64
+
+INSTANTIATE_TEST_CASE_P(
+    FullDistortionKernel16FuncTest_NEON, FullDistortionKernel16BitsFuncTest,
+    ::testing::Combine(
+        ::testing::ValuesIn(TEST_AREA_SIZES),
+        ::testing::ValuesIn(TEST_PATTERNS),
+        ::testing::Values(svt_full_distortion_kernel16_bits_neon)));
+
+#endif
+
 typedef void (*fullDistortionKernel32BitsFunc)(
     int32_t *coeff, uint32_t coeff_stride, int32_t *recon_coeff,
     uint32_t recon_coeff_stride, uint64_t distortion_result[DIST_CALC_TOTAL],
-- 
GitLab


From c16446870bf2ba63bc4f976d5463c50e9f6c5552 Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Mon, 3 Jun 2024 13:41:16 +0000
Subject: [PATCH 10/13] NEON port of svt_av1_highbd_jnt_convolve_y_c

---
 .../{highbd_jnt_convolve_neon.c => highbd_jnt_convolve_neon_y.c}  | 0
 1 file changed, 0 insertions(+), 0 deletions(-)
 rename Source/Lib/Common/ASM_NEON/{highbd_jnt_convolve_neon.c => highbd_jnt_convolve_neon_y.c} (100%)

diff --git a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon_y.c
similarity index 100%
rename from Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
rename to Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon_y.c
-- 
GitLab


From b1cc534691bf9dbd23f399212a844e3e627b5e62 Mon Sep 17 00:00:00 2001
From: Rodrigo Causarano <rodrigo_causarano@ekumenlabs.com>
Date: Tue, 28 May 2024 18:51:43 +0000
Subject: [PATCH 11/13] NEON port of svt_av1_highbd_jnt_convolve_x_c

---
 Source/Lib/Common/ASM_NEON/CMakeLists.txt     |   3 +-
 ...ve_neon_y.c => highbd_jnt_convolve_neon.c} | 229 ++++++++++++++----
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   1 +
 test/convolve_2d_test.cc                      |  11 +-
 5 files changed, 186 insertions(+), 60 deletions(-)
 rename Source/Lib/Common/ASM_NEON/{highbd_jnt_convolve_neon_y.c => highbd_jnt_convolve_neon.c} (54%)

diff --git a/Source/Lib/Common/ASM_NEON/CMakeLists.txt b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
index 47472f455..8e7232c1f 100644
--- a/Source/Lib/Common/ASM_NEON/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
@@ -26,13 +26,12 @@ target_sources(
   PUBLIC convolve_2d_neon.c
   PUBLIC convolve_neon.c
   PUBLIC highbd_convolve_2d_neon.c
+  PUBLIC highbd_jnt_convolve_neon.c
   PUBLIC highbd_inv_txfm_neon.c
   PUBLIC EbBlend_a64_mask_neon.c
   PUBLIC EbDeblockingFilter_Intrinsic_neon.c
   PUBLIC EbIntraPrediction_neon.c
   PUBLIC EbPictureOperators_Intrinsic_neon.c
-  PUBLIC highbd_convolve_2d_neon.c
-  PUBLIC highbd_jnt_convolve_neon.c
   PUBLIC highbd_convolve_neon.c
   PUBLIC selfguided_neon.c
   PUBLIC sse_neon.c
diff --git a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon_y.c b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
similarity index 54%
rename from Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon_y.c
rename to Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
index 8d7a7005d..368a32ea3 100644
--- a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon_y.c
+++ b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
@@ -10,7 +10,6 @@
  */
 
 #include <arm_neon.h>
-
 #include <assert.h>
 
 #include "EbDefinitions.h"
@@ -19,54 +18,178 @@
 
 static INLINE int32x4_t svt_aom_convolve(const uint16x8_t *const s, const int16x8_t *const coeffs) {
     const int32x4_t res_0 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[0])), vget_low_s16(coeffs[0])),
-                                       vmull_high_s16(vreinterpretq_s16_u16(s[0]), coeffs[0]));
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[0])), vget_high_s16(coeffs[0])));
     const int32x4_t res_1 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[1])), vget_low_s16(coeffs[1])),
-                                       vmull_high_s16(vreinterpretq_s16_u16(s[1]), coeffs[1]));
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[1])), vget_high_s16(coeffs[1])));
     const int32x4_t res_2 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[2])), vget_low_s16(coeffs[2])),
-                                       vmull_high_s16(vreinterpretq_s16_u16(s[2]), coeffs[2]));
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[2])), vget_high_s16(coeffs[2])));
     const int32x4_t res_3 = vpaddq_s32(vmull_s16(vget_low_s16(vreinterpretq_s16_u16(s[3])), vget_low_s16(coeffs[3])),
-                                       vmull_high_s16(vreinterpretq_s16_u16(s[3]), coeffs[3]));
+                                       vmull_s16(vget_high_s16(vreinterpretq_s16_u16(s[3])), vget_high_s16(coeffs[3])));
+
+    const int32x4_t res = vaddq_s32(vaddq_s32(res_0, res_1), vaddq_s32(res_2, res_3));
 
-    return vaddq_s32(vaddq_s32(res_0, res_1), vaddq_s32(res_2, res_3));
+    return res;
 }
 
-static INLINE int32x4_t highbd_convolve_rounding_neon(const int32x4_t *const res_unsigned,
-                                                      const int32x4_t *const offset_const, const int round_shift) {
-    const int32x4_t res_signed = vsubq_s32(*res_unsigned, *offset_const);
+static INLINE void prepare_coeffs(const int16_t *const filter, int16x8_t *const coeffs /* [4] */) {
+    const int16x8_t coeff = vld1q_s16(filter);
 
-    return vrshlq_s32(res_signed, vdupq_n_s32(-round_shift));
+    // coeffs 0 1 0 1 0 1 0 1
+    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 0)));
+    // coeffs 2 3 2 3 2 3 2 3
+    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 1)));
+    // coeffs 4 5 4 5 4 5 4 5
+    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 2)));
+    // coeffs 6 7 6 7 6 7 6 7
+    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_s16(coeff), 3)));
 }
 
-static INLINE int32x4_t highbd_comp_avg_neon(const uint16x8_t *const data_ref_0, const int32x4_t *const res_unsigned,
+static INLINE int32x4_t highbd_comp_avg_neon(const uint32x4_t *const data_ref_0, const uint32x4_t *const res_unsigned,
                                              const int32x4_t *const wt0, const int32x4_t *const wt1,
                                              const int use_dist_wtd_avg) {
     int32x4_t res;
     if (use_dist_wtd_avg) {
-        const int32x4_t wt0_res = vmulq_s32(vreinterpretq_s32_u16(*data_ref_0), *wt0);
-        const int32x4_t wt1_res = vmulq_s32(*res_unsigned, *wt1);
+        const int32x4_t wt0_res = vmulq_s32(vreinterpretq_s32_u32(*data_ref_0), *wt0);
+        const int32x4_t wt1_res = vmulq_s32(vreinterpretq_s32_u32(*res_unsigned), *wt1);
 
         const int32x4_t wt_res = vaddq_s32(wt0_res, wt1_res);
         res                    = vshrq_n_s32(wt_res, DIST_PRECISION_BITS);
     } else {
-        const int32x4_t wt_res = vaddq_s32(vreinterpretq_s32_u16(*data_ref_0), *res_unsigned);
+        const int32x4_t wt_res = vaddq_s32(vreinterpretq_s32_u32(*data_ref_0), vreinterpretq_s32_u32(*res_unsigned));
         res                    = vshrq_n_s32(wt_res, 1);
     }
     return res;
 }
 
-static INLINE void prepare_coeffs(const InterpFilterParams *const filter_params, const int subpel_q4,
-                                  int16x8_t *const coeffs /* [4] */) {
-    const int16_t  *filter = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
-    const int32x4_t coeff  = vld1q_s32((const int32_t *)filter);
+static INLINE int32x4_t highbd_convolve_rounding_neon(const int32x4_t *const res_unsigned,
+                                                      const int32x4_t *const offset_const, const int round_shift) {
+    const int32x4_t res_signed = vsubq_s32(*res_unsigned, *offset_const);
 
-    // coeffs 0 1 0 1 0 1 0 1
-    coeffs[0] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 0)));
-    // coeffs 2 3 2 3 2 3 2 3
-    coeffs[1] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 1)));
-    // coeffs 4 5 4 5 4 5 4 5
-    coeffs[2] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 2)));
-    // coeffs 6 7 6 7 6 7 6 7
-    coeffs[3] = vreinterpretq_s16_s32(vdupq_n_s32(vgetq_lane_s32(coeff, 3)));
+    return vrshlq_s32(res_signed, vdupq_n_s32(-round_shift));
+}
+
+void svt_av1_highbd_jnt_convolve_x_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst0, int32_t dst_stride0,
+                                        int32_t w, int32_t h, const InterpFilterParams *filter_params_x,
+                                        const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4,
+                                        const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd) {
+    if (w <= 4) {
+        svt_av1_highbd_jnt_convolve_x_c(src,
+                                        src_stride,
+                                        dst0,
+                                        dst_stride0,
+                                        w,
+                                        h,
+                                        filter_params_x,
+                                        filter_params_y,
+                                        subpel_x_q4,
+                                        subpel_y_q4,
+                                        conv_params,
+                                        bd);
+        return;
+    }
+    CONV_BUF_TYPE        *dst        = conv_params->dst;
+    int                   dst_stride = conv_params->dst_stride;
+    const int             fo_horiz   = filter_params_x->taps / 2 - 1;
+    const uint16_t *const src_ptr    = src - fo_horiz;
+
+    int        i, j;
+    uint16x8_t s[4];
+    int16x8_t  coeffs_x[4];
+
+    const int        do_average       = conv_params->do_average;
+    const int        use_jnt_comp_avg = conv_params->use_jnt_comp_avg;
+    const int        w0               = conv_params->fwd_offset;
+    const int        w1               = conv_params->bck_offset;
+    const int32x4_t  wt0              = vdupq_n_s32(w0);
+    const int32x4_t  wt1              = vdupq_n_s32(w1);
+    const int32x4_t  round_const_x    = vdupq_n_s32(((1 << conv_params->round_0) >> 1));
+    const int        offset_0         = bd + 2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+    const int        offset           = (1 << offset_0) + (1 << (offset_0 - 1));
+    const int32x4_t  offset_const     = vdupq_n_s32(offset);
+    const int        rounding_shift   = 2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+    const uint16x8_t clip_pixel_to_bd = vdupq_n_u16(bd == 10 ? 1023 : (bd == 12 ? 4095 : 255));
+
+    const int16_t *filter_x = av1_get_interp_filter_subpel_kernel(*filter_params_x, subpel_x_q4 & SUBPEL_MASK);
+    prepare_coeffs(filter_x, coeffs_x);
+
+    for (j = 0; j < w; j += 8) {
+        /* Horizontal filter */
+        for (i = 0; i < h; i += 1) {
+            const uint16x8_t row00 = vld1q_u16(&src_ptr[i * src_stride + j]);
+            const uint16x8_t row01 = vld1q_u16(&src_ptr[i * src_stride + (j + 8)]);
+
+            // even pixels
+            s[0] = vextq_u16(row00, row01, 0);
+            s[1] = vextq_u16(row00, row01, 2);
+            s[2] = vextq_u16(row00, row01, 4);
+            s[3] = vextq_u16(row00, row01, 6);
+
+            int32x4_t res_even = svt_aom_convolve(s, coeffs_x);
+            res_even           = vshlq_s32(vaddq_s32(res_even, round_const_x), vdupq_n_s32(-conv_params->round_0));
+
+            // odd pixels
+            s[0] = vextq_u16(row00, row01, 1);
+            s[1] = vextq_u16(row00, row01, 3);
+            s[2] = vextq_u16(row00, row01, 5);
+            s[3] = vextq_u16(row00, row01, 7);
+
+            int32x4_t res_odd = svt_aom_convolve(s, coeffs_x);
+            res_odd           = vshlq_s32(vaddq_s32(res_odd, round_const_x), vdupq_n_s32(-conv_params->round_0));
+
+            res_even = vreinterpretq_s32_u32(
+                vshlq_u32(vreinterpretq_u32_s32(res_even), vdupq_n_s32(FILTER_BITS - conv_params->round_1)));
+            res_odd = vreinterpretq_s32_u32(
+                vshlq_u32(vreinterpretq_u32_s32(res_odd), vdupq_n_s32(FILTER_BITS - conv_params->round_1)));
+
+            int32x4_t  res1            = vzip1q_s32(res_even, res_odd);
+            uint32x4_t res_unsigned_lo = vreinterpretq_u32_s32(vaddq_s32(res1, offset_const));
+            if (w - j < 8) {
+                if (do_average) {
+                    const uint16x4_t data_0     = vld1_u16(&dst[i * dst_stride + j]);
+                    const uint32x4_t data_ref_0 = vmovl_u16(data_0);
+
+                    const int32x4_t comp_avg_res = highbd_comp_avg_neon(
+                        &data_ref_0, &res_unsigned_lo, &wt0, &wt1, use_jnt_comp_avg);
+                    const int32x4_t round_result = highbd_convolve_rounding_neon(
+                        &comp_avg_res, &offset_const, rounding_shift);
+
+                    const uint16x8_t res_16b  = vcombine_u16(vqmovun_s32(round_result), vqmovun_s32(round_result));
+                    const uint16x8_t res_clip = vminq_u16(res_16b, clip_pixel_to_bd);
+                    vst1_u16(&dst0[i * dst_stride0 + j], vget_low_u16(res_clip));
+                } else {
+                    uint16x8_t res_16b = vcombine_u16(vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo)),
+                                                      vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo)));
+                    vst1_u16(&dst[i * dst_stride + j], vget_low_u16(res_16b));
+                }
+            } else {
+                int32x4_t  res2            = vzip2q_s32(res_even, res_odd);
+                uint32x4_t res_unsigned_hi = vreinterpretq_u32_s32(vaddq_s32(res2, offset_const));
+                if (do_average) {
+                    const uint16x8_t data_0        = vld1q_u16(&dst[i * dst_stride + j]);
+                    const uint32x4_t data_ref_0_lo = vmovl_u16(vget_low_u16(data_0));
+                    const uint32x4_t data_ref_0_hi = vmovl_u16(vget_high_u16(data_0));
+
+                    const int32x4_t comp_avg_res_lo = highbd_comp_avg_neon(
+                        &data_ref_0_lo, &res_unsigned_lo, &wt0, &wt1, use_jnt_comp_avg);
+                    const int32x4_t comp_avg_res_hi = highbd_comp_avg_neon(
+                        &data_ref_0_hi, &res_unsigned_hi, &wt0, &wt1, use_jnt_comp_avg);
+
+                    const int32x4_t round_result_lo = highbd_convolve_rounding_neon(
+                        &comp_avg_res_lo, &offset_const, rounding_shift);
+                    const int32x4_t round_result_hi = highbd_convolve_rounding_neon(
+                        &comp_avg_res_hi, &offset_const, rounding_shift);
+
+                    const uint16x8_t res_16b = vcombine_u16(vqmovun_s32(round_result_lo), vqmovun_s32(round_result_hi));
+                    const uint16x8_t res_clip = vminq_u16(res_16b, clip_pixel_to_bd);
+                    vst1q_u16(&dst0[i * dst_stride0 + j], res_clip);
+                } else {
+                    uint16x8_t res_16b = vcombine_u16(vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo)),
+                                                      vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_hi)));
+                    vst1q_u16(&dst[i * dst_stride + j], res_16b);
+                }
+            }
+        }
+    }
 }
 
 void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst0, int32_t dst_stride0,
@@ -109,11 +232,11 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
     const int32x4_t  offset_const     = vdupq_n_s32(offset);
     const int        rounding_shift   = 2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
     const uint16x8_t clip_pixel_to_bd = vdupq_n_u16(bd == 10 ? 1023 : (bd == 12 ? 4095 : 255));
-    const uint16x8_t zero             = vdupq_n_u16(0);
     uint16x8_t       s[16];
     int16x8_t        coeffs_y[4];
 
-    prepare_coeffs(filter_params_y, subpel_y_q4, coeffs_y);
+    const int16_t *filter_y = av1_get_interp_filter_subpel_kernel(*filter_params_y, subpel_y_q4 & SUBPEL_MASK);
+    prepare_coeffs(filter_y, coeffs_y);
 
     for (j = 0; j < w; j += 8) {
         const uint16_t *data = &src_ptr[j];
@@ -163,17 +286,16 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
                 int32x4_t       res_a_round1 = vshlq_s32(res_a1, vdupq_n_s32(FILTER_BITS - conv_params->round_0));
                 res_a_round1 = vshlq_s32(vaddq_s32(res_a_round1, round_const_y), vdupq_n_s32(-conv_params->round_1));
 
-                const int32x4_t res_unsigned_lo_0 = vaddq_s32(res_a_round0, offset_const);
-                const int32x4_t res_unsigned_lo_1 = vaddq_s32(res_a_round1, offset_const);
+                const uint32x4_t res_unsigned_lo_0 = vreinterpretq_u32_s32(vaddq_s32(res_a_round0, offset_const));
+                const uint32x4_t res_unsigned_lo_1 = vreinterpretq_u32_s32(vaddq_s32(res_a_round1, offset_const));
 
                 if (w - j < 8) {
                     if (do_average) {
-                        const uint16x8_t data_0 = vcombine_u16(vld1_u16(&dst[i * dst_stride + j]), vdup_n_u16(0));
-                        const uint16x8_t data_1 = vcombine_u16(vld1_u16(&dst[i * dst_stride + j + dst_stride]),
-                                                               vdup_n_u16(0));
+                        const uint16x4_t data_0 = vld1_u16(&dst[i * dst_stride + j]);
+                        const uint16x4_t data_1 = vld1_u16(&dst[i * dst_stride + j + dst_stride]);
 
-                        const uint16x8_t data_ref_0 = vzip1q_u16(data_0, zero);
-                        const uint16x8_t data_ref_1 = vzip1q_u16(data_1, zero);
+                        const uint32x4_t data_ref_0 = vmovl_u16(data_0);
+                        const uint32x4_t data_ref_1 = vmovl_u16(data_1);
 
                         const int32x4_t comp_avg_res_0 = highbd_comp_avg_neon(
                             &data_ref_0, &res_unsigned_lo_0, &wt0, &wt1, use_jnt_comp_avg);
@@ -194,11 +316,13 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
                         vst1_u16(&dst0[i * dst_stride0 + j + dst_stride0], vget_low_u16(res_clip_1));
 
                     } else {
-                        const uint16x8_t res_16b_0 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_0),
-                                                                      res_unsigned_lo_0);
+                        const uint16x8_t res_16b_0 = vqmovun_high_s32(
+                            vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo_0)),
+                            vreinterpretq_s32_u32(res_unsigned_lo_0));
 
-                        const uint16x8_t res_16b_1 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_1),
-                                                                      res_unsigned_lo_1);
+                        const uint16x8_t res_16b_1 = vqmovun_high_s32(
+                            vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo_1)),
+                            vreinterpretq_s32_u32(res_unsigned_lo_1));
 
                         vst1_u16(&dst[i * dst_stride + j], vget_low_u16(res_16b_0));
                         vst1_u16(&dst[i * dst_stride + j + dst_stride], vget_low_u16(res_16b_1));
@@ -214,17 +338,18 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
                     res_b_round1                 = vshlq_s32(vaddq_s32(res_b_round1, round_const_y),
                                              vdupq_n_s32(-conv_params->round_1));
 
-                    int32x4_t res_unsigned_hi_0 = vaddq_s32(res_b_round0, offset_const);
-                    int32x4_t res_unsigned_hi_1 = vaddq_s32(res_b_round1, offset_const);
+                    uint32x4_t res_unsigned_hi_0 = vreinterpretq_u32_s32(vaddq_s32(res_b_round0, offset_const));
+                    uint32x4_t res_unsigned_hi_1 = vreinterpretq_u32_s32(vaddq_s32(res_b_round1, offset_const));
 
                     if (do_average) {
-                        const uint16x8_t data_0          = vld1q_u16(&dst[i * dst_stride + j]);
-                        const uint16x8_t data_1          = vld1q_u16(&dst[i * dst_stride + j + dst_stride]);
-                        const uint16x8_t data_ref_0_lo_0 = vzip1q_u16(data_0, zero);
-                        const uint16x8_t data_ref_0_lo_1 = vzip1q_u16(data_1, zero);
+                        const uint16x8_t data_0 = vld1q_u16(&dst[i * dst_stride + j]);
+                        const uint16x8_t data_1 = vld1q_u16(&dst[i * dst_stride + j + dst_stride]);
+
+                        const uint32x4_t data_ref_0_lo_0 = vmovl_u16(vget_low_u16(data_0));
+                        const uint32x4_t data_ref_0_lo_1 = vmovl_u16(vget_low_u16(data_1));
 
-                        const uint16x8_t data_ref_0_hi_0 = vzip2q_u16(data_0, zero);
-                        const uint16x8_t data_ref_0_hi_1 = vzip2q_u16(data_1, zero);
+                        const uint32x4_t data_ref_0_hi_0 = vmovl_high_u16(data_0);
+                        const uint32x4_t data_ref_0_hi_1 = vmovl_high_u16(data_1);
 
                         const int32x4_t comp_avg_res_lo_0 = highbd_comp_avg_neon(
                             &data_ref_0_lo_0, &res_unsigned_lo_0, &wt0, &wt1, use_jnt_comp_avg);
@@ -255,10 +380,12 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
                         vst1q_u16(&dst0[i * dst_stride0 + j], res_clip_0);
                         vst1q_u16(&dst0[i * dst_stride0 + j + dst_stride0], res_clip_1);
                     } else {
-                        const uint16x8_t res_16bit0 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_0),
-                                                                       res_unsigned_hi_0);
-                        const uint16x8_t res_16bit1 = vqmovun_high_s32(vqmovun_s32(res_unsigned_lo_1),
-                                                                       res_unsigned_hi_1);
+                        const uint16x8_t res_16bit0 = vqmovun_high_s32(
+                            vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo_0)),
+                            vreinterpretq_s32_u32(res_unsigned_hi_0));
+                        const uint16x8_t res_16bit1 = vqmovun_high_s32(
+                            vqmovun_s32(vreinterpretq_s32_u32(res_unsigned_lo_1)),
+                            vreinterpretq_s32_u32(res_unsigned_hi_1));
                         vst1q_u16(&dst[i * dst_stride + j], res_16bit0);
                         vst1q_u16(&dst[i * dst_stride + j + dst_stride], res_16bit1);
                     }
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index cdf031e53..42c45a9f5 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -893,8 +893,8 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_av1_highbd_convolve_2d_copy_sr, svt_av1_highbd_convolve_2d_copy_sr_c);
     SET_NEON(svt_av1_highbd_jnt_convolve_2d, svt_av1_highbd_jnt_convolve_2d_c, svt_av1_highbd_jnt_convolve_2d_neon);
     SET_ONLY_C(svt_av1_highbd_jnt_convolve_2d_copy, svt_av1_highbd_jnt_convolve_2d_copy_c);
-    SET_ONLY_C(svt_av1_highbd_jnt_convolve_x, svt_av1_highbd_jnt_convolve_x_c);
     SET_NEON(svt_av1_highbd_jnt_convolve_y, svt_av1_highbd_jnt_convolve_y_c, svt_av1_highbd_jnt_convolve_y_neon);
+    SET_NEON(svt_av1_highbd_jnt_convolve_x, svt_av1_highbd_jnt_convolve_x_c, svt_av1_highbd_jnt_convolve_x_neon);
     SET_ONLY_C(svt_av1_highbd_convolve_x_sr, svt_av1_highbd_convolve_x_sr_c);
     SET_NEON(svt_av1_convolve_2d_sr, svt_av1_convolve_2d_sr_c, svt_av1_convolve_2d_sr_neon);
     SET_NEON(svt_av1_convolve_2d_copy_sr, svt_av1_convolve_2d_copy_sr_c, svt_av1_convolve_2d_copy_sr_neon);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index 0bafc751a..28e5f37a0 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1381,6 +1381,7 @@ extern "C" {
                                            const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
     void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
     void svt_av1_highbd_convolve_y_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
+    void svt_av1_highbd_jnt_convolve_x_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
     void svt_av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
 
diff --git a/test/convolve_2d_test.cc b/test/convolve_2d_test.cc
index 9e4aa3f91..c2a070e97 100644
--- a/test/convolve_2d_test.cc
+++ b/test/convolve_2d_test.cc
@@ -1055,7 +1055,7 @@ class AV1HbdJntConvolve2DTest : public AV1HbdConvolve2DTest {
             if (has_subx == 1 && has_suby == 1) {
                 func_tst_ = svt_av1_highbd_jnt_convolve_2d_neon;
             } else if (has_subx == 1) {
-                func_tst_ = func_ref_;  // not yet ported
+                func_tst_ = svt_av1_highbd_jnt_convolve_x_neon;
             } else if (has_suby == 1) {
                 func_tst_ = svt_av1_highbd_jnt_convolve_y_neon;
             } else {
@@ -1106,11 +1106,10 @@ INSTANTIATE_TEST_SUITE_P(ConvolveTestY, AV1HbdJntConvolve2DTest,
 // not yet ported
 // INSTANTIATE_TEST_SUITE_P(NEON_COPY, AV1HbdJntConvolve2DTest,
 //                         BuildParams(0, 0, 2, 1));
-INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTest2D, AV1HbdJntConvolve2DTest,
-                         BuildParams(1, 1, 2, 1));
-// not yet ported
-// INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestX, AV1HbdJntConvolve2DTest,
-//                         BuildParams(1, 0, 2, 1));
+INSTANTIATE_TEST_CASE_P(NEON_ConvolveTest2D, AV1HbdJntConvolve2DTest,
+                        BuildParams(1, 1, 2, 1));
+INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestX, AV1HbdJntConvolve2DTest,
+                        BuildParams(1, 0, 2, 1));
 INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdJntConvolve2DTest,
                         BuildParams(0, 1, 2, 1));
 
-- 
GitLab


From 36bfff5b7827773cb4b2d85a521776d02cb54524 Mon Sep 17 00:00:00 2001
From: Gerardo Puga <glpuga@gmail.com>
Date: Thu, 30 May 2024 14:23:23 +0000
Subject: [PATCH 12/13] Port of svt_aom_compute_cdef_dist_8bit_c and others

---
 Source/Lib/Common/ASM_NEON/cdef_block_neon.c  |  12 +
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |   2 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   3 +
 Source/Lib/Encoder/ASM_NEON/EbCdef_neon.c     | 222 ++++++++++++++++++
 Source/Lib/Encoder/ASM_NEON/encodetxb_neon.c  |  29 +++
 .../Encoder/ASM_NEON/highbd_fwd_txfm_neon.c   |   5 +-
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.c       |   4 +-
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.h       |   5 +
 test/AdaptiveScanTest.cc                      |  16 +-
 test/CMakeLists.txt                           |   2 +-
 test/CdefTest.cc                              |  52 +++-
 test/EbUnitTestUtility.c                      |   3 +-
 test/FwdTxfm2dAsmTest.cc                      |  35 +++
 test/TemporalFilterTestPlanewise.cc           |   1 +
 14 files changed, 374 insertions(+), 17 deletions(-)

diff --git a/Source/Lib/Common/ASM_NEON/cdef_block_neon.c b/Source/Lib/Common/ASM_NEON/cdef_block_neon.c
index 67e0a92d4..611368c2d 100644
--- a/Source/Lib/Common/ASM_NEON/cdef_block_neon.c
+++ b/Source/Lib/Common/ASM_NEON/cdef_block_neon.c
@@ -152,6 +152,18 @@ static INLINE void array_reverse_transpose_8x8(int16x8_t *in, int16x8_t *res) {
     res[0] = vreinterpretq_s16_s32(vcombine_s32(vget_high_s32(tr1_6), vget_high_s32(tr1_7)));
 }
 
+void svt_aom_copy_rect8_8bit_to_16bit_neon(uint16_t *dst, int32_t dstride, const uint8_t *src, int32_t sstride,
+                                           int32_t v, int32_t h) {
+    int32_t i, j;
+    for (i = 0; i < v; i++) {
+        for (j = 0; j < (h & ~0x7); j += 8) {
+            const uint8x8_t row = vld1_u8(&src[i * sstride + j]);
+            vst1q_u16(&dst[i * dstride + j], vmovl_u8(row));
+        }
+        for (; j < h; j++) { dst[i * dstride + j] = src[i * sstride + j]; }
+    }
+}
+
 uint8_t svt_aom_cdef_find_dir_neon(const uint16_t *img, int32_t stride, int32_t *var, int32_t coeff_shift) {
     int16x8_t       lines[8];
     const int16x8_t const_128 = vdupq_n_s16(128);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 42c45a9f5..873180fbc 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -1160,7 +1160,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_aom_cdef_find_dir_dual, svt_aom_cdef_find_dir_dual_c, svt_aom_cdef_find_dir_dual_neon);
     SET_NEON(svt_cdef_filter_block, svt_cdef_filter_block_c, svt_cdef_filter_block_neon);
 
-    SET_ONLY_C(svt_aom_copy_rect8_8bit_to_16bit, svt_aom_copy_rect8_8bit_to_16bit_c);
+    SET_NEON(svt_aom_copy_rect8_8bit_to_16bit, svt_aom_copy_rect8_8bit_to_16bit_c, svt_aom_copy_rect8_8bit_to_16bit_neon);
     SET_ONLY_C(svt_av1_highbd_warp_affine, svt_av1_highbd_warp_affine_c);
     SET_ONLY_C(dec_svt_av1_highbd_warp_affine, svt_aom_dec_svt_av1_highbd_warp_affine_c);
     SET_NEON(svt_av1_warp_affine, svt_av1_warp_affine_c, svt_av1_warp_affine_neon);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index 28e5f37a0..fb0623896 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -1324,6 +1324,9 @@ extern "C" {
     void svt_aom_v_predictor_64x32_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
     void svt_aom_v_predictor_64x64_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
 
+    void svt_aom_copy_rect8_8bit_to_16bit_neon(uint16_t *dst, int32_t dstride, const uint8_t *src, int32_t sstride,
+                                               int32_t v, int32_t h);
+
     void svt_aom_h_predictor_4x4_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
     void svt_aom_h_predictor_4x8_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
     void svt_aom_h_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride, const uint8_t *above,const uint8_t *left);
diff --git a/Source/Lib/Encoder/ASM_NEON/EbCdef_neon.c b/Source/Lib/Encoder/ASM_NEON/EbCdef_neon.c
index 097201acb..b8779e6c8 100644
--- a/Source/Lib/Encoder/ASM_NEON/EbCdef_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/EbCdef_neon.c
@@ -35,6 +35,228 @@ static INLINE void sum_32_to_64(const int32x4_t src, int64x2_t *dst) {
     *dst                  = vaddq_s64(*dst, vreinterpretq_s64_s32(src_h));
 }
 
+static INLINE uint64_t dist_8xn_8bit_neon(const uint8_t **src, const uint8_t *dst, const int32_t dstride,
+                                          const int32_t coeff_shift, uint8_t height, uint8_t subsampling_factor) {
+    int16x8_t ss = vdupq_n_s16(0);
+    int16x8_t dd = vdupq_n_s16(0);
+    int32x4_t s2 = vdupq_n_s32(0);
+    int32x4_t sd = vdupq_n_s32(0);
+    int32x4_t d2 = vdupq_n_s32(0);
+
+    for (int32_t r = 0; r < height; r += 2 * subsampling_factor) {
+        const int16x8_t s_16_0 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(*src + subsampling_factor * 8)));
+        const int16x8_t s_16_1 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(*src + 0 * 8)));
+        const int16x8_t d_16_0 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(dst + (r + subsampling_factor) * dstride)));
+        const int16x8_t d_16_1 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(dst + r * dstride)));
+
+        ss = vaddq_s16(ss, s_16_0);
+        ss = vaddq_s16(ss, s_16_1);
+        dd = vaddq_s16(dd, d_16_0);
+        dd = vaddq_s16(dd, d_16_1);
+
+        s2 = vaddq_s32(s2,
+                       vpaddq_s32(vmull_s16(vget_low_s16(s_16_0), vget_low_s16(s_16_0)),
+                                  vmull_s16(vget_high_s16(s_16_0), vget_high_s16(s_16_0))));
+        s2 = vaddq_s32(s2,
+                       vpaddq_s32(vmull_s16(vget_low_s16(s_16_1), vget_low_s16(s_16_1)),
+                                  vmull_s16(vget_high_s16(s_16_1), vget_high_s16(s_16_1))));
+        sd = vaddq_s32(sd,
+                       vpaddq_s32(vmull_s16(vget_low_s16(s_16_0), vget_low_s16(d_16_0)),
+                                  vmull_s16(vget_high_s16(s_16_0), vget_high_s16(d_16_0))));
+        sd = vaddq_s32(sd,
+                       vpaddq_s32(vmull_s16(vget_low_s16(s_16_1), vget_low_s16(d_16_1)),
+                                  vmull_s16(vget_high_s16(s_16_1), vget_high_s16(d_16_1))));
+        d2 = vaddq_s32(d2,
+                       vpaddq_s32(vmull_s16(vget_low_s16(d_16_0), vget_low_s16(d_16_0)),
+                                  vmull_s16(vget_high_s16(d_16_0), vget_high_s16(d_16_0))));
+        d2 = vaddq_s32(d2,
+                       vpaddq_s32(vmull_s16(vget_low_s16(d_16_1), vget_low_s16(d_16_1)),
+                                  vmull_s16(vget_high_s16(d_16_1), vget_high_s16(d_16_1))));
+
+        *src += 8 * 2 * subsampling_factor; // width * 2 lines per iter. * subsampling
+    }
+
+    int16x8_t ssdd    = vpaddq_s16(ss, dd);
+    ssdd              = vpaddq_s16(ssdd, ssdd);
+    int32x4_t ssdd_32 = vreinterpretq_s32_u32(vmovl_u16(vreinterpret_u16_s16(vget_low_s16(ssdd))));
+
+    int32x4_t sum = vpaddq_s32(ssdd_32, ssdd_32);
+
+    uint64_t sum_s = vgetq_lane_s32(sum, 0);
+    uint64_t sum_d = vgetq_lane_s32(sum, 1);
+
+    uint64_t sum_s2 = sum32(s2);
+    uint64_t sum_d2 = sum32(d2);
+    uint64_t sum_sd = sum32(sd);
+
+    /* Compute the variance -- the calculation cannot go negative. */
+    uint64_t svar = sum_s2 - ((sum_s * sum_s + 32) >> 6);
+    uint64_t dvar = sum_d2 - ((sum_d * sum_d + 32) >> 6);
+    return (uint64_t)floor(.5 +
+                           (sum_d2 + sum_s2 - 2 * sum_sd) * .5 * (svar + dvar + (400 << 2 * coeff_shift)) /
+                               (sqrt((20000 << 4 * coeff_shift) + svar * (double)dvar)));
+}
+
+static INLINE void mse_4xn_8bit_neon(const uint8_t **src, const uint8_t *dst, const int32_t dstride, int32x4_t *sum,
+                                     uint8_t height, uint8_t subsampling_factor) {
+    for (int32_t r = 0; r < height; r += 4 * subsampling_factor) {
+        const uint32_t   aa = *(uint32_t *)(*src + (0 * subsampling_factor) * 4);
+        const uint32_t   ab = *(uint32_t *)(*src + (1 * subsampling_factor) * 4);
+        const uint32_t   ac = *(uint32_t *)(*src + (2 * subsampling_factor) * 4);
+        const uint32_t   ad = *(uint32_t *)(*src + (3 * subsampling_factor) * 4);
+        const uint8x16_t s  = vcombine_u8( //
+            vreinterpret_u8_u32(vzip1_u32(vdup_n_u32(aa), vdup_n_u32(ab))),
+            vreinterpret_u8_u32(vzip1_u32(vdup_n_u32(ac), vdup_n_u32(ad))));
+
+        const uint32_t   ba = *(uint32_t *)(dst + (r + (0 * subsampling_factor)) * dstride);
+        const uint32_t   bb = *(uint32_t *)(dst + (r + (1 * subsampling_factor)) * dstride);
+        const uint32_t   bc = *(uint32_t *)(dst + (r + (2 * subsampling_factor)) * dstride);
+        const uint32_t   bd = *(uint32_t *)(dst + (r + (3 * subsampling_factor)) * dstride);
+        const uint8x16_t d  = vcombine_u8( //
+            vreinterpret_u8_u32(vzip1_u32(vdup_n_u32(ba), vdup_n_u32(bb))),
+            vreinterpret_u8_u32(vzip1_u32(vdup_n_u32(bc), vdup_n_u32(bd))));
+
+        const int16x8_t s_16_0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(s)));
+        const int16x8_t s_16_1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(s)));
+        const int16x8_t d_16_0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(d)));
+        const int16x8_t d_16_1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(d)));
+
+        const int16x8_t diff_0 = vsubq_s16(d_16_0, s_16_0);
+        const int16x8_t diff_1 = vsubq_s16(d_16_1, s_16_1);
+        const int32x4_t mse_0  = vpaddq_s32(
+            vmulq_s32(vmovl_s16(vget_low_s16(diff_0)), vmovl_s16(vget_low_s16(diff_0))),
+            vmulq_s32(vmovl_s16(vget_high_s16(diff_0)), vmovl_s16(vget_high_s16(diff_0))));
+        const int32x4_t mse_1 = vpaddq_s32(
+            vmulq_s32(vmovl_s16(vget_low_s16(diff_1)), vmovl_s16(vget_low_s16(diff_1))),
+            vmulq_s32(vmovl_s16(vget_high_s16(diff_1)), vmovl_s16(vget_high_s16(diff_1))));
+
+        *sum = vaddq_s32(*sum, mse_0);
+        *sum = vaddq_s32(*sum, mse_1);
+
+        *src += 4 * 4 * subsampling_factor; // with * 4 rows per iter * subsampling
+    }
+}
+
+static INLINE void mse_8xn_8bit_neon(const uint8_t **src, const uint8_t *dst, const int32_t dstride, int32x4_t *sum,
+                                     uint8_t height, uint8_t subsampling_factor) {
+    for (int32_t r = 0; r < height; r += 2 * subsampling_factor) {
+        const int16x8_t s_16_0 = vreinterpretq_s16_u16(vmovl_u8((vld1_u8(*src + subsampling_factor * 8))));
+        const int16x8_t s_16_1 = vreinterpretq_s16_u16(vmovl_u8((vld1_u8(*src + 0 * 8))));
+        const int16x8_t d_16_0 = vreinterpretq_s16_u16(vmovl_u8((vld1_u8(dst + (r + subsampling_factor) * dstride))));
+        const int16x8_t d_16_1 = vreinterpretq_s16_u16(vmovl_u8((vld1_u8(dst + r * dstride))));
+
+        const int16x8_t diff_0 = vsubq_s16(d_16_0, s_16_0);
+        const int16x8_t diff_1 = vsubq_s16(d_16_1, s_16_1);
+        const int32x4_t mse_0  = vpaddq_s32(
+            vmulq_s32(vmovl_s16(vget_low_s16(diff_0)), vmovl_s16(vget_low_s16(diff_0))),
+            vmulq_s32(vmovl_s16(vget_high_s16(diff_0)), vmovl_s16(vget_high_s16(diff_0))));
+        const int32x4_t mse_1 = vpaddq_s32(
+            vmulq_s32(vmovl_s16(vget_low_s16(diff_1)), vmovl_s16(vget_low_s16(diff_1))),
+            vmulq_s32(vmovl_s16(vget_high_s16(diff_1)), vmovl_s16(vget_high_s16(diff_1))));
+
+        *sum = vaddq_s32(*sum, mse_0);
+        *sum = vaddq_s32(*sum, mse_1);
+
+        *src += 8 * 2 * subsampling_factor;
+    }
+}
+
+static INLINE void mse_4x4_8bit_2x_subsampled_neon(const uint8_t **src, const uint8_t *dst, const int32_t dstride,
+                                                   int32x4_t *sum) {
+    const uint8x16_t s = vld1q_u8(*src);
+
+    const uint32_t   aa = *(uint32_t *)(dst + 0 * dstride);
+    const uint32_t   ab = *(uint32_t *)(*src + 1 * 4);
+    const uint32_t   ac = *(uint32_t *)(dst + 2 * dstride);
+    const uint32_t   ad = *(uint32_t *)(*src + 3 * 4);
+    const uint8x16_t d  = vcombine_u8( // don't add r * dstride b/c add it at end of loop iterations
+        vreinterpret_u8_u32(vzip1_u32(vdup_n_u32(aa), vdup_n_u32(ab))),
+        vreinterpret_u8_u32(vzip1_u32(vdup_n_u32(ac), vdup_n_u32(ad))));
+
+    const int16x8_t s_16_0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(s)));
+    const int16x8_t s_16_1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(s)));
+    const int16x8_t d_16_0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(d)));
+    const int16x8_t d_16_1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(d)));
+
+    const int16x8_t diff_0 = vsubq_s16(d_16_0, s_16_0);
+    const int16x8_t diff_1 = vsubq_s16(d_16_1, s_16_1);
+    const int32x4_t mse_0  = vpaddq_s32(vmulq_s32(vmovl_s16(vget_low_s16(diff_0)), vmovl_s16(vget_low_s16(diff_0))),
+                                       vmulq_s32(vmovl_s16(vget_high_s16(diff_0)), vmovl_s16(vget_high_s16(diff_0))));
+    const int32x4_t mse_1  = vpaddq_s32(vmulq_s32(vmovl_s16(vget_low_s16(diff_1)), vmovl_s16(vget_low_s16(diff_1))),
+                                       vmulq_s32(vmovl_s16(vget_high_s16(diff_1)), vmovl_s16(vget_high_s16(diff_1))));
+
+    *sum = vaddq_s32(*sum, mse_0);
+    *sum = vaddq_s32(*sum, mse_1);
+
+    *src += 16;
+}
+
+uint64_t svt_aom_compute_cdef_dist_8bit_neon(const uint8_t *dst8, int32_t dstride, const uint8_t *src8,
+                                             const CdefList *dlist, int32_t cdef_count, BlockSize bsize,
+                                             int32_t coeff_shift, int32_t pli, uint8_t subsampling_factor) {
+    uint64_t sum;
+    int32_t  bi, bx, by;
+
+    if ((bsize == BLOCK_8X8) && (pli == 0)) {
+        sum = 0;
+        for (bi = 0; bi < cdef_count; bi++) {
+            by = dlist[bi].by;
+            bx = dlist[bi].bx;
+            sum += dist_8xn_8bit_neon(
+                &src8, dst8 + 8 * by * dstride + 8 * bx, dstride, coeff_shift, 8, subsampling_factor);
+        }
+    } else {
+        int64x2_t mse64 = vdupq_n_s64(0);
+
+        if (bsize == BLOCK_8X8) {
+            for (bi = 0; bi < cdef_count; bi++) {
+                int32x4_t mse32 = vdupq_n_s32(0);
+                by              = dlist[bi].by;
+                bx              = dlist[bi].bx;
+                mse_8xn_8bit_neon(
+                    &src8, dst8 + (8 * by + 0) * dstride + 8 * bx, dstride, &mse32, 8, subsampling_factor);
+                sum_32_to_64(mse32, &mse64);
+            }
+        } else if (bsize == BLOCK_4X8) {
+            for (bi = 0; bi < cdef_count; bi++) {
+                int32x4_t mse32 = vdupq_n_s32(0);
+                by              = dlist[bi].by;
+                bx              = dlist[bi].bx;
+                mse_4xn_8bit_neon(
+                    &src8, dst8 + (8 * by + 0) * dstride + 4 * bx, dstride, &mse32, 8, subsampling_factor);
+                sum_32_to_64(mse32, &mse64);
+            }
+        } else if (bsize == BLOCK_8X4) {
+            for (bi = 0; bi < cdef_count; bi++) {
+                int32x4_t mse32 = vdupq_n_s32(0);
+                by              = dlist[bi].by;
+                bx              = dlist[bi].bx;
+                mse_8xn_8bit_neon(&src8, dst8 + 4 * by * dstride + 8 * bx, dstride, &mse32, 4, subsampling_factor);
+                sum_32_to_64(mse32, &mse64);
+            }
+        } else {
+            assert(bsize == BLOCK_4X4);
+            for (bi = 0; bi < cdef_count; bi++) {
+                int32x4_t mse32 = vdupq_n_s32(0);
+                by              = dlist[bi].by;
+                bx              = dlist[bi].bx;
+                // For 4x4 blocks, all points can be computed at once.  Subsampling is done in a special function
+                // to avoid accessing memory that doesn't belong to the current picture (since subsampling is implemented
+                // as a multiplier to the step size).
+                if (subsampling_factor == 2)
+                    mse_4x4_8bit_2x_subsampled_neon(&src8, dst8 + 4 * by * dstride + 4 * bx, dstride, &mse32);
+                else
+                    mse_4xn_8bit_neon(&src8, dst8 + 4 * by * dstride + 4 * bx, dstride, &mse32, 4,
+                                      1); // no subsampling
+                sum_32_to_64(mse32, &mse64);
+            }
+        }
+
+        sum = sum64(mse64);
+    }
+    return sum >> 2 * coeff_shift;
+}
+
 static INLINE uint64_t dist_8xn_16bit_neon(const uint16_t **src, const uint16_t *dst, const int32_t dstride,
                                            const int32_t coeff_shift, uint8_t height, uint8_t subsampling_factor) {
     int16x8_t ss = vdupq_n_s16(0);
diff --git a/Source/Lib/Encoder/ASM_NEON/encodetxb_neon.c b/Source/Lib/Encoder/ASM_NEON/encodetxb_neon.c
index e4fd8d019..039d79b66 100644
--- a/Source/Lib/Encoder/ASM_NEON/encodetxb_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/encodetxb_neon.c
@@ -688,3 +688,32 @@ void svt_aom_downsample_2d_neon(uint8_t *input_samples, // input parameter, inpu
             input_samples, input_stride, input_area_width, input_area_height, decim_samples, decim_stride, decim_step);
     }
 }
+
+void svt_copy_mi_map_grid_neon(ModeInfo **mi_grid_ptr, uint32_t mi_stride, uint8_t num_rows, uint8_t num_cols) {
+    ModeInfo *target = mi_grid_ptr[0];
+    if (num_cols == 1) {
+        for (uint8_t mi_y = 0; mi_y < num_rows; mi_y++) {
+            const int32_t mi_idx = 0 + mi_y * mi_stride;
+            // width is 1 block (corresponds to block width 4)
+            mi_grid_ptr[mi_idx] = target;
+        }
+    } else if (num_cols == 2) {
+        const uint64x2_t target_sse = vdupq_n_u64((uint64_t)target);
+        for (uint8_t mi_y = 0; mi_y < num_rows; mi_y++) {
+            const int32_t mi_idx = 0 + mi_y * mi_stride;
+            // width is 2 blocks, so can copy 2 at once (corresponds to block width 8)
+            vst1q_u64((uint64_t *)&mi_grid_ptr[mi_idx], target_sse);
+        }
+    } else {
+        const uint64x2_t target_avx = vdupq_n_u64((uint64_t)target);
+        for (uint8_t mi_y = 0; mi_y < num_rows; mi_y++) {
+            for (uint8_t mi_x = 0; mi_x < num_cols; mi_x += 4) {
+                const int32_t mi_idx = mi_x + mi_y * mi_stride;
+                // width is >=4 blocks, so can copy 4 at once; (corresponds to block width >=16).
+                // All blocks >= 16 have widths that are divisible by 16, so it is ok to copy 4 blocks at once
+                vst1q_u64((uint64_t *)&mi_grid_ptr[mi_idx + 0], target_avx);
+                vst1q_u64((uint64_t *)&mi_grid_ptr[mi_idx + 2], target_avx);
+            }
+        }
+    }
+}
diff --git a/Source/Lib/Encoder/ASM_NEON/highbd_fwd_txfm_neon.c b/Source/Lib/Encoder/ASM_NEON/highbd_fwd_txfm_neon.c
index d0113be34..525304b9e 100644
--- a/Source/Lib/Encoder/ASM_NEON/highbd_fwd_txfm_neon.c
+++ b/Source/Lib/Encoder/ASM_NEON/highbd_fwd_txfm_neon.c
@@ -3659,7 +3659,10 @@ static INLINE void int16_array_with_stride_to_int32_array_without_stride(const i
                                                                          int32_t *output, int txfm1d_size) {
     int r, c;
     for (r = 0; r < txfm1d_size; r++) {
-        for (c = 0; c < txfm1d_size; c++) { output[r * txfm1d_size + c] = (int32_t)input[r * stride + c]; }
+        for (c = 0; c < txfm1d_size; c += 4) {
+            const int32x4_t value = vmovl_s16(vld1_s16(input + r * stride + c));
+            vst1q_s32(output + r * txfm1d_size + c, value);
+        }
     }
 }
 
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
index 11e04818c..0e961ece6 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
@@ -534,7 +534,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_ONLY_C(svt_av1_wedge_compute_delta_squares, svt_av1_wedge_compute_delta_squares_c);
     SET_NEON(svt_av1_wedge_sign_from_residuals, svt_av1_wedge_sign_from_residuals_c, svt_av1_wedge_sign_from_residuals_neon);
     SET_NEON(svt_compute_cdef_dist_16bit, svt_aom_compute_cdef_dist_c, svt_aom_compute_cdef_dist_16bit_neon);
-    SET_ONLY_C(svt_compute_cdef_dist_8bit, svt_aom_compute_cdef_dist_8bit_c);
+    SET_NEON(svt_compute_cdef_dist_8bit, svt_aom_compute_cdef_dist_8bit_c, svt_aom_compute_cdef_dist_8bit_neon);
     SET_NEON(svt_av1_compute_stats, svt_av1_compute_stats_c, svt_av1_compute_stats_neon);
     SET_NEON(svt_av1_compute_stats_highbd, svt_av1_compute_stats_highbd_c, svt_av1_compute_stats_highbd_neon);
     SET_NEON(svt_av1_lowbd_pixel_proj_error, svt_av1_lowbd_pixel_proj_error_c, svt_av1_lowbd_pixel_proj_error_neon);
@@ -859,7 +859,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_NEON(svt_unpack_and_2bcompress, svt_unpack_and_2bcompress_c, svt_unpack_and_2bcompress_neon);
     SET_NEON(svt_estimate_noise_fp16, svt_estimate_noise_fp16_c, svt_estimate_noise_fp16_neon);
     SET_ONLY_C(svt_estimate_noise_highbd_fp16, svt_estimate_noise_highbd_fp16_c);
-    SET_ONLY_C(svt_copy_mi_map_grid, svt_copy_mi_map_grid_c);
+    SET_NEON(svt_copy_mi_map_grid, svt_copy_mi_map_grid_c, svt_copy_mi_map_grid_neon);
     SET_ONLY_C(svt_av1_add_block_observations_internal, svt_av1_add_block_observations_internal_c);
     SET_ONLY_C(svt_av1_pointwise_multiply, svt_av1_pointwise_multiply_c);
     SET_ONLY_C(svt_av1_apply_window_function_to_plane, svt_av1_apply_window_function_to_plane_c);
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
index de7a46870..d70e333dc 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
@@ -1128,6 +1128,9 @@ extern "C" {
     void svt_av1_get_nz_map_contexts_neon(const uint8_t *const levels, const int16_t *const scan, const uint16_t eob, TxSize tx_size, const TxClass tx_class, int8_t *const coeff_contexts);
 
     int32_t svt_estimate_noise_fp16_neon(const uint8_t *src, uint16_t width, uint16_t height, uint16_t stride_y);
+    uint64_t svt_aom_compute_cdef_dist_8bit_neon(const uint8_t *dst8, int32_t dstride, const uint8_t *src8,
+                                                 const CdefList *dlist, int32_t cdef_count, BlockSize bsize,
+                                                 int32_t coeff_shift, int32_t pli, uint8_t subsampling_factor);
 
     void svt_av1_fwd_txfm2d_16x16_N4_neon(int16_t *input, int32_t *output, uint32_t input_stride, TxType transform_type, uint8_t  bit_depth);
     void svt_av1_fwd_txfm2d_32x32_N4_neon(int16_t *input, int32_t *output, uint32_t stride, TxType tx_type, uint8_t bd);
@@ -1152,6 +1155,8 @@ extern "C" {
 
     int svt_aom_satd_neon(const TranLow *coeff, int length);
 
+    void svt_copy_mi_map_grid_neon(ModeInfo **mi_grid_ptr, uint32_t mi_stride, uint8_t num_rows, uint8_t num_cols);
+
     int64_t svt_aom_sse_neon(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride, int width, int height);
     int64_t svt_av1_block_error_neon(const TranLow *coeff, const TranLow *dqcoeff, intptr_t block_size, int64_t *ssz);
     int8_t svt_av1_wedge_sign_from_residuals_neon(const int16_t *ds, const uint8_t *m, int N, int64_t limit);
diff --git a/test/AdaptiveScanTest.cc b/test/AdaptiveScanTest.cc
index 8de3ffd2e..c6368e5c0 100644
--- a/test/AdaptiveScanTest.cc
+++ b/test/AdaptiveScanTest.cc
@@ -142,7 +142,7 @@ TEST(AdaptiveScanTest, scan_tables_test) {
 }
 
 using svt_av1_test_tool::SVTRandom;
-TEST(CopyMiMapGrid, avx2) {
+TEST(CopyMiMapGrid, TestCopyGridMap) {
     SVTRandom rnd(0, (1 << 10) - 1);
     const int max_size = 100;
     ModeInfo *mi_grid_ref[max_size * max_size];
@@ -159,7 +159,14 @@ TEST(CopyMiMapGrid, avx2) {
         mi_grid_ref[0] = mi_grid_tst[0] = (ModeInfo *)((uint64_t)rnd.random());
 
         svt_copy_mi_map_grid_c(mi_grid_ref, stride, txb_height, txb_width);
+
+#if defined(ARCH_X86_64)
         svt_copy_mi_map_grid_avx2(mi_grid_tst, stride, txb_height, txb_width);
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+        svt_copy_mi_map_grid_neon(mi_grid_tst, stride, txb_height, txb_width);
+#endif  // defined(ARCH_AARCH64)
 
         EXPECT_TRUE(memcmp(mi_grid_ref, mi_grid_tst, sizeof(mi_grid_ref)) == 0);
     }
@@ -174,7 +181,14 @@ TEST(CopyMiMapGrid, avx2) {
         mi_grid_ref[0] = mi_grid_tst[0] = (ModeInfo *)((uint64_t)rnd.random());
 
         svt_copy_mi_map_grid_c(mi_grid_ref, stride, cols, cols);
+
+#if defined(ARCH_X86_64)
         svt_copy_mi_map_grid_avx2(mi_grid_tst, stride, cols, cols);
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+        svt_copy_mi_map_grid_neon(mi_grid_tst, stride, cols, cols);
+#endif  // defined(ARCH_AARCH64)
 
         EXPECT_TRUE(memcmp(mi_grid_ref, mi_grid_tst, sizeof(mi_grid_ref)) == 0);
     }
diff --git a/test/CMakeLists.txt b/test/CMakeLists.txt
index dc3b675a3..acf9cc199 100644
--- a/test/CMakeLists.txt
+++ b/test/CMakeLists.txt
@@ -87,6 +87,7 @@ set(arch_neutral_files
     ../third_party/aom_dsp/src/entdec.c)
 
 set(multi_arch_files
+    AdaptiveScanTest.cc
     CdefTest.cc
     FwdTxfm2dAsmTest.cc
     InvTxfm2dAsmTest.cc
@@ -106,7 +107,6 @@ set(multi_arch_files
 
 if(HAVE_X86_PLATFORM)
     set(x86_arch_files
-        AdaptiveScanTest.cc
         CompoundUtilTest.cc
         DeblockTest.cc
         EbNoiseModel_test.cc
diff --git a/test/CdefTest.cc b/test/CdefTest.cc
index f75083afc..a89920b58 100644
--- a/test/CdefTest.cc
+++ b/test/CdefTest.cc
@@ -659,8 +659,6 @@ INSTANTIATE_TEST_SUITE_P(
 
 }  // namespace
 
-#if defined(ARCH_X86_64)
-
 /**
  * @brief Unit test for svt_aom_copy_rect8_8bit_to_16bit_avx2
  *
@@ -704,6 +702,7 @@ TEST(CdefToolTest, CopyRectMatchTest) {
             svt_aom_copy_rect8_8bit_to_16bit_c(
                 dst_ref_, CDEF_BSTRIDE, src_, CDEF_BSTRIDE, vsize, hsize);
 
+#if defined(ARCH_X86_64)
             // Test the SSE4.1 copy function
             memset(dst_data_tst_, 0, sizeof(dst_data_tst_));
             svt_aom_copy_rect8_8bit_to_16bit_sse4_1(
@@ -713,8 +712,8 @@ TEST(CdefToolTest, CopyRectMatchTest) {
                 for (int j = 0; j < hsize; ++j)
                     ASSERT_EQ(dst_ref_[i * CDEF_BSTRIDE + j],
                               dst_tst_[i * CDEF_BSTRIDE + j])
-                        << "copy_rect8_8bit_to_16bit failed with pos(" << i
-                        << " " << j << ")";
+                        << "copy_rect8_8bit_to_16bit_sse4_1 failed with pos("
+                        << i << " " << j << ")";
             }
 
             // Test the AVX2 copy function
@@ -725,12 +724,27 @@ TEST(CdefToolTest, CopyRectMatchTest) {
                 for (int j = 0; j < hsize; ++j)
                     ASSERT_EQ(dst_ref_[i * CDEF_BSTRIDE + j],
                               dst_tst_[i * CDEF_BSTRIDE + j])
-                        << "copy_rect8_8bit_to_16bit failed with pos(" << i
+                        << "copy_rect8_8bit_to_16bit_avx2 failed with pos(" << i
                         << " " << j << ")";
             }
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+            // Test the NEON copy function
+            memset(dst_data_tst_, 0, sizeof(dst_data_tst_));
+            svt_aom_copy_rect8_8bit_to_16bit_neon(
+                dst_tst_, CDEF_BSTRIDE, src_, CDEF_BSTRIDE, vsize, hsize);
+
+            for (int i = 0; i < vsize; ++i) {
+                for (int j = 0; j < hsize; ++j)
+                    ASSERT_EQ(dst_ref_[i * CDEF_BSTRIDE + j],
+                              dst_tst_[i * CDEF_BSTRIDE + j])
+                        << "copy_rect8_8bit_to_16bit_neon failed with pos(" << i
+                        << " " << j << ")";
+            }
+#endif  // defined(ARCH_AARCH64)
         }
 }
-#endif  // defined(ARCH_X86_64)
 
 /**
  * @brief Unit test for svt_aom_compute_cdef_dist_16bit_avx2
@@ -863,8 +877,6 @@ TEST(CdefToolTest, ComputeCdefDistMatchTest) {
     }
 }
 
-#if defined(ARCH_X86_64)
-
 TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
     const int stride = 1 << MAX_SB_SIZE_LOG2;
     const int buf_size = 1 << (MAX_SB_SIZE_LOG2 * 2);
@@ -882,7 +894,7 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
 
         const int coeff_shift = bd - 8;
         SVTRandom skip_rnd_(0, 1);
-        for (int k = 0; k < 100; ++k) {
+        for (int k = 0; k < 10; ++k) {
             CdefList dlist[MI_SIZE_128X128 * MI_SIZE_128X128];
             int cdef_count = 0;
 
@@ -915,6 +927,8 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
                                                              coeff_shift,
                                                              plane,
                                                              subsampling);
+
+#if defined(ARCH_X86_64)
                         // SSE4.1
                         const uint64_t sse_mse =
                             svt_aom_compute_cdef_dist_8bit_sse4_1(dst_data_,
@@ -946,13 +960,31 @@ TEST(CdefToolTest, ComputeCdefDist8bitMatchTest) {
                             << "svt_aom_compute_cdef_dist_8bit_avx2 failed "
                             << "bitdepth: " << bd << " plane: " << plane
                             << " BlockSize " << test_bs[i] << " loop: " << k;
+#endif  // defined(ARCH_X86_64)
+
+#if defined(ARCH_AARCH64)
+                        // NEON
+                        const uint64_t sse_mse =
+                            svt_aom_compute_cdef_dist_8bit_neon(dst_data_,
+                                                                stride,
+                                                                src_data_,
+                                                                dlist,
+                                                                cdef_count,
+                                                                test_bs[i],
+                                                                coeff_shift,
+                                                                plane,
+                                                                subsampling);
+                        ASSERT_EQ(c_mse, sse_mse)
+                            << "svt_aom_compute_cdef_dist_8bit_neon failed "
+                            << "bitdepth: " << bd << " plane: " << plane
+                            << " BlockSize " << test_bs[i] << " loop: " << k;
+#endif  // defined(ARCH_AARCH64)
                     }
                 }
             }
         }
     }
 }
-#endif  // defined(ARCH_X86_64)
 
 /**
  * @brief Unit test for svt_search_one_dual_avx2
diff --git a/test/EbUnitTestUtility.c b/test/EbUnitTestUtility.c
index 6a4f716b6..e83f7d380 100644
--- a/test/EbUnitTestUtility.c
+++ b/test/EbUnitTestUtility.c
@@ -41,8 +41,9 @@
 void svt_buf_random_void(void *const buf, const uint32_t sizeBuf) {
     uint8_t *const buffer = (uint8_t *)buf;
 
-    for (uint32_t i = 0; i < sizeBuf; i++)
+    for (uint32_t i = 0; i < sizeBuf; i++) {
         buffer[i] = (uint8_t)(rand() % 256);
+    }
 }
 
 void svt_buf_random_u8(uint8_t *const buf, const uint32_t sizeBuf) {
diff --git a/test/FwdTxfm2dAsmTest.cc b/test/FwdTxfm2dAsmTest.cc
index f5c435869..f6c3691ad 100644
--- a/test/FwdTxfm2dAsmTest.cc
+++ b/test/FwdTxfm2dAsmTest.cc
@@ -207,6 +207,19 @@ static const FwdTxfm2dFunc fwd_txfm_2d_N4_asm512_func[TX_SIZES_ALL] = {
 
 #ifdef ARCH_AARCH64
 
+static const FwdTxfm2dFunc fwd_txfm_2d_neon_func[TX_SIZES_ALL] = {
+    svt_av1_fwd_txfm2d_4x4_neon,   svt_av1_fwd_txfm2d_8x8_neon,
+    svt_av1_fwd_txfm2d_16x16_neon, svt_av1_fwd_txfm2d_32x32_neon,
+    svt_av1_fwd_txfm2d_64x64_neon, svt_av1_fwd_txfm2d_4x8_neon,
+    svt_av1_fwd_txfm2d_8x4_neon,   svt_av1_fwd_txfm2d_8x16_neon,
+    svt_av1_fwd_txfm2d_16x8_neon,  svt_av1_fwd_txfm2d_16x32_neon,
+    svt_av1_fwd_txfm2d_32x16_neon, svt_av1_fwd_txfm2d_32x64_neon,
+    svt_av1_fwd_txfm2d_64x32_neon, svt_av1_fwd_txfm2d_4x16_neon,
+    svt_av1_fwd_txfm2d_16x4_neon,  svt_av1_fwd_txfm2d_8x32_neon,
+    svt_av1_fwd_txfm2d_32x8_neon,  svt_av1_fwd_txfm2d_16x64_neon,
+    svt_av1_fwd_txfm2d_64x16_neon,
+};
+
 static const FwdTxfm2dFunc fwd_txfm_2d_N4_neon_func[TX_SIZES_ALL] = {
     NULL,
     NULL,
@@ -410,6 +423,20 @@ class FwdTxfm2dAsmTest : public ::testing::TestWithParam<FwdTxfm2dAsmParam> {
 
 #ifdef ARCH_AARCH64
 
+    void run_match_test_neon() {
+        FwdTxfm2dFunc ref_func = fwd_txfm_2d_c_func[tx_size_];
+        FwdTxfm2dFunc test_func = fwd_txfm_2d_neon_func[tx_size_];
+        execute_test(test_func, ref_func, DEFAULT_SHAPE);
+    }
+
+    void speed_test_neon() {
+        FwdTxfm2dFunc ref_func = fwd_txfm_2d_c_func[tx_size_];
+        FwdTxfm2dFunc test_func = fwd_txfm_2d_neon_func[tx_size_];
+        run_speed_test("C    AND NEON", test_func, ref_func);
+        run_speed_test(
+            "NEON AND N4  ", fwd_txfm_2d_N4_neon_func[tx_size_], test_func);
+    }
+
     void run_match_test_neon_N4() {
         FwdTxfm2dFunc test_func_neon = fwd_txfm_2d_N4_neon_func[tx_size_];
         FwdTxfm2dFunc test_func_c = fwd_txfm_2d_N4_c_func[tx_size_];
@@ -649,10 +676,18 @@ TEST_P(FwdTxfm2dAsmTest, DISABLED_speed_test_512) {
 
 #ifdef ARCH_AARCH64
 
+TEST_P(FwdTxfm2dAsmTest, match_test_neon) {
+    run_match_test_neon();
+}
+
 TEST_P(FwdTxfm2dAsmTest, match_test_neon_N4) {
     run_match_test_neon_N4();
 }
 
+TEST_P(FwdTxfm2dAsmTest, DISABLED_speed_test_neon) {
+    speed_test_neon();
+}
+
 #endif /* ARCH_AARCH64 */
 
 INSTANTIATE_TEST_SUITE_P(
diff --git a/test/TemporalFilterTestPlanewise.cc b/test/TemporalFilterTestPlanewise.cc
index 5a8916bac..24ee38739 100644
--- a/test/TemporalFilterTestPlanewise.cc
+++ b/test/TemporalFilterTestPlanewise.cc
@@ -1351,6 +1351,7 @@ int32_t estimate_noise_fp16_c_wrapper(const uint16_t *src, int width,
     return svt_estimate_noise_fp16_c(
         (const uint8_t *)src, width, height, stride);
 }
+
 int32_t estimate_noise_fp16_neon_wrapper(const uint16_t *src, int width,
                                          int height, int stride, int bd) {
     UNUSED(bd);
-- 
GitLab


From a137700337c2be7dd52df95f3bfcd7124396885a Mon Sep 17 00:00:00 2001
From: Gerardo Puga <glpuga@gmail.com>
Date: Fri, 14 Jun 2024 13:03:05 +0000
Subject: [PATCH 13/13] Fix bad assert failing in debug build

---
 .../ASM_NEON/highbd_jnt_convolve_neon.c       |  1 -
 test/PackUnPackTest.cc                        | 14 ++---
 test/SpatialFullDistortionTest.cc             |  6 +-
 test/TemporalFilterTestPlanewise.cc           |  2 +-
 test/VarianceTest.cc                          | 27 ++++-----
 test/convolve_2d_test.cc                      | 58 +++++++++----------
 test/quantize_func_test.cc                    | 16 +++++
 7 files changed, 69 insertions(+), 55 deletions(-)

diff --git a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
index 368a32ea3..3ac691a07 100644
--- a/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
+++ b/Source/Lib/Common/ASM_NEON/highbd_jnt_convolve_neon.c
@@ -216,7 +216,6 @@ void svt_av1_highbd_jnt_convolve_y_neon(const uint16_t *src, int32_t src_stride,
     const int             fo_vert    = filter_params_y->taps / 2 - 1;
     const uint16_t *const src_ptr    = src - fo_vert * src_stride;
 
-    assert(bits >= 0);
     int       i, j;
     const int do_average       = conv_params->do_average;
     const int use_jnt_comp_avg = conv_params->use_jnt_comp_avg;
diff --git a/test/PackUnPackTest.cc b/test/PackUnPackTest.cc
index 006c49c8f..4a442dce1 100644
--- a/test/PackUnPackTest.cc
+++ b/test/PackUnPackTest.cc
@@ -306,7 +306,7 @@ TEST_P(PackMsbTest, PackMsbTest) {
 
 #ifdef ARCH_X86_64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     PACKMSB, PackMsbTest,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
                        ::testing::Values(svt_compressed_packmsb_sse4_1_intrin,
@@ -322,12 +322,12 @@ INSTANTIATE_TEST_SUITE_P(
 
 #ifdef ARCH_AARCH64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     PACKMSB, PackMsbTest,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
                        ::testing::Values(svt_compressed_packmsb_neon)));
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     PACKMSB_EXTEND, PackMsbTest,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES_EXTEND),
                        ::testing::Values(svt_compressed_packmsb_neon)));
@@ -466,7 +466,7 @@ TEST_P(Unpack2bCompress, Unpack2bCompress) {
 
 #ifdef ARCH_X86_64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     UNPACK2BCOMPRESS, Unpack2bCompress,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
                        ::testing::Values(svt_unpack_and_2bcompress_sse4_1,
@@ -493,17 +493,17 @@ INSTANTIATE_TEST_SUITE_P(
 
 #ifdef ARCH_AARCH64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     UNPACK2BCOMPRESS, Unpack2bCompress,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES),
                        ::testing::Values(svt_unpack_and_2bcompress_neon)));
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     UNPACK2BCOMPRESS_EXTEND, Unpack2bCompress,
     ::testing::Combine(::testing::ValuesIn(TEST_PACK_SIZES_EXTEND),
                        ::testing::Values(svt_unpack_and_2bcompress_neon)));
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     UNPACK2BCOMPRESS_EXTEND2, Unpack2bCompress,
     ::testing::Combine(::testing::Values(AreaSize(32, 1), AreaSize(32, 2),
                                          AreaSize(32, 3), AreaSize(32, 5),
diff --git a/test/SpatialFullDistortionTest.cc b/test/SpatialFullDistortionTest.cc
index 6f864993e..15d053b1e 100644
--- a/test/SpatialFullDistortionTest.cc
+++ b/test/SpatialFullDistortionTest.cc
@@ -194,9 +194,7 @@ INSTANTIATE_TEST_SUITE_P(
 #endif
 
 typedef enum { VAL_MIN, VAL_MAX, VAL_RANDOM } TestPattern;
-#ifdef ARCH_X86_64
 TestPattern TEST_PATTERNS[] = {VAL_MIN, VAL_MAX, VAL_RANDOM};
-#endif  // ARCH_X86_64
 typedef std::tuple<uint32_t, uint32_t> AreaSize;
 
 /**
@@ -574,7 +572,7 @@ INSTANTIATE_TEST_SUITE_P(
 
 #ifdef ARCH_AARCH64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     FullDistortionKernel16FuncTest_NEON, FullDistortionKernel16BitsFuncTest,
     ::testing::Combine(
         ::testing::ValuesIn(TEST_AREA_SIZES),
@@ -757,7 +755,7 @@ INSTANTIATE_TEST_SUITE_P(
 
 #ifdef ARCH_AARCH64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     NEON, fullDistortionKernelCbfZero32Bits,
     ::testing::Values(svt_full_distortion_kernel_cbf_zero32_bits_neon));
 
diff --git a/test/TemporalFilterTestPlanewise.cc b/test/TemporalFilterTestPlanewise.cc
index 24ee38739..5a12a1f03 100644
--- a/test/TemporalFilterTestPlanewise.cc
+++ b/test/TemporalFilterTestPlanewise.cc
@@ -1446,7 +1446,7 @@ INSTANTIATE_TEST_SUITE_P(
 
 #ifdef ARCH_AARCH64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     NEON_lbd, EstimateNoiseTestFP,
     ::testing::Combine(::testing::Values(estimate_noise_fp16_c_wrapper),
                        ::testing::Values(estimate_noise_fp16_neon_wrapper),
diff --git a/test/VarianceTest.cc b/test/VarianceTest.cc
index e28fa06ea..95a4575e6 100644
--- a/test/VarianceTest.cc
+++ b/test/VarianceTest.cc
@@ -120,6 +120,8 @@ class MseTest : public ::testing::TestWithParam<TestMseParam> {
     MSE_NXM_FUNC mse_ref_;
 };
 
+#ifdef ARCH_X86_64
+
 TEST_P(MseTest, MatchTest) {
     run_match_test();
 };
@@ -128,9 +130,7 @@ TEST_P(MseTest, MaxTest) {
     run_max_test();
 };
 
-#ifdef ARCH_X86_64
-
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     Variance, MseTest,
     ::testing::Values(
         TestMseParam(16, 16, &svt_aom_mse16x16_sse2, &svt_aom_mse16x16_c),
@@ -213,6 +213,8 @@ class MseTestHighbd : public ::testing::TestWithParam<TestMseParamHighbd> {
     MSE_HIGHBD_NXM_FUNC mse_ref_;
 };
 
+#ifdef ARCH_X86_64
+
 TEST_P(MseTestHighbd, MatchTest) {
     run_match_test();
 };
@@ -221,12 +223,10 @@ TEST_P(MseTestHighbd, MaxTest) {
     run_max_test();
 };
 
-#ifdef ARCH_X86_64
-
-INSTANTIATE_TEST_CASE_P(Variance, MseTestHighbd,
-                        ::testing::Values(TestMseParamHighbd(
-                            16, 16, &svt_aom_highbd_8_mse16x16_sse2,
-                            &svt_aom_highbd_8_mse16x16_c)));
+INSTANTIATE_TEST_SUITE_P(Variance, MseTestHighbd,
+                         ::testing::Values(TestMseParamHighbd(
+                             16, 16, &svt_aom_highbd_8_mse16x16_sse2,
+                             &svt_aom_highbd_8_mse16x16_c)));
 
 #endif  // ARCH_X86_64
 
@@ -411,7 +411,7 @@ TEST_P(VarianceTest, OneQuarterTest) {
 
 #ifdef ARCH_X86_64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     Variance, VarianceTest,
     ::testing::Values(
         VarianceParam(4, 4, &svt_aom_variance4x4_c, &svt_aom_variance4x4_sse2),
@@ -517,7 +517,7 @@ INSTANTIATE_TEST_SUITE_P(
 
 #ifdef ARCH_AARCH64
 
-INSTANTIATE_TEST_CASE_P(
+INSTANTIATE_TEST_SUITE_P(
     Variance_NEON, VarianceTest,
     ::testing::Values(VarianceParam(16, 4, &svt_aom_variance16x4_c,
                                     &svt_aom_variance16x4_neon),
@@ -1033,12 +1033,13 @@ INSTANTIATE_TEST_SUITE_P(AVX512, AvxSubpelVarianceTest,
                          ::testing::ValuesIn(kArraySubpelVariance_avx512));
 #endif
 
-#endif  // ARCH_X86_64
-
 TEST_P(AvxSubpelVarianceTest, Ref) {
     RefTest();
 }
 TEST_P(AvxSubpelVarianceTest, ExtremeRef) {
     ExtremeRefTest();
 }
+
+#endif  // ARCH_X86_64
+
 }  // namespace
diff --git a/test/convolve_2d_test.cc b/test/convolve_2d_test.cc
index c2a070e97..b676a6fb5 100644
--- a/test/convolve_2d_test.cc
+++ b/test/convolve_2d_test.cc
@@ -1106,12 +1106,12 @@ INSTANTIATE_TEST_SUITE_P(ConvolveTestY, AV1HbdJntConvolve2DTest,
 // not yet ported
 // INSTANTIATE_TEST_SUITE_P(NEON_COPY, AV1HbdJntConvolve2DTest,
 //                         BuildParams(0, 0, 2, 1));
-INSTANTIATE_TEST_CASE_P(NEON_ConvolveTest2D, AV1HbdJntConvolve2DTest,
-                        BuildParams(1, 1, 2, 1));
-INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestX, AV1HbdJntConvolve2DTest,
-                        BuildParams(1, 0, 2, 1));
-INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdJntConvolve2DTest,
-                        BuildParams(0, 1, 2, 1));
+INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTest2D, AV1HbdJntConvolve2DTest,
+                         BuildParams(1, 1, 2, 1));
+INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestX, AV1HbdJntConvolve2DTest,
+                         BuildParams(1, 0, 2, 1));
+INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestY, AV1HbdJntConvolve2DTest,
+                         BuildParams(0, 1, 2, 1));
 
 #endif  // ARCH_AARCH64
 
@@ -1179,36 +1179,36 @@ TEST_P(AV1HbdSrConvolve2DTest, DISABLED_SpeedTest) {
 
 #ifdef ARCH_X86_64
 
-INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestX, AV1HbdSrConvolve2DTest,
-                        BuildParams(1, 0, 1, 1));
-INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTest2D, AV1HbdSrConvolve2DTest,
-                        BuildParams(1, 1, 1, 1));
-INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestY, AV1HbdSrConvolve2DTest,
-                        BuildParams(0, 1, 1, 1));
-INSTANTIATE_TEST_CASE_P(SSS3E_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
-                        BuildParams(0, 0, 1, 1));
-
-INSTANTIATE_TEST_CASE_P(ConvolveTestX, AV1HbdSrConvolve2DTest,
-                        BuildParams(1, 0, 0, 1));
-INSTANTIATE_TEST_CASE_P(ConvolveTest2D, AV1HbdSrConvolve2DTest,
-                        BuildParams(1, 1, 0, 1));
-INSTANTIATE_TEST_CASE_P(ConvolveTestY, AV1HbdSrConvolve2DTest,
-                        BuildParams(0, 1, 0, 1));
-INSTANTIATE_TEST_CASE_P(ConvolveTestCopy, AV1HbdSrConvolve2DTest,
-                        BuildParams(0, 0, 0, 1));
+INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTestX, AV1HbdSrConvolve2DTest,
+                         BuildParams(1, 0, 1, 1));
+INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTest2D, AV1HbdSrConvolve2DTest,
+                         BuildParams(1, 1, 1, 1));
+INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTestY, AV1HbdSrConvolve2DTest,
+                         BuildParams(0, 1, 1, 1));
+INSTANTIATE_TEST_SUITE_P(SSS3E_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
+                         BuildParams(0, 0, 1, 1));
+
+INSTANTIATE_TEST_SUITE_P(ConvolveTestX, AV1HbdSrConvolve2DTest,
+                         BuildParams(1, 0, 0, 1));
+INSTANTIATE_TEST_SUITE_P(ConvolveTest2D, AV1HbdSrConvolve2DTest,
+                         BuildParams(1, 1, 0, 1));
+INSTANTIATE_TEST_SUITE_P(ConvolveTestY, AV1HbdSrConvolve2DTest,
+                         BuildParams(0, 1, 0, 1));
+INSTANTIATE_TEST_SUITE_P(ConvolveTestCopy, AV1HbdSrConvolve2DTest,
+                         BuildParams(0, 0, 0, 1));
 
 #endif  // ARCH_X86_64
 
 #ifdef ARCH_AARCH64
 
-// INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestX, AV1HbdSrConvolve2DTest,
+// INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestX, AV1HbdSrConvolve2DTest,
 //                         BuildParams(1, 0, 2, 1));            Not yet
 //                         implemented
-INSTANTIATE_TEST_CASE_P(NEON_ConvolveTest2D, AV1HbdSrConvolve2DTest,
-                        BuildParams(1, 1, 2, 1));
-INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestY, AV1HbdSrConvolve2DTest,
-                        BuildParams(0, 1, 2, 1));
-// INSTANTIATE_TEST_CASE_P(NEON_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
+INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTest2D, AV1HbdSrConvolve2DTest,
+                         BuildParams(1, 1, 2, 1));
+INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestY, AV1HbdSrConvolve2DTest,
+                         BuildParams(0, 1, 2, 1));
+// INSTANTIATE_TEST_SUITE_P(NEON_ConvolveTestCopy, AV1HbdSrConvolve2DTest,
 //                         BuildParams(0, 0, 2, 1));            Not yet
 //                         implemented
 
diff --git a/test/quantize_func_test.cc b/test/quantize_func_test.cc
index 1d17af762..7083fe5c4 100644
--- a/test/quantize_func_test.cc
+++ b/test/quantize_func_test.cc
@@ -262,6 +262,8 @@ class QuantizeLbdTest : public QuantizeTest<QuantizeParam, QuantizeFunc> {
     }
 };
 
+#ifdef ARCH_X86_64
+
 TEST_P(QuantizeLbdTest, ZeroInput) {
     FillCoeffZero();
     QuantizeRun(false);
@@ -376,6 +378,8 @@ TEST_P(QuantizeLbdTest, DISABLED_Speed) {
     }
 }
 
+#endif  // ARCH_X86_64
+
 class QuantizeHbdTest : public QuantizeTest<QuantizeHbdParam, QuantizeHbdFunc> {
   protected:
     QuantizeHbdTest() {
@@ -468,6 +472,8 @@ class QuantizeHbdTest : public QuantizeTest<QuantizeHbdParam, QuantizeHbdFunc> {
     }
 };
 
+#ifdef ARCH_X86_64
+
 TEST_P(QuantizeHbdTest, ZeroInput) {
     FillCoeffZero();
     QuantizeRun(false);
@@ -584,6 +590,8 @@ TEST_P(QuantizeHbdTest, DISABLED_Speed) {
     }
 }
 
+#endif  // ARCH_X86_64
+
 class QuantizeQmTest : public QuantizeTest<QuantizeQmParam, QuantizeQmFunc> {
   protected:
     QuantizeQmTest() {
@@ -728,6 +736,8 @@ class QuantizeQmTest : public QuantizeTest<QuantizeQmParam, QuantizeQmFunc> {
     int qm_level_;
 };
 
+#ifdef ARCH_X86_64
+
 TEST_P(QuantizeQmTest, ZeroInput) {
     FillCoeffZero();
     QuantizeRun(false);
@@ -760,6 +770,10 @@ TEST_P(QuantizeQmTest, CoeffHalfDequant) {
     QuantizeRun(false, 25, 1);
 }
 
+#endif  // ARCH_X86_64
+
+#ifdef ARCH_X86_64
+
 using QuantizeQmHbdTest = QuantizeQmTest;
 
 TEST_P(QuantizeQmHbdTest, ZeroInput) {
@@ -794,6 +808,8 @@ TEST_P(QuantizeQmHbdTest, CoeffHalfDequant) {
     QuantizeRun(false, 25, 1);
 }
 
+#endif  // ARCH_X86_64
+
 using std::make_tuple;
 
 #ifdef ARCH_X86_64
-- 
GitLab

